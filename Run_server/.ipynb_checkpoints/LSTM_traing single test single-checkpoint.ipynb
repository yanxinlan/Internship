{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-19 21:22:18.130225\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "\n",
    "#data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "\n",
    "# NN\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score, roc_auc_score, auc, accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the full files pathes are here\n",
    "# time dependent\n",
    "DATA_PATH_stages=\"../data/kdigo_stages_measured.csv\" \n",
    "DATA_PATH_labs = \"../data/labs-kdigo_stages_measured.csv\" \n",
    "DATA_PATH_vitals = \"../data/vitals-kdigo_stages_measured.csv\" \n",
    "DATA_PATH_vents = \"../data/vents-vasopressor-sedatives-kdigo_stages_measured.csv\"\n",
    "#no time dependent\n",
    "DATA_PATH_detail=\"../data/icustay_detail-kdigo_stages_measured.csv\" #age constraint\n",
    "#DATA_PATH_icd = \"../data/diagnoses_icd_aki_measured.csv\" #AL was \"...measured 2.csv\"\n",
    "\n",
    "SEPARATOR=\";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter as constant \n",
    "\n",
    "TESTING = False \n",
    "TEST_SIZE = 0.01\n",
    "\n",
    "SPLIT_SIZE = 0.2 \n",
    "MAX_DAYS = 35\n",
    "\n",
    "#which classifier to use, only run one classifier at one time \n",
    "CLASS1 = True   #AnyAKI\n",
    "#CLASS2 = False    #ModerateSevereAKI\n",
    "#CLASS3 = False    #SevereAKI\n",
    "ALL_STAGES = False # not binary label, each class separately 0,1,2,3\n",
    "    \n",
    "SELECTED_FEATURE_SET = False\n",
    "MAX_FEATURE_SET = True\n",
    "#DIAGNOSIS = False\n",
    "\n",
    "FIRST_TURN_POS = True # creating one label per one ICU stay id\n",
    "\n",
    "# resampling  and imputing\n",
    "TIME_SAMPLING = True \n",
    "SAMPLING_INTERVAL = '6H'\n",
    "RESAMPLE_LIMIT = 16 # 4 days*6h interval\n",
    "MOST_COMMON = False #resampling with most common\n",
    "# if MOST_COMMON is not applied,sampling with different strategies per kind of variable, \n",
    "# numeric variables use mean value, categorical variables use max value\n",
    "IMPUTE_EACH_ID = True # imputation within each icustay_id with most common value\n",
    "IMPUTE_COLUMN = True # imputation based on whole column\n",
    "IMPUTE_METHOD = 'most_frequent' \n",
    "FILL_VALUE = 0 #fill missing value and ragged part of 3d array\n",
    "\n",
    "#Age constraints: adults\n",
    "ADULTS_MIN_AGE = 18\n",
    "ADULTS_MAX_AGE = -1\n",
    "\n",
    "NORMALIZATION = True\n",
    "CAPPING = True\n",
    "\n",
    "if CAPPING:\n",
    "    CAPPING_THRESHOLD_UPPER = 0.99\n",
    "    CAPPING_THRESHOLD_LOWER = 0.01\n",
    "\n",
    "# How much time the prediction should occur (hours)\n",
    "HOURS_AHEAD = 48\n",
    "\n",
    "NORM_TYPE = 'min_max'\n",
    "\n",
    "RANDOM = 42\n",
    "\n",
    "#set changable info corresponding to each classifier as variables\n",
    "\n",
    "min_set =  [\"icustay_id\", \"charttime\", \"creat\", \"uo_rt_6hr\", \"uo_rt_12hr\", \"uo_rt_24hr\", \"aki_stage\"]\n",
    "\n",
    "\n",
    "#selected_set = \n",
    "\n",
    "\n",
    "max_set = ['icustay_id', 'charttime', 'aki_stage', 'hadm_id', 'albumin_avg','aniongap_avg', 'bicarbonate_avg', \n",
    "           'bilirubin_avg', 'bun_avg','chloride_avg', 'creat', 'diasbp_mean', 'glucose_avg', 'heartrate_mean',\n",
    "           'hematocrit_avg', 'hemoglobin_avg', 'potassium_avg', 'resprate_mean','sodium_avg', 'spo2_mean', 'sysbp_mean', \n",
    "           'uo_rt_12hr', 'uo_rt_24hr','uo_rt_6hr', 'wbc_avg', 'sedative', 'vasopressor', 'vent', 'age', 'F','M', \n",
    "           'asian', 'black', 'hispanic', 'native', 'other', 'unknown','white', 'ELECTIVE', 'EMERGENCY', 'URGENT']\n",
    "\n",
    "# naming model and plot\n",
    "classifier_name = \"None vs. Any AKI\"    ###change every time #Moderate vs. Severe #None vs. Any #Others vs. Severe\n",
    "plot_name = \"adult_AnyAKI_LR\"    ###change every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions used later\n",
    "if CAPPING:\n",
    "    def cap_data(df):\n",
    "        print(\"Capping between the {} and {} quantile\".format(CAPPING_THRESHOLD_LOWER, CAPPING_THRESHOLD_UPPER))\n",
    "        cap_mask = df.columns.difference(['icustay_id', 'charttime', 'aki_stage'])\n",
    "        df[cap_mask] = df[cap_mask].clip(df[cap_mask].quantile(CAPPING_THRESHOLD_LOWER),\n",
    "                                         df[cap_mask].quantile(CAPPING_THRESHOLD_UPPER),\n",
    "                                         axis=1)\n",
    "\n",
    "        return df\n",
    " \n",
    "    \n",
    "def normalise_data(df, norm_mask):\n",
    "    print(\"Normalizing in [0,1] with {} normalization\".format(NORMALIZATION))\n",
    "\n",
    "    if NORM_TYPE == 'min_max':\n",
    "        df[norm_mask] = (df[norm_mask] - df[norm_mask].min()) / (df[norm_mask].max() - df[norm_mask].min())\n",
    "    else:\n",
    "        df[norm_mask] = (df[norm_mask] - df[norm_mask].mean()) / df[norm_mask].std()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# impute missing value in resampleing data with most common based on each id\n",
    "def fast_mode(df, key_cols, value_col):\n",
    "    \"\"\" Calculate a column mode, by group, ignoring null values. \n",
    "    \n",
    "    key_cols : list of str - Columns to groupby for calculation of mode.\n",
    "    value_col : str - Column for which to calculate the mode. \n",
    "\n",
    "    Return\n",
    "    pandas.DataFrame\n",
    "        One row for the mode of value_col per key_cols group. If ties, returns the one which is sorted first. \"\"\"\n",
    "    return (df.groupby(key_cols + [value_col]).size() \n",
    "              .to_frame('counts').reset_index() \n",
    "              .sort_values('counts', ascending=False) \n",
    "              .drop_duplicates(subset=key_cols)).drop('counts',axis=1)\n",
    "\n",
    "\n",
    "#get max shape of 3d array\n",
    "def get_dimensions(array, level=0):   \n",
    "    yield level, len(array)\n",
    "    try:\n",
    "        for row in array:\n",
    "            yield from get_dimensions(row, level + 1)\n",
    "    except TypeError: #not an iterable\n",
    "        pass\n",
    "\n",
    "def get_max_shape(array):\n",
    "    dimensions = defaultdict(int)\n",
    "    for level, length in get_dimensions(array):\n",
    "        dimensions[level] = max(dimensions[level], length)\n",
    "    return [value for _, value in sorted(dimensions.items())]\n",
    "\n",
    "#pad the ragged 3d array to rectangular shape based on max size\n",
    "def iterate_nested_array(array, index=()):\n",
    "    try:\n",
    "        for idx, row in enumerate(array):\n",
    "            yield from iterate_nested_array(row, (*index, idx))\n",
    "    except TypeError: # final level            \n",
    "        yield (*index, slice(len(array))), array\n",
    "\n",
    "def pad(array, fill_value):\n",
    "    dimensions = get_max_shape(array)\n",
    "    result = np.full(dimensions, fill_value)\n",
    "    for index, value in iterate_nested_array(array):\n",
    "        result[index] = value\n",
    "    return result\n",
    "\n",
    "def bin_total(y_true, y_prob, n_bins):\n",
    "    bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "\n",
    "    # In sklearn.calibration.calibration_curve,\n",
    "    # the last value in the array is always 0.\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "    return np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "def missing_bin(bin_array):\n",
    "    midpoint = \" \"    \n",
    "    if bin_array[0]==0:\n",
    "        midpoint = \"5%, \"\n",
    "    if bin_array[1]==0:\n",
    "        midpoint = midpoint + \"15%, \"\n",
    "    if bin_array[2]==0:\n",
    "        midpoint = midpoint + \"25%, \"\n",
    "    if bin_array[3]==0:\n",
    "        midpoint = midpoint + \"35%, \" \n",
    "    if bin_array[4]==0:\n",
    "        midpoint = midpoint + \"45%, \"\n",
    "    if bin_array[5]==0:\n",
    "        midpoint = midpoint + \"55%, \"\n",
    "    if bin_array[6]==0:\n",
    "        midpoint = midpoint + \"65%, \"\n",
    "    if bin_array[7]==0:\n",
    "        midpoint = midpoint + \"75%, \"\n",
    "    if bin_array[8]==0:\n",
    "        midpoint = midpoint + \"85%, \"\n",
    "    if bin_array[9]==0:\n",
    "        midpoint = midpoint + \"95%, \"\n",
    "    return \"The missing bins have midpoint values of \"+ str(midpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read csv files\n",
      "convert charttime to timestamp\n"
     ]
    }
   ],
   "source": [
    "print(\"read csv files\")\n",
    "#reading csv files\n",
    "X = pd.read_csv(DATA_PATH_stages, sep= SEPARATOR)\n",
    "X.drop([\"aki_stage_creat\", \"aki_stage_uo\"], axis = 1, inplace = True)\n",
    "#remove totally empty rows \n",
    "X = X.dropna(how = 'all', subset = ['creat','uo_rt_6hr','uo_rt_12hr','uo_rt_24hr','aki_stage'])\n",
    "print(\"convert charttime to timestamp\")\n",
    "X['charttime'] = pd.to_datetime(X['charttime'])\n",
    "# AL it substitutes missing values with zero!\n",
    "#merge rows if they have exact timestamp within same icustay_id\n",
    "#X = X.groupby(['icustay_id', 'charttime']).sum().reset_index(['icustay_id', 'charttime'])\n",
    "\n",
    "dataset_detail = pd.read_csv(DATA_PATH_detail, sep= SEPARATOR)  #age constraint\n",
    "dataset_detail.drop(['dod', 'admittime','dischtime', 'los_hospital','ethnicity','hospital_expire_flag', 'hospstay_seq',\n",
    "       'first_hosp_stay', 'intime', 'outtime', 'los_icu', 'icustay_seq','first_icu_stay'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert charttime to timestamp\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "dataset_labs = pd.read_csv(DATA_PATH_labs, sep= SEPARATOR) # 'bands lactate platelet ptt inr pt\n",
    "dataset_labs.drop(['albumin_min', 'albumin_max','bilirubin_min', 'bilirubin_max','bands_min', 'bands_max',\n",
    "                   'lactate_min', 'lactate_max','platelet_min', 'platelet_max','ptt_min', 'ptt_max', \n",
    "                   'inr_min', 'inr_max', 'pt_min', 'pt_max'], axis = 1, inplace = True)\n",
    "dataset_labs = dataset_labs.dropna(subset=['charttime'])\n",
    "dataset_labs = dataset_labs.dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "dataset_labs['charttime'] = pd.to_datetime(dataset_labs['charttime'])\n",
    "dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "if SELECTED_FEATURE_SET or MAX_FEATURE_SET:\n",
    "    #4,5,6\n",
    "    dataset_vitals = pd.read_csv(DATA_PATH_vitals, sep= SEPARATOR)  #'meanbp_mean', 'tempc_mean',\n",
    "    dataset_vents = pd.read_csv(DATA_PATH_vents , sep= SEPARATOR)\n",
    "    #dataset_icd = pd.read_csv(DATA_PATH_icd, sep= SEPARATOR)\n",
    "\n",
    "    dataset_vitals.drop([\"heartrate_min\", \"heartrate_max\",\"sysbp_min\", \"sysbp_max\",\"diasbp_min\", \"diasbp_max\",\n",
    "                        'meanbp_min','meanbp_max', 'meanbp_mean','tempc_min', 'tempc_max', 'tempc_mean',\n",
    "                        \"resprate_min\", \"resprate_max\", \"spo2_min\", \"spo2_max\", \"glucose_min\", \"glucose_max\"], axis = 1, inplace = True)\n",
    "          \n",
    "    print(\"convert charttime to timestamp\")\n",
    "    dataset_vitals['charttime'] = pd.to_datetime(dataset_vitals['charttime'])\n",
    "    dataset_vents['charttime'] = pd.to_datetime(dataset_vents['charttime'])\n",
    "    \n",
    "    dataset_vitals = dataset_vitals.sort_values(by=['icustay_id', 'charttime'])\n",
    "    dataset_vents = dataset_vents.sort_values(by=['icustay_id', 'charttime'])\n",
    "    \n",
    "    # AL drop those where all columns are nan\n",
    "    dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')   \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labs file: instead of min and max their avg\n",
    "counter = 0\n",
    "col1 = 4\n",
    "col2 = 5\n",
    "null_l = [] # no null values in those that are different\n",
    "changed = 0 # 4316 records changed to avg\n",
    "\n",
    "while counter < 11:\n",
    "    row = 0\n",
    "# find where min and max are different and save their row indices \n",
    "    while row < len(dataset_labs):\n",
    "        a = dataset_labs.iloc[row,col1]\n",
    "        b = dataset_labs.iloc[row,col2]\n",
    "        if a==b or (np.isnan(a) and np.isnan(b)):\n",
    "            pass\n",
    "        elif a!=b:\n",
    "            changed +=1\n",
    "            avg = (a+b)/2\n",
    "            dataset_labs.iloc[row,col1] = avg\n",
    "            if (np.isnan(a) and ~np.isnan(b)) or (np.isnan(b) and ~np.isnan(a)):\n",
    "                null_l.append(row)\n",
    "        else:\n",
    "            print(a)\n",
    "            print(b)\n",
    "        row +=1       \n",
    "    # delete the redundant column max, update counters\n",
    "    dataset_labs.drop(dataset_labs.columns[col2], axis=1, inplace = True)\n",
    "    counter = counter+1\n",
    "    col1 = col1+1\n",
    "    col2 = col2+1\n",
    "\n",
    "dataset_labs.columns = ['subject_id','hadm_id', 'icustay_id', 'charttime', 'aniongap_avg', 'bicarbonate_avg', \n",
    "                        'creatinine_avg', 'chloride_avg', 'glucose_avg', 'hematocrit_avg','hemoglobin_avg',\n",
    "                        'potassium_avg', 'sodium_avg', 'bun_avg', 'wbc_avg']\n",
    "if len(null_l)>0:\n",
    "    print(\"null values encountered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge creatinine and glucose.\n"
     ]
    }
   ],
   "source": [
    "print(\"Merge creatinine and glucose.\")\n",
    "# merge creatinine from labs and set with labels\n",
    "creat_l = dataset_labs[['icustay_id','charttime','creatinine_avg']].copy()\n",
    "creat_l = creat_l.dropna(subset=['creatinine_avg'])\n",
    "creat = X[['icustay_id','charttime', 'creat']].copy()\n",
    "creat = creat.dropna(subset=['creat'])\n",
    "creat_l = creat_l.rename(columns={\"creatinine_avg\": \"creat\"})\n",
    "creat = creat.append(creat_l, ignore_index=True)\n",
    "creat.drop_duplicates(inplace = True)\n",
    "#delete old columns\n",
    "dataset_labs.drop([\"creatinine_avg\"], axis = 1, inplace = True)\n",
    "dataset_labs = dataset_labs.dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "X.drop([\"creat\"], axis = 1, inplace = True)\n",
    "#merge new column\n",
    "X = pd.merge(X, creat, on = [\"icustay_id\", \"charttime\"], sort = True, how= \"outer\", copy = False)\n",
    "\n",
    "if SELECTED_FEATURE_SET or MAX_FEATURE_SET:\n",
    "    # merge glucose from vitals and labs\n",
    "    glucose_v = dataset_vitals[['subject_id','hadm_id','icustay_id','charttime', 'glucose_mean']].copy()\n",
    "    glucose_v = glucose_v.dropna(subset=['glucose_mean'])\n",
    "    glucose = dataset_labs[['subject_id','hadm_id','icustay_id','charttime', 'glucose_avg']].copy()\n",
    "    glucose = glucose.dropna(subset=['glucose_avg'])\n",
    "    glucose_v = glucose_v.rename(columns={\"glucose_mean\": \"glucose_avg\"})\n",
    "    glucose = glucose.append(glucose_v, ignore_index=True)\n",
    "    glucose.drop_duplicates(inplace = True)\n",
    "    #delete old columns\n",
    "    dataset_labs.drop([\"glucose_avg\"], axis = 1, inplace = True)\n",
    "    dataset_vitals.drop([\"glucose_mean\"], axis = 1, inplace = True)\n",
    "    dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')\n",
    "    #merge new column\n",
    "    dataset_labs = pd.merge(dataset_labs, glucose, on = ['subject_id','hadm_id','icustay_id','charttime',], sort = True, how= \"outer\", copy = False)\n",
    "    \n",
    "dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging labs, vitals and vents files\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging labs, vitals and vents files\")\n",
    "#merge files with time-dependent data, based on icustay_id and charttime\n",
    "if SELECTED_FEATURE_SET or MAX_FEATURE_SET:\n",
    "    X = pd.merge(X, dataset_labs, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False)\n",
    "    X = pd.merge(X, dataset_vitals, on = [\"icustay_id\", \"charttime\",\"subject_id\", \"hadm_id\"], how= \"outer\", copy = False)\n",
    "    X = pd.merge(X, dataset_vents, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False) \n",
    "    X.drop([\"subject_id\"], axis = 1, inplace = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocessing time dependent data\n",
      "Removing patients under the min age\n"
     ]
    }
   ],
   "source": [
    "print(\"start preprocessing time dependent data\") # AL removed a line where rows with missing labels are deleted (we will ffil)\n",
    "print(\"Removing patients under the min age\")\n",
    "dataset_detail = dataset_detail.loc[dataset_detail['age'] >= ADULTS_MIN_AGE]\n",
    "adults_icustay_id_list = dataset_detail['icustay_id'].unique()\n",
    "X = X[X.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id'], ignore_index = True)\n",
    "X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "adults_icustay_id_list = np.sort(adults_icustay_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop icustay_id with time span less than 48hrs\n",
      "2302 long stays\n",
      "there are 5214 id-s shorter than 48 hours\n"
     ]
    }
   ],
   "source": [
    "print(\"drop icustay_id with time span less than 48hrs\")\n",
    "def more_than_HOURS_ahead(adults_icustay_id_list, X):\n",
    "    drop_list = []\n",
    "    los_list = [] # calculating LOS ICU based on charttime\n",
    "    long_stays_id = [] # LOS longer than MAX DAYS days\n",
    "    last_charttime_list = []\n",
    "    seq_length = X.groupby(['icustay_id'],as_index=False).size().to_frame('size')\n",
    "    id_count = 0\n",
    "    first_row_index = 0\n",
    "\n",
    "    while id_count < len(adults_icustay_id_list):\n",
    "        icustay_id = adults_icustay_id_list[id_count]\n",
    "        last_row_index = first_row_index + seq_length.iloc[id_count,0]-1\n",
    "        first_time = X.iat[first_row_index, X.columns.get_loc('charttime')]\n",
    "        last_time = X.iat[last_row_index, X.columns.get_loc('charttime')]\n",
    "        los = round(float((last_time - first_time).total_seconds()/60/60/24),4) # in days\n",
    "        if los < HOURS_AHEAD/24:\n",
    "            drop_list.append(icustay_id)\n",
    "        else:\n",
    "            los_list.append(los)\n",
    "            if los > MAX_DAYS:\n",
    "                long_stays_id.append(icustay_id)\n",
    "                last_charttime_list.append(last_time)\n",
    "        # udpate for the next icustay_id\n",
    "        first_row_index = last_row_index+1\n",
    "        id_count +=1\n",
    "    if len(long_stays_id) != len(last_charttime_list):\n",
    "        print('ERROR')\n",
    "    print(\"%d long stays\" % len(long_stays_id))\n",
    "    # drop all the rows with the saved icustay_id\n",
    "    print(\"there are %d id-s shorter than 48 hours\" % len(drop_list))\n",
    "    X = X[~X.icustay_id.isin(drop_list)]\n",
    "    id_list = X['icustay_id'].unique()\n",
    "    X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "    \n",
    "    return id_list, X, long_stays_id,last_charttime_list\n",
    "\n",
    "id_list, X, long_stays_id,last_charttime_list  = more_than_HOURS_ahead(adults_icustay_id_list, X)\n",
    "\n",
    "long = pd.DataFrame()\n",
    "long['icustay_id']  = long_stays_id\n",
    "long['last_time']  = last_charttime_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 long stays\n",
      "there are 1 id-s shorter than 48 hours\n"
     ]
    }
   ],
   "source": [
    "# deleting rows that are not within MAX_DAYS (35) period\n",
    "i = 0 # long df index\n",
    "drop_long_time = []\n",
    "    \n",
    "while i < len(long_stays_id):\n",
    "    j = 0\n",
    "    all_rows = X.index[X['icustay_id'] == long.loc[i,'icustay_id']].tolist()\n",
    "    while j < len(all_rows):\n",
    "        time = X.iat[all_rows[j], X.columns.get_loc('charttime')]\n",
    "        # if keep last MAX_DAYS \n",
    "        if (long.loc[i,'last_time'] - time).total_seconds() > MAX_DAYS*24*60*60:\n",
    "            drop_long_time.append(all_rows[j])\n",
    "            j +=1\n",
    "        else:\n",
    "            break\n",
    "    i +=1       \n",
    "X.drop(X.index[drop_long_time], inplace=True) \n",
    "\n",
    "# checking for 48h min length again\n",
    "id_list, X, long_stays_id,last_charttime_list  = more_than_HOURS_ahead(id_list, X)\n",
    "dataset_detail = dataset_detail[dataset_detail.icustay_id.isin(id_list)].sort_values(by=['icustay_id'], ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SELECTED_FEATURE_SET or MAX_FEATURE_SET:\n",
    "    # AL create a dictionary for hadm\n",
    "    hadm = dataset_detail.filter(['hadm_id','icustay_id'],axis = 1)\n",
    "    dict_hadm = pd.Series(hadm.hadm_id.values,index=hadm.icustay_id).to_dict()\n",
    "    # fill in the missing values (to ensure correct merging of icd below)\n",
    "    X.hadm_id = X.hadm_id.fillna(FILL_VALUE)\n",
    "    # AL change the type to prevent warning of merging int on float\n",
    "    X = X.astype({\"hadm_id\": int})\n",
    "    a = -1\n",
    "    while a < X.shape[0]-1:\n",
    "        a = a+1\n",
    "        if X.iat[a, X.columns.get_loc('hadm_id')] !=-1 :\n",
    "            continue\n",
    "        elif X.iat[a, X.columns.get_loc('hadm_id')]==-1:\n",
    "            X.iat[a, X.columns.get_loc('hadm_id')] = dict_hadm[X.iat[a, X.columns.get_loc('icustay_id')]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For testing purpose, use small amount of data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing purpose, use small amount of data first\n",
    "if TESTING:\n",
    "    rest, id_list = train_test_split(id_list, test_size= TEST_SIZE, random_state=42)\n",
    "    X = X[X.icustay_id.isin(id_list)].sort_values(by=['icustay_id'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.sort_values(by=['icustay_id', 'charttime'])\n",
    "id_list = X['icustay_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling , imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resampling: MEAN & ZERO\n"
     ]
    }
   ],
   "source": [
    "if (TIME_SAMPLING and MOST_COMMON):\n",
    "    print(\"resampling: MOST_COMMON\")\n",
    "    # Resample the data using assigned interval,mode() for most common\n",
    "    X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL).mode().reset_index()\n",
    "elif TIME_SAMPLING:\n",
    "    print(\"resampling: MEAN & ZERO\")\n",
    "    # Sampling with different strategies per kind of variable\n",
    "    label = ['aki_stage']\n",
    "    skip = ['icustay_id', 'charttime', 'aki_stage']\n",
    "    if SELECTED_FEATURE_SET or MAX_FEATURE_SET:\n",
    "        discrete_feat = ['sedative', 'vasopressor', 'vent', 'hadm_id']\n",
    "        skip.extend(discrete_feat)    \n",
    "    # all features that are not in skip are numeric\n",
    "    numeric_feat = list(X.columns.difference(skip))\n",
    "    \n",
    "    # Applying aggregation to features depending on their type\n",
    "    X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n",
    "    if SELECTED_FEATURE_SET or MAX_FEATURE_SET:\n",
    "        X_discrete = X[discrete_feat].max().fillna(FILL_VALUE).astype(np.int64)\n",
    "    X_numeric = X[numeric_feat].mean() \n",
    "    X_label = X['aki_stage'].max()\n",
    "    print(\"Merging sampled features\")\n",
    "    try:\n",
    "        X = pd.concat([X_numeric, X_discrete,X_label], axis=1).reset_index()\n",
    "    except:\n",
    "        X = pd.concat([X_numeric,X_label], axis=1).reset_index()\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Imputation.\")\n",
    "\n",
    "#Label\n",
    "X['aki_stage'] = X['aki_stage'].ffill(limit=RESAMPLE_LIMIT)\n",
    "# do imputation of label with zero if there are still missing values\n",
    "X['aki_stage'] = X['aki_stage'].fillna(0)\n",
    "# using most common within each icustay_id\n",
    "if IMPUTE_EACH_ID:\n",
    "    # set a new variable so won't change the orginial X\n",
    "    column_name = list(X.columns)\n",
    "    column_name.remove(column_name[0]) \n",
    "    for feature in column_name:\n",
    "        X.loc[X[feature].isnull(), feature] = X.icustay_id.map(fast_mode(X, ['icustay_id'], feature).set_index('icustay_id')[feature])       \n",
    "\n",
    "# imputation based on whole column\n",
    "if IMPUTE_COLUMN:\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy= IMPUTE_METHOD)\n",
    "    cols = list(X.columns)\n",
    "    cols = cols[2:23]\n",
    "    X[cols]=imp.fit_transform(X[cols])  \n",
    "\n",
    "# If no imputation method selected or only impute each id, for the remaining nan impute direclty with FILL_VALUE\n",
    "X = X.fillna(FILL_VALUE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more comfortable to review in this order, aki stage to be first after all else is disregarded\n",
    "try:\n",
    "    cols = ['icustay_id', 'charttime','aki_stage','hadm_id','aniongap_avg','bicarbonate_avg', 'bun_avg','chloride_avg',\n",
    "            'creat','diasbp_mean', 'glucose_avg', 'heartrate_mean', 'hematocrit_avg','hemoglobin_avg', \n",
    "            'potassium_avg', 'resprate_mean', 'sodium_avg','spo2_mean', 'sysbp_mean', 'uo_rt_12hr', \n",
    "            'uo_rt_24hr', 'uo_rt_6hr','wbc_avg', 'sedative', 'vasopressor', 'vent' ]\n",
    "    X = X[cols]\n",
    "except:\n",
    "    try:\n",
    "        cols = ['icustay_id', 'charttime','aki_stage','creat','uo_rt_12hr', 'uo_rt_24hr', 'uo_rt_6hr']\n",
    "        X = X[cols]\n",
    "    except:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Cap features between 0.01 / 0.99 quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if CAPPING:\n",
    "    X = cap_data(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if NORMALIZATION:\n",
    "    X = normalise_data(X, numeric_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create a label df (1 label per ICU stay id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarise the labels\n",
    "if ALL_STAGES:\n",
    "    pass\n",
    "elif CLASS1:\n",
    "    X.loc[X['aki_stage'] > 1, 'aki_stage'] = 1\n",
    "elif CLASS2:\n",
    "    X.loc[X['aki_stage'] < 2, 'aki_stage'] = 0\n",
    "    X.loc[X['aki_stage'] > 1, 'aki_stage'] = 1\n",
    "elif CLASS3:\n",
    "    X.loc[X['aki_stage'] < 3, 'aki_stage'] = 0\n",
    "    X.loc[X['aki_stage'] > 2, 'aki_stage'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['aki_stage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X['icustay_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create 1 label per ICU stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "last_charttime_list= []\n",
    "charttime_index_list = []\n",
    "label_list = []\n",
    "first_row_index = 0\n",
    "id_count = 0\n",
    "\n",
    "seq_length = X.groupby(['icustay_id'],as_index=False).size().to_frame('size')\n",
    "for ID in id_list:\n",
    "    last_row_index = first_row_index + seq_length.iloc[id_count,0]-1\n",
    "    a = X.loc[X['icustay_id']==ID].aki_stage\n",
    "    if 1 not in a.values:\n",
    "        label_list.append(0)\n",
    "        last_charttime_list.append(X.iat[last_row_index, X.columns.get_loc('charttime')]) \n",
    "        charttime_index_list.append(last_row_index)\n",
    "    elif 1 in a.values:\n",
    "        label_list.append(1)\n",
    "        row = first_row_index\n",
    "        while row != last_row_index+1:\n",
    "            if X.iat[row, X.columns.get_loc('aki_stage')]==0:\n",
    "                row +=1\n",
    "            elif X.iat[row, X.columns.get_loc('aki_stage')]==1:\n",
    "                last_charttime_list.append(X.iat[row, X.columns.get_loc('charttime')])\n",
    "                charttime_index_list.append(row)\n",
    "                break\n",
    "    first_row_index = last_row_index+1\n",
    "    id_count +=1\n",
    "\n",
    "label_df = pd.DataFrame({'icustay_id': id_list, \n",
    "        'charttime_label': last_charttime_list, \n",
    "        'aki_stage': label_list} )\n",
    "Thresholds = pd.DataFrame({'icustay_id':id_list, 'charttime': last_charttime_list})\n",
    "X = (Thresholds.merge(X, on='icustay_id', how='left',suffixes=('_x','')).query(\"charttime_x >= charttime\").reindex(columns=X.columns)) \n",
    "id_list, X, m,n  = more_than_HOURS_ahead(id_list, X)\n",
    "label_df = label_df[label_df.icustay_id.isin(id_list)].sort_values(by=['icustay_id'], ignore_index = True)\n",
    "last_charttime_list = list(label_df['charttime_label'])\n",
    "X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#a threshold list to store the threshold time(last charttime-48hrs) for every icustay_id\n",
    "threshold_list=[]\n",
    "id_count = 0\n",
    "t=0 #index for last_charttime_list\n",
    "\n",
    "for ID in id_list:\n",
    "    a = X.loc[X['icustay_id']==ID]\n",
    "    c = 1\n",
    "    index = a.shape[0]-1\n",
    "    while c != index+1 and t < len(id_list):\n",
    "        if (last_charttime_list[t]- a.iat[index-c, a.columns.get_loc('charttime')]).total_seconds() < HOURS_AHEAD*60*60:\n",
    "            c +=1\n",
    "        elif (last_charttime_list[t]-a.iat[index-c, a.columns.get_loc('charttime')]).total_seconds() >= HOURS_AHEAD*60*60:\n",
    "            threshold_list.append(a.iloc[index-c]['charttime'])\n",
    "            c=index+1    \n",
    "    t +=1\n",
    "            \n",
    "Thresholds = pd.DataFrame({'icustay_id':id_list, 'charttime': threshold_list})\n",
    "X = (Thresholds.merge(X, on='icustay_id', how='left',suffixes=('_x','')).query(\"charttime_x >= charttime\").reindex(columns=X.columns)) \n",
    "X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "label_df['aki_stage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['icustay_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['aki_stage'].value_counts() # this is ok as we select for negative set those that have no 1, \n",
    "                              # for the positive set we select 48h before first turns positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add categorical features (details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no time dependent data\n",
    "print(\"start preprocessing not time dependent data\")\n",
    "if SELECTED_FEATURE_SET or MAX_FEATURE_SET:\n",
    "    #extract datasets based on id_list\n",
    "    dataset_detail = dataset_detail.loc[dataset_detail['icustay_id'].isin(id_list)]\n",
    "    #sort by ascending order\n",
    "    dataset_detail = dataset_detail.sort_values(by=['icustay_id'])\n",
    "    subject_id = dataset_detail[\"subject_id\"].unique()\n",
    "    \n",
    "    #transfrom categorical data to binary form\n",
    "    dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop('gender')))\n",
    "    dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop(\"ethnicity_grouped\")))\n",
    "    dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop('admission_type')))\n",
    "    dataset_detail = dataset_detail.drop(['subject_id', 'hadm_id'], axis=1)\n",
    "    # AL merge\n",
    "    X =  pd.merge(X, dataset_detail, on = [\"icustay_id\"], how= \"left\", copy = False) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Filter for the selected features\")\n",
    "if SELECTED_FEATURE_SET:\n",
    "    X = X[selected_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = X.copy(deep = True)\n",
    "#X = test.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AL re-write as try except to make it work as hadm_id is not used if only one csv file is used and none are merged\n",
    "try:\n",
    "    X.drop(['hadm_id','aki_stage', 'charttime'], axis=1, inplace = True)\n",
    "except:\n",
    "    X.drop(['aki_stage', 'charttime'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = X.shape[1]-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"divide dataset into train, test sets\")\n",
    "# divide dataset into train, test sets\n",
    "train, test_val = train_test_split(X, test_size = SPLIT_SIZE, random_state = 42)\n",
    "test, val = train_test_split(test_val, test_size = 0.5, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"reshape 2D dataframe to 3D Array, group by icustay_id\")\n",
    "train = np.array(sorted(list(train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)),key=len, reverse=True))\n",
    "test = np.array(sorted(list(test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)),key=len, reverse=True))\n",
    "validation = np.array(sorted(list(validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)),key=len, reverse=True))\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_y(X,label_df):\n",
    "    order = []\n",
    "    i = 0\n",
    "    while i < X.shape[0]:\n",
    "        a = X[i]\n",
    "        a = a[0]\n",
    "        order.append(int(a[0]))\n",
    "        i +=1\n",
    "    y = []\n",
    "    for i in order:\n",
    "        a = label_df.loc[label_df['icustay_id']==i]\n",
    "        y.append(a.iloc[0,2])\n",
    "        y = numpy.array(y)\n",
    "    return(y)\n",
    "y_train = create_y(train, label_df)\n",
    "y_test = create_y(test,label_df)\n",
    "y_val = create_y(val, label_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (torch.cuda.is_available()):\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('Training on CPU') # On mac book GPU is not possible =() \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size \n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.activation = nn.Sigmoid() #here might use RELU\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden_size) # I can have a few (IV) within this line - documentation        \n",
    "        self.fc2 = nn.LSTM(self.hidden_size, self.output_size,num_layers=2, batch_first = True) # this is where it returns a tuple instead of tensor\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.fc1(x) \n",
    "        h, _ = self.fc2(h) # h, _ : as I have 2outputs (tuple), only take the real output [0]. \n",
    "        #print(type(h)) # Underscore throughs away the rest, _ \"I do not care\" variable notation in python\n",
    "        h = self.activation(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 988257, 1: 117462})\n"
     ]
    }
   ],
   "source": [
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    times = math.floor(data.shape[0]/batch_size)\n",
    "    remainder = data.shape[0]%times\n",
    "    a = 0\n",
    "    start = 0\n",
    "    end = start+batch_size\n",
    "    if remainder ==0:\n",
    "        a +=1\n",
    "    while a<times:\n",
    "        temp = pad(data[start:end,],0)\n",
    "        x = Variable(torch.from_numpy(temp[:,:,2:]).float())\n",
    "        y = torch.flatten(Variable(torch.from_numpy(temp[:, :,1].reshape(-1,1)).float()).long())\n",
    "        X_batches.append(x)\n",
    "        y_batches.append(y)\n",
    "        start = end\n",
    "        end = start+batch_size\n",
    "        a +=1\n",
    "    temp = pad(data[start:data.shape[0]],0)\n",
    "    x = Variable(torch.from_numpy(temp[:,:,2:]).float())\n",
    "    y = torch.flatten(Variable(torch.from_numpy(temp[:, :,1].reshape(-1,1)).float()).long())\n",
    "    X_batches.append(x)\n",
    "    y_batches.append(y)\n",
    "    if len(X_batches) != len(y_batches):\n",
    "        print(\"length error\")\n",
    "    return X_batches, y_batches # arrays\n",
    "\n",
    "# batching\n",
    "batch_size = 5\n",
    "X_train, y_train = batch(train, batch_size)\n",
    "X_test, y_test = batch(test, batch_size)\n",
    "X_val, y_val = batch(validation, batch_size)\n",
    "\n",
    "# counting balance of the classes\n",
    "y = []\n",
    "for i in y_train:\n",
    "    for element in i:\n",
    "        y.append(element.item())\n",
    "for i in y_test:\n",
    "    for element in i:\n",
    "        y.append(element.item())\n",
    "for i in y_val:\n",
    "    for element in i:\n",
    "        y.append(element.item())\n",
    "#  weights\n",
    "counter=collections.Counter(y)\n",
    "print(counter)\n",
    "zeroes = counter[0]\n",
    "ones = counter[1]\n",
    "\n",
    "X_val, y_val = batch(validation, validation.shape[0])\n",
    "X_val = X_val[0]\n",
    "y_val = y_val[0]\n",
    "\n",
    "X_test, y_test = batch(test, test.shape[0])\n",
    "X_test = X_test[0]\n",
    "y_test = y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 8.])\n"
     ]
    }
   ],
   "source": [
    "input_size = features\n",
    "hidden_size = round(input_size/1)# smaller number of features, fixed for now. Might be turn into a parameter if I find out how.\n",
    "if ALL_STAGES and not CLASS1:\n",
    "    output_size = 4\n",
    "else:\n",
    "    output_size = 2 # for CEL 2 for BCE 1\n",
    "\n",
    "#create a network \n",
    "nn_model = Net(input_size, hidden_size, output_size)\n",
    "\n",
    "#print(nn_model)\n",
    "#print(list(nn_model.parameters()))\n",
    "\n",
    "# Cross Entropy Loss  \n",
    "weights = torch.tensor([round(zeroes/zeroes,0), round(zeroes/ones,0)])\n",
    "print(weights)\n",
    "criterion = nn.CrossEntropyLoss(weight = weights)\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=0.001) # 0.001 or 0.0001 is ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch [1] out of 100\n",
      "Training loss: 0.635.. Validation loss: 0.584.. \n",
      "AUC: 0.159 PR AUC: 0.019 \n",
      "\n",
      " Epoch [2] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.181 PR AUC: 0.021 \n",
      "\n",
      " Epoch [3] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.213 PR AUC: 0.024 \n",
      "\n",
      " Epoch [4] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.229 PR AUC: 0.036 \n",
      "\n",
      " Epoch [5] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.287 PR AUC: 0.039 \n",
      "\n",
      " Epoch [6] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [7] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [8] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [9] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [10] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [11] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [12] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [13] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [14] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [15] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [16] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [17] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [18] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [19] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [20] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [21] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [22] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [23] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [24] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [25] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [26] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [27] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [28] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [29] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [30] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [31] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [32] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [33] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [34] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [35] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [36] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [37] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [38] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [39] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [40] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [41] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [42] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [43] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [44] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [45] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [46] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [47] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [48] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [49] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [50] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [51] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [52] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [53] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [54] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [55] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [56] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [57] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [58] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [59] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [60] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [61] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [62] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [63] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [64] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [65] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [66] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [67] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [68] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [69] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [70] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [71] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [72] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [73] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [74] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [75] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [76] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [77] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [78] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [79] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [80] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [81] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [82] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [83] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [84] out of 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [85] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [86] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [87] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [88] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [89] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [90] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [91] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [92] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [93] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [94] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [95] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [96] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [97] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [98] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [99] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "\n",
      " Epoch [100] out of 100\n",
      "Training loss: 0.641.. Validation loss: 0.577.. \n",
      "AUC: 0.560 PR AUC: 0.073 \n",
      "Finished Training\n",
      "starttime = 2020-11-18 22:55:35.612928\n",
      "now = 2020-11-19 09:58:21.463732\n"
     ]
    }
   ],
   "source": [
    "# training loop (full data 3.5 hours)\n",
    "\n",
    "epochs = 100\n",
    "starttime = datetime.now() # datetime object containing current date and time\n",
    "train_losses, validation_losses = [], []\n",
    "best = 0\n",
    "exception = 0\n",
    "\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    print (\"\\n Epoch [%d] out of %d\" % (epoch + 1, epochs))\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    validation_loss = 0.0\n",
    "    roc_auc = 0.0\n",
    "    pr_auc = 0.0\n",
    "    m = 0\n",
    "    \n",
    "    #train\n",
    "    for i in X_train:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad() # zero the gradient buffers not to consider gradients of previous iterations\n",
    "        X_batch = X_train[m]\n",
    "        y_batch = y_train[m]\n",
    "        # forward + backward + optimize\n",
    "        outputs = nn_model(X_batch)\n",
    "        outputs = outputs.view(X_batch.shape[1]*X_batch.shape[0],output_size)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step() # Does the update\n",
    "        running_loss += loss.item()\n",
    "        m +=1\n",
    "    \n",
    "    #validation \n",
    "    nn_model.eval()\n",
    "    with torch.no_grad():\n",
    "        v_out = nn_model(X_val) \n",
    "        v_out = v_out.view(-1,output_size)\n",
    "        v_loss = criterion(v_out, y_val)\n",
    "        val_loss = v_loss.item()\n",
    "        # auc and pr auc\n",
    "        v_out = v_out[:,1]\n",
    "        precision, recall, thresholds = precision_recall_curve(y_val, v_out)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        roc_auc = roc_auc_score(y_val,v_out)\n",
    "        \n",
    "    \n",
    "    print(f\"Training loss: {running_loss/len(X_train):.3f}.. \"\n",
    "                    f\"Validation loss: {val_loss:.3f}.. \")\n",
    "    print(f\"AUC: {roc_auc:.3f} \" f\"PR AUC: {pr_auc:.3f} \")\n",
    "        \n",
    "    nn_model.train()\n",
    "    \n",
    "    validation_losses.append(validation_loss) \n",
    "    train_losses.append(running_loss/len(X_train)) \n",
    "    \n",
    "   \n",
    "    if pr_auc > best:\n",
    "        best = pr_auc\n",
    "        # save the model\n",
    "        PATH = './18Nov_100%35d_imp_id_col_mode_hid2_best.pth'\n",
    "        torch.save(nn_model.state_dict(), PATH)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "       \n",
    "        \n",
    "print('Finished Training')\n",
    "print(\"starttime =\", starttime)\n",
    "now = datetime.now()\n",
    "print(\"now =\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "PATH = './18Nov_100%35d_imp_id_col_mode_hid2.pth'\n",
    "torch.save(nn_model.state_dict(), PATH)\n",
    "\n",
    "# code to load saved model\n",
    "#nn_model = Net(input_size, hidden_size, output_size)\n",
    "#nn_model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next step testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROSS E LOSS scenario\n",
    "pred = nn_model(X_test) # 2 columns that sum up to 1\n",
    "max_rows = pred.shape[1]\n",
    "predictions = pred.detach().numpy()\n",
    "predictions = predictions.reshape(-1,2)\n",
    "predictions = np.delete(predictions, 0, axis=1) # as 2 columns that sum up to 1, we can use one only (for 1)\n",
    "# select 1 per icu stay id by index\n",
    "prob_1_label = []\n",
    "row = 0\n",
    "\n",
    "for i in individual_index:\n",
    "    prob_1_label.append(predictions[row+i])\n",
    "    row += pred.shape[1]\n",
    "prob_1_label = np.array(prob_1_label).reshape(-1,1)\n",
    "\n",
    "labels = np.array(test_label)\n",
    "labels = labels.reshape(-1,1)\n",
    "labels = labels.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1591, 1: 1257})\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "c=collections.Counter(test_label)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold: 0.10 AUC: 0.500\n",
      "threshold: 0.15 AUC: 0.500\n",
      "threshold: 0.20 AUC: 0.500\n",
      "threshold: 0.25 AUC: 0.500\n",
      "threshold: 0.30 AUC: 0.569\n",
      "threshold: 0.35 AUC: 0.500\n",
      "threshold: 0.40 AUC: 0.500\n",
      "threshold: 0.45 AUC: 0.500\n",
      "threshold: 0.50 AUC: 0.500\n",
      "threshold: 0.55 AUC: 0.500\n",
      "threshold: 0.60 AUC: 0.500\n",
      "threshold: 0.65 AUC: 0.500\n",
      "threshold: 0.70 AUC: 0.500\n",
      "threshold: 0.75 AUC: 0.500\n",
      "threshold: 0.80 AUC: 0.500\n",
      "threshold: 0.85 AUC: 0.500\n"
     ]
    }
   ],
   "source": [
    "auc_list = []\n",
    "\n",
    "def preliminary_view (threshold):\n",
    "    a = np.where(prob_1_label > threshold, 1, 0)\n",
    "    #print(\"\\nFrequency of unique values of the predicted array:\")\n",
    "    #print(np.asarray((unique_elements, counts_elements)))\n",
    "    auc = roc_auc_score(labels,a)\n",
    "    auc_list.append(auc)\n",
    "    print (f\"threshold: {i:.2f} \"   f\"AUC: {auc:.3f}\") \n",
    "    \n",
    "# thresholds\n",
    "thresholds = np.arange(0.1,0.9,0.05)\n",
    "for i in thresholds:\n",
    "    preliminary_view(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "standard_threshold_pred = np.where( prob_1_label > 0.5, 1, 0)\n",
    "auc_score = round(roc_auc_score(labels,standard_threshold_pred),2)\n",
    "print(auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 = 0.000, PR auc =0.652\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(labels, prob_1_label)\n",
    "f1 = f1_score(labels,standard_threshold_pred)\n",
    "prauc =auc(recall, precision)\n",
    "print('F1 = %.3f, PR auc =%.3f' % (f1,prauc))\n",
    "\n",
    "# plot the precision-recall curves\n",
    "no_skill = len(labels[labels==1]) / len(labels)\n",
    "plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "plt.plot(recall, precision, marker='.', label='LSTM')\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1591    0]\n",
      " [1257    0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "a = np.where(prob_1_label > 0.5, 1, 0)\n",
    "print(metrics.confusion_matrix(labels, a, labels=None, sample_weight=None, normalize=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my idea\n",
    "''' # does not change much =(\n",
    "if IMPUTE_COLUMN:\n",
    "    cols = list(X.columns)\n",
    "    cols = cols[2:23]\n",
    "    for feature in cols:\n",
    "        X[feature].fillna(X[feature].median(), inplace=True)\n",
    "''' "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
