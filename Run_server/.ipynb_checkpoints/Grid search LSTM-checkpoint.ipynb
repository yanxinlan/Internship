{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECTED_FEATURE_SET = False\n",
    "MAX_FEATURE_SET = True\n",
    "#DIAGNOSIS = False\n",
    "\n",
    "IMPUTE_EACH_ID = False # imputation within each icustay_id with most common value\n",
    "IMPUTE_COLUMN = False # imputation based on whole column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-12 13:32:31.091039\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "#data preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "# NN\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score, roc_auc_score, auc, accuracy_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# the full files pathes are here\n",
    "DATA_PATH_stages=\"../data/kdigo_stages_measured.csv\" \n",
    "DATA_PATH_labs = \"../data/labs-kdigo_stages_measured.csv\" \n",
    "DATA_PATH_vitals = \"../data/vitals-kdigo_stages_measured.csv\" \n",
    "DATA_PATH_vents = \"../data/vents-vasopressor-sedatives-kdigo_stages_measured.csv\"\n",
    "DATA_PATH_detail=\"../data/icustay_detail-kdigo_stages_measured.csv\" \n",
    "#DATA_PATH_icd = \"../data/diagnoses_icd_aki_measured.csv\" #AL was \"...measured 2.csv\"\n",
    "SEPARATOR=\";\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter as constant \n",
    "\n",
    "TESTING = False \n",
    "TEST_SIZE = 0.10\n",
    "\n",
    "SPLIT_SIZE = 0.2 \n",
    "MAX_DAYS = 35\n",
    "\n",
    "#which classifier to use, only run one classifier at one time \n",
    "CLASS1 = True   #AnyAKI\n",
    "#CLASS2 = False    #ModerateSevereAKI\n",
    "#CLASS3 = False    #SevereAKI\n",
    "ALL_STAGES = False # not binary label, each class separately 0,1,2,3\n",
    "\n",
    "FIRST_TURN_POS = True # creating one label per one ICU stay id\n",
    "\n",
    "# resampling  and imputing\n",
    "TIME_SAMPLING = True \n",
    "SAMPLING_INTERVAL = '6H'\n",
    "RESAMPLE_LIMIT = 16 # 4 days*6h interval\n",
    "MOST_COMMON = False #resampling with most common\n",
    "# if MOST_COMMON is not applied,sampling with different strategies per kind of variable, \n",
    "# numeric variables use mean value, categorical variables use max value\n",
    "\n",
    "IMPUTE_METHOD = 'most_frequent' \n",
    "FILL_VALUE = 0 #fill missing value and ragged part of 3d array\n",
    "\n",
    "#Age constraints: adults\n",
    "ADULTS_MIN_AGE = 18\n",
    "ADULTS_MAX_AGE = -1\n",
    "\n",
    "NORMALIZATION = 'min-max' \n",
    "CAPPING = True\n",
    "\n",
    "if CAPPING:\n",
    "    CAPPING_THRESHOLD_UPPER = 0.99\n",
    "    CAPPING_THRESHOLD_LOWER = 0.01\n",
    "\n",
    "# How much time the prediction should occur (hours)\n",
    "HOURS_AHEAD = 48\n",
    "\n",
    "NORM_TYPE = 'min_max'\n",
    "\n",
    "# AL for reproducibility\n",
    "MAN_SEED = 42\n",
    "np.random.seed(MAN_SEED)\n",
    "torch.manual_seed(MAN_SEED)\n",
    "\n",
    "#set changable info corresponding to each classifier as variables\n",
    "\n",
    "min_set =  [\"icustay_id\", \"charttime\", \"creat\", \"uo_rt_6hr\", \"uo_rt_12hr\", \"uo_rt_24hr\", \"aki_stage\"]\n",
    "\n",
    "\n",
    "#selected_set = \n",
    "\n",
    "\n",
    "max_set = ['icustay_id', 'charttime', 'aki_stage', 'hadm_id', 'albumin_avg','aniongap_avg', 'bicarbonate_avg', \n",
    "           'bilirubin_avg', 'bun_avg','chloride_avg', 'creat', 'diasbp_mean', 'glucose_avg', 'heartrate_mean',\n",
    "           'hematocrit_avg', 'hemoglobin_avg', 'potassium_avg', 'resprate_mean','sodium_avg', 'spo2_mean', 'sysbp_mean', \n",
    "           'uo_rt_12hr', 'uo_rt_24hr','uo_rt_6hr', 'wbc_avg', 'sedative', 'vasopressor', 'vent', 'age', 'F','M', \n",
    "           'asian', 'black', 'hispanic', 'native', 'other', 'unknown','white', 'ELECTIVE', 'EMERGENCY', 'URGENT']\n",
    "\n",
    "# naming model and plot\n",
    "classifier_name = \"None vs. Any AKI\"    ###change every time #Moderate vs. Severe #None vs. Any #Others vs. Severe\n",
    "plot_name = \"adult_AnyAKI_LR\"    ###change every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions used later\n",
    "if CAPPING:\n",
    "    def cap_data(df):\n",
    "        print(\"Capping between the {} and {} quantile\".format(CAPPING_THRESHOLD_LOWER, CAPPING_THRESHOLD_UPPER))\n",
    "        cap_mask = df.columns.difference(['icustay_id', 'charttime', 'aki_stage'])\n",
    "        df[cap_mask] = df[cap_mask].clip(df[cap_mask].quantile(CAPPING_THRESHOLD_LOWER),\n",
    "                                         df[cap_mask].quantile(CAPPING_THRESHOLD_UPPER),\n",
    "                                         axis=1)\n",
    "\n",
    "        return df\n",
    " \n",
    "    \n",
    "def normalise_data(df, norm_mask):\n",
    "    print(\"Normalizing in [0,1] with {} normalization\".format(NORMALIZATION))\n",
    "    \n",
    "    df[norm_mask] = (df[norm_mask] - df[norm_mask].min()) / (df[norm_mask].max() - df[norm_mask].min())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# impute missing value in resampleing data with most common based on each id\n",
    "def fast_mode(df, key_cols, value_col):\n",
    "    \"\"\" Calculate a column mode, by group, ignoring null values. \n",
    "    \n",
    "    key_cols : list of str - Columns to groupby for calculation of mode.\n",
    "    value_col : str - Column for which to calculate the mode. \n",
    "\n",
    "    Return\n",
    "    pandas.DataFrame\n",
    "        One row for the mode of value_col per key_cols group. If ties, returns the one which is sorted first. \"\"\"\n",
    "    return (df.groupby(key_cols + [value_col]).size() \n",
    "              .to_frame('counts').reset_index() \n",
    "              .sort_values('counts', ascending=False) \n",
    "              .drop_duplicates(subset=key_cols)).drop('counts',axis=1)\n",
    "\n",
    "\n",
    "#get max shape of 3d array\n",
    "def get_dimensions(array, level=0):   \n",
    "    yield level, len(array)\n",
    "    try:\n",
    "        for row in array:\n",
    "            yield from get_dimensions(row, level + 1)\n",
    "    except TypeError: #not an iterable\n",
    "        pass\n",
    "\n",
    "def get_max_shape(array):\n",
    "    dimensions = defaultdict(int)\n",
    "    for level, length in get_dimensions(array):\n",
    "        dimensions[level] = max(dimensions[level], length)\n",
    "    return [value for _, value in sorted(dimensions.items())]\n",
    "\n",
    "#pad the ragged 3d array to rectangular shape based on max size\n",
    "def iterate_nested_array(array, index=()):\n",
    "    try:\n",
    "        for idx, row in enumerate(array):\n",
    "            yield from iterate_nested_array(row, (*index, idx)) \n",
    "    except TypeError: # final level            \n",
    "        yield (*index, slice(len(array))), array # think of the types\n",
    "\n",
    "def pad(array, fill_value):\n",
    "    dimensions = get_max_shape(array)\n",
    "    result = np.full(dimensions, fill_value, dtype = np.float64)  \n",
    "    for index, value in iterate_nested_array(array):\n",
    "        result[index] = value \n",
    "    return result\n",
    "\n",
    "def bin_total(y_true, y_prob, n_bins):\n",
    "    bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "\n",
    "    # In sklearn.calibration.calibration_curve,\n",
    "    # the last value in the array is always 0.\n",
    "    binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "    return np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "def missing_bin(bin_array):\n",
    "    midpoint = \" \"    \n",
    "    if bin_array[0]==0:\n",
    "        midpoint = \"5%, \"\n",
    "    if bin_array[1]==0:\n",
    "        midpoint = midpoint + \"15%, \"\n",
    "    if bin_array[2]==0:\n",
    "        midpoint = midpoint + \"25%, \"\n",
    "    if bin_array[3]==0:\n",
    "        midpoint = midpoint + \"35%, \" \n",
    "    if bin_array[4]==0:\n",
    "        midpoint = midpoint + \"45%, \"\n",
    "    if bin_array[5]==0:\n",
    "        midpoint = midpoint + \"55%, \"\n",
    "    if bin_array[6]==0:\n",
    "        midpoint = midpoint + \"65%, \"\n",
    "    if bin_array[7]==0:\n",
    "        midpoint = midpoint + \"75%, \"\n",
    "    if bin_array[8]==0:\n",
    "        midpoint = midpoint + \"85%, \"\n",
    "    if bin_array[9]==0:\n",
    "        midpoint = midpoint + \"95%, \"\n",
    "    return \"The missing bins have midpoint values of \"+ str(midpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read csv files\n",
      "convert charttime to timestamp\n"
     ]
    }
   ],
   "source": [
    "print(\"read csv files\")\n",
    "#reading csv files\n",
    "X = pd.read_csv(DATA_PATH_stages, sep= SEPARATOR)\n",
    "X.drop([\"aki_stage_creat\", \"aki_stage_uo\"], axis = 1, inplace = True)\n",
    "#remove totally empty rows \n",
    "X = X.dropna(how = 'all', subset = ['creat','uo_rt_6hr','uo_rt_12hr','uo_rt_24hr','aki_stage'])\n",
    "print(\"convert charttime to timestamp\")\n",
    "X['charttime'] = pd.to_datetime(X['charttime'])\n",
    "# AL it substitutes missing values with zero!\n",
    "#merge rows if they have exact timestamp within same icustay_id\n",
    "#X = X.groupby(['icustay_id', 'charttime']).sum().reset_index(['icustay_id', 'charttime'])\n",
    "\n",
    "dataset_detail = pd.read_csv(DATA_PATH_detail, sep= SEPARATOR)  #age constraint\n",
    "dataset_detail.drop(['dod', 'admittime','dischtime', 'los_hospital','ethnicity','hospital_expire_flag', 'hospstay_seq',\n",
    "       'first_hosp_stay', 'intime', 'outtime', 'los_icu', 'icustay_seq','first_icu_stay'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert charttime to timestamp\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "dataset_labs = pd.read_csv(DATA_PATH_labs, sep= SEPARATOR) # 'bands lactate platelet ptt inr pt\n",
    "dataset_labs.drop(['albumin_min', 'albumin_max','bilirubin_min', 'bilirubin_max','bands_min', 'bands_max',\n",
    "                   'lactate_min', 'lactate_max','platelet_min', 'platelet_max','ptt_min', 'ptt_max', \n",
    "                   'inr_min', 'inr_max', 'pt_min', 'pt_max'], axis = 1, inplace = True)\n",
    "dataset_labs = dataset_labs.dropna(subset=['charttime'])\n",
    "dataset_labs = dataset_labs.dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "dataset_labs['charttime'] = pd.to_datetime(dataset_labs['charttime'])\n",
    "dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "if SELECTED_FEATURE_SET or MAX_FEATURE_SET:\n",
    "    #4,5,6\n",
    "    dataset_vitals = pd.read_csv(DATA_PATH_vitals, sep= SEPARATOR)  #'meanbp_mean', 'tempc_mean',\n",
    "    dataset_vents = pd.read_csv(DATA_PATH_vents , sep= SEPARATOR)\n",
    "    #dataset_icd = pd.read_csv(DATA_PATH_icd, sep= SEPARATOR)\n",
    "\n",
    "    dataset_vitals.drop([\"heartrate_min\", \"heartrate_max\",\"sysbp_min\", \"sysbp_max\",\"diasbp_min\", \"diasbp_max\",\n",
    "                        'meanbp_min','meanbp_max', 'meanbp_mean','tempc_min', 'tempc_max', 'tempc_mean',\n",
    "                        \"resprate_min\", \"resprate_max\", \"spo2_min\", \"spo2_max\", \"glucose_min\", \"glucose_max\"], axis = 1, inplace = True)\n",
    "          \n",
    "    print(\"convert charttime to timestamp\")\n",
    "    dataset_vitals['charttime'] = pd.to_datetime(dataset_vitals['charttime'])\n",
    "    dataset_vents['charttime'] = pd.to_datetime(dataset_vents['charttime'])\n",
    "    \n",
    "    dataset_vitals = dataset_vitals.sort_values(by=['icustay_id', 'charttime'])\n",
    "    dataset_vents = dataset_vents.sort_values(by=['icustay_id', 'charttime'])\n",
    "    \n",
    "    # AL drop those where all columns are nan\n",
    "    dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')   \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labs file: instead of min and max their avg\n",
    "counter = 0\n",
    "col1 = 4\n",
    "col2 = 5\n",
    "null_l = [] # no null values in those that are different\n",
    "changed = 0 # 4316 records changed to avg\n",
    "\n",
    "while counter < 11:\n",
    "    row = 0\n",
    "# find where min and max are different and save their row indices \n",
    "    while row < len(dataset_labs):\n",
    "        a = dataset_labs.iloc[row,col1]\n",
    "        b = dataset_labs.iloc[row,col2]\n",
    "        if a==b or (np.isnan(a) and np.isnan(b)):\n",
    "            pass\n",
    "        elif a!=b:\n",
    "            changed +=1\n",
    "            avg = (a+b)/2\n",
    "            dataset_labs.iloc[row,col1] = avg\n",
    "            if (np.isnan(a) and ~np.isnan(b)) or (np.isnan(b) and ~np.isnan(a)):\n",
    "                null_l.append(row)\n",
    "        else:\n",
    "            print(a)\n",
    "            print(b)\n",
    "        row +=1       \n",
    "    # delete the redundant column max, update counters\n",
    "    dataset_labs.drop(dataset_labs.columns[col2], axis=1, inplace = True)\n",
    "    counter = counter+1\n",
    "    col1 = col1+1\n",
    "    col2 = col2+1\n",
    "\n",
    "dataset_labs.columns = ['subject_id','hadm_id', 'icustay_id', 'charttime', 'aniongap_avg', 'bicarbonate_avg', \n",
    "                        'creatinine_avg', 'chloride_avg', 'glucose_avg', 'hematocrit_avg','hemoglobin_avg',\n",
    "                        'potassium_avg', 'sodium_avg', 'bun_avg', 'wbc_avg']\n",
    "if len(null_l)>0:\n",
    "    print(\"null values encountered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge creatinine and glucose.\n"
     ]
    }
   ],
   "source": [
    "print(\"Merge creatinine and glucose.\")\n",
    "# merge creatinine from labs and set with labels\n",
    "creat_l = dataset_labs[['icustay_id','charttime','creatinine_avg']].copy()\n",
    "creat_l = creat_l.dropna(subset=['creatinine_avg'])\n",
    "creat = X[['icustay_id','charttime', 'creat']].copy()\n",
    "creat = creat.dropna(subset=['creat'])\n",
    "creat_l = creat_l.rename(columns={\"creatinine_avg\": \"creat\"})\n",
    "creat = creat.append(creat_l, ignore_index=True)\n",
    "creat.drop_duplicates(inplace = True)\n",
    "#delete old columns\n",
    "dataset_labs.drop([\"creatinine_avg\"], axis = 1, inplace = True)\n",
    "dataset_labs = dataset_labs.dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "X.drop([\"creat\"], axis = 1, inplace = True)\n",
    "#merge new column\n",
    "X = pd.merge(X, creat, on = [\"icustay_id\", \"charttime\"], sort = True, how= \"outer\", copy = False)\n",
    "\n",
    "if SELECTED_FEATURE_SET or MAX_FEATURE_SET:\n",
    "    # merge glucose from vitals and labs\n",
    "    glucose_v = dataset_vitals[['subject_id','hadm_id','icustay_id','charttime', 'glucose_mean']].copy()\n",
    "    glucose_v = glucose_v.dropna(subset=['glucose_mean'])\n",
    "    glucose = dataset_labs[['subject_id','hadm_id','icustay_id','charttime', 'glucose_avg']].copy()\n",
    "    glucose = glucose.dropna(subset=['glucose_avg'])\n",
    "    glucose_v = glucose_v.rename(columns={\"glucose_mean\": \"glucose_avg\"})\n",
    "    glucose = glucose.append(glucose_v, ignore_index=True)\n",
    "    glucose.drop_duplicates(inplace = True)\n",
    "    #delete old columns\n",
    "    dataset_labs.drop([\"glucose_avg\"], axis = 1, inplace = True)\n",
    "    dataset_vitals.drop([\"glucose_mean\"], axis = 1, inplace = True)\n",
    "    dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')\n",
    "    #merge new column\n",
    "    dataset_labs = pd.merge(dataset_labs, glucose, on = ['subject_id','hadm_id','icustay_id','charttime',], sort = True, how= \"outer\", copy = False)\n",
    "    \n",
    "dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging labs, vitals and vents files\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging labs, vitals and vents files\")\n",
    "#merge files with time-dependent data, based on icustay_id and charttime\n",
    "if SELECTED_FEATURE_SET or MAX_FEATURE_SET:\n",
    "    X = pd.merge(X, dataset_labs, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False)\n",
    "    X = pd.merge(X, dataset_vitals, on = [\"icustay_id\", \"charttime\",\"subject_id\", \"hadm_id\"], how= \"outer\", copy = False)\n",
    "    X = pd.merge(X, dataset_vents, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False) \n",
    "    X.drop([\"subject_id\"], axis = 1, inplace = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocessing time dependent data\n",
      "Removing patients under the min age\n"
     ]
    }
   ],
   "source": [
    "print(\"start preprocessing time dependent data\") # AL removed a line where rows with missing labels are deleted (we will ffil)\n",
    "print(\"Removing patients under the min age\")\n",
    "dataset_detail = dataset_detail.loc[dataset_detail['age'] >= ADULTS_MIN_AGE]\n",
    "adults_icustay_id_list = dataset_detail['icustay_id'].unique()\n",
    "X = X[X.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id'], ignore_index = True)\n",
    "X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "adults_icustay_id_list = np.sort(adults_icustay_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop icustay_id with time span less than 48hrs\n",
      "2302 long stays\n",
      "there are 5214 id-s shorter than 48 hours\n"
     ]
    }
   ],
   "source": [
    "print(\"drop icustay_id with time span less than 48hrs\")\n",
    "def more_than_HOURS_ahead(adults_icustay_id_list, X):\n",
    "    drop_list = []\n",
    "    los_list = [] # calculating LOS ICU based on charttime\n",
    "    long_stays_id = [] # LOS longer than MAX DAYS days\n",
    "    last_charttime_list = []\n",
    "    seq_length = X.groupby(['icustay_id'],as_index=False).size().to_frame('size')\n",
    "    id_count = 0\n",
    "    first_row_index = 0\n",
    "\n",
    "    while id_count < len(adults_icustay_id_list):\n",
    "        icustay_id = adults_icustay_id_list[id_count]\n",
    "        last_row_index = first_row_index + seq_length.iloc[id_count,0]-1\n",
    "        first_time = X.iat[first_row_index, X.columns.get_loc('charttime')]\n",
    "        last_time = X.iat[last_row_index, X.columns.get_loc('charttime')]\n",
    "        los = round(float((last_time - first_time).total_seconds()/60/60/24),4) # in days\n",
    "        if los < HOURS_AHEAD/24:\n",
    "            drop_list.append(icustay_id)\n",
    "        else:\n",
    "            los_list.append(los)\n",
    "            if los > MAX_DAYS:\n",
    "                long_stays_id.append(icustay_id)\n",
    "                last_charttime_list.append(last_time)\n",
    "        # udpate for the next icustay_id\n",
    "        first_row_index = last_row_index+1\n",
    "        id_count +=1\n",
    "    if len(long_stays_id) != len(last_charttime_list):\n",
    "        print('ERROR')\n",
    "    print(\"%d long stays\" % len(long_stays_id))\n",
    "    # drop all the rows with the saved icustay_id\n",
    "    print(\"there are %d id-s shorter than 48 hours\" % len(drop_list))\n",
    "    X = X[~X.icustay_id.isin(drop_list)]\n",
    "    id_list = X['icustay_id'].unique()\n",
    "    X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "    \n",
    "    return id_list, X, long_stays_id,last_charttime_list\n",
    "\n",
    "id_list, X, long_stays_id,last_charttime_list  = more_than_HOURS_ahead(adults_icustay_id_list, X)\n",
    "\n",
    "long = pd.DataFrame()\n",
    "long['icustay_id']  = long_stays_id\n",
    "long['last_time']  = last_charttime_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 long stays\n",
      "there are 1 id-s shorter than 48 hours\n"
     ]
    }
   ],
   "source": [
    "# deleting rows that are not within MAX_DAYS (35) period\n",
    "i = 0 # long df index\n",
    "drop_long_time = []\n",
    "    \n",
    "while i < len(long_stays_id):\n",
    "    j = 0\n",
    "    all_rows = X.index[X['icustay_id'] == long.loc[i,'icustay_id']].tolist()\n",
    "    while j < len(all_rows):\n",
    "        time = X.iat[all_rows[j], X.columns.get_loc('charttime')]\n",
    "        # if keep last MAX_DAYS \n",
    "        if (long.loc[i,'last_time'] - time).total_seconds() > MAX_DAYS*24*60*60:\n",
    "            drop_long_time.append(all_rows[j])\n",
    "            j +=1\n",
    "        else:\n",
    "            break\n",
    "    i +=1       \n",
    "X.drop(X.index[drop_long_time], inplace=True) \n",
    "\n",
    "# checking for 48h min length again\n",
    "id_list, X, long_stays_id,last_charttime_list  = more_than_HOURS_ahead(id_list, X)\n",
    "dataset_detail = dataset_detail[dataset_detail.icustay_id.isin(id_list)].sort_values(by=['icustay_id'], ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SELECTED_FEATURE_SET or MAX_FEATURE_SET:\n",
    "    # AL create a dictionary for hadm\n",
    "    hadm = dataset_detail.filter(['hadm_id','icustay_id'],axis = 1)\n",
    "    dict_hadm = pd.Series(hadm.hadm_id.values,index=hadm.icustay_id).to_dict()\n",
    "    # fill in the missing values (to ensure correct merging of icd below)\n",
    "    X.hadm_id = X.hadm_id.fillna(FILL_VALUE)\n",
    "    # AL change the type to prevent warning of merging int on float\n",
    "    X = X.astype({\"hadm_id\": int})\n",
    "    a = -1\n",
    "    while a < X.shape[0]-1:\n",
    "        a = a+1\n",
    "        if X.iat[a, X.columns.get_loc('hadm_id')] !=-1 :\n",
    "            continue\n",
    "        elif X.iat[a, X.columns.get_loc('hadm_id')]==-1:\n",
    "            X.iat[a, X.columns.get_loc('hadm_id')] = dict_hadm[X.iat[a, X.columns.get_loc('icustay_id')]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For testing purpose, use small amount of data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing purpose, use small amount of data first\n",
    "if TESTING:\n",
    "    rest, id_list = train_test_split(id_list, test_size= TEST_SIZE, random_state=MAN_SEED)\n",
    "    X = X[X.icustay_id.isin(id_list)].sort_values(by=['icustay_id'])\n",
    "    dataset_detail = dataset_detail[dataset_detail.icustay_id.isin(id_list)].sort_values(by=['icustay_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling , imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resampling: MEAN & ZERO\n",
      "Merging sampled features\n",
      "(2156523, 26)\n"
     ]
    }
   ],
   "source": [
    "if (TIME_SAMPLING and MOST_COMMON):\n",
    "    print(\"resampling: MOST_COMMON\")\n",
    "    # Resample the data using assigned interval,mode() for most common\n",
    "    X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL).mode().reset_index()\n",
    "elif TIME_SAMPLING:\n",
    "    print(\"resampling: MEAN & ZERO\")\n",
    "    # Sampling with different strategies per kind of variable\n",
    "    label = ['aki_stage']\n",
    "    skip = ['icustay_id', 'charttime', 'aki_stage']\n",
    "    if SELECTED_FEATURE_SET or MAX_FEATURE_SET:\n",
    "        discrete_feat = ['sedative', 'vasopressor', 'vent', 'hadm_id']\n",
    "        skip.extend(discrete_feat)    \n",
    "    # all features that are not in skip are numeric\n",
    "    numeric_feat = list(X.columns.difference(skip))\n",
    "    \n",
    "    # Applying aggregation to features depending on their type\n",
    "    X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n",
    "    if SELECTED_FEATURE_SET or MAX_FEATURE_SET:\n",
    "        X_discrete = X[discrete_feat].max().fillna(FILL_VALUE).astype(np.int64)\n",
    "    X_numeric = X[numeric_feat].mean() \n",
    "    X_label = X['aki_stage'].max()\n",
    "    print(\"Merging sampled features\")\n",
    "    try:\n",
    "        X = pd.concat([X_numeric, X_discrete,X_label], axis=1).reset_index()\n",
    "    except:\n",
    "        X = pd.concat([X_numeric,X_label], axis=1).reset_index()\n",
    "print(X.shape)\n",
    "#Label forward fill\n",
    "X['aki_stage'] = X['aki_stage'].ffill(limit=RESAMPLE_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation.\n"
     ]
    }
   ],
   "source": [
    "print(\"Imputation.\")\n",
    "# do imputation of label with zero if there are still missing values\n",
    "X['aki_stage'] = X['aki_stage'].fillna(0)\n",
    "# using most common within each icustay_id\n",
    "if IMPUTE_EACH_ID:\n",
    "    # set a new variable so won't change the orginial X\n",
    "    column_name = list(X.columns)\n",
    "    column_name.remove(column_name[0]) \n",
    "    for feature in column_name:\n",
    "        X.loc[X[feature].isnull(), feature] = X.icustay_id.map(fast_mode(X, ['icustay_id'], feature).set_index('icustay_id')[feature])       \n",
    "\n",
    "# imputation based on whole column\n",
    "if IMPUTE_COLUMN:\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy= IMPUTE_METHOD)\n",
    "    cols = list(X.columns)\n",
    "    cols = cols[2:23]\n",
    "    X[cols]=imp.fit_transform(X[cols])  \n",
    "\n",
    "# If no imputation method selected or only impute each id, for the remaining nan impute direclty with FILL_VALUE\n",
    "X = X.fillna(FILL_VALUE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n"
     ]
    }
   ],
   "source": [
    "# more comfortable to review in this order\n",
    "try:\n",
    "    cols = ['icustay_id', 'charttime','aki_stage','hadm_id','aniongap_avg','bicarbonate_avg', 'bun_avg','chloride_avg',\n",
    "            'creat','diasbp_mean', 'glucose_avg', 'heartrate_mean', 'hematocrit_avg','hemoglobin_avg', \n",
    "            'potassium_avg', 'resprate_mean', 'sodium_avg','spo2_mean', 'sysbp_mean', 'uo_rt_12hr', \n",
    "            'uo_rt_24hr', 'uo_rt_6hr','wbc_avg', 'sedative', 'vasopressor', 'vent' ]\n",
    "    X = X[cols]\n",
    "    print(\"success\")\n",
    "except:\n",
    "    try:\n",
    "        cols = ['icustay_id', 'charttime','aki_stage','creat','uo_rt_12hr', 'uo_rt_24hr', 'uo_rt_6hr']\n",
    "        X = X[cols]\n",
    "    except:\n",
    "        print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binarise labels\n"
     ]
    }
   ],
   "source": [
    "print(\"binarise labels\")\n",
    "if ALL_STAGES:\n",
    "    pass\n",
    "elif CLASS1:\n",
    "    X.loc[X['aki_stage'] > 1, 'aki_stage'] = 1\n",
    "elif CLASS2:\n",
    "    X.loc[X['aki_stage'] < 2, 'aki_stage'] = 0\n",
    "    X.loc[X['aki_stage'] > 1, 'aki_stage'] = 1\n",
    "elif CLASS3:\n",
    "    X.loc[X['aki_stage'] < 3, 'aki_stage'] = 0\n",
    "    X.loc[X['aki_stage'] > 2, 'aki_stage'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    1769822\n",
       "1.0     386701\n",
       "Name: aki_stage, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['aki_stage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47751"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['icustay_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2156523, 26)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "work = X.copy(deep = True)\n",
    "#X = work.copy(deep = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHIFTING labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47751"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(\"Shifting the labels 48 h\") # by 8 position : 6h sampling*8=48h and ffil 8 newly empty ones\n",
    "# group by\n",
    "X['aki_stage'] = X.groupby('icustay_id')['aki_stage'].shift(-(HOURS_AHEAD // int(SAMPLING_INTERVAL[:-1])))\n",
    "X = X.dropna(subset=['aki_stage'])\n",
    "X['icustay_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add categorical features (details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocessing not time dependent data\n"
     ]
    }
   ],
   "source": [
    "#no time dependent data\n",
    "print(\"start preprocessing not time dependent data\")\n",
    "if SELECTED_FEATURE_SET or MAX_FEATURE_SET:\n",
    "    #extract datasets based on id_list\n",
    "    dataset_detail = dataset_detail.loc[dataset_detail['icustay_id'].isin(id_list)]\n",
    "    #sort by ascending order\n",
    "    dataset_detail = dataset_detail.sort_values(by=['icustay_id'])\n",
    "    subject_id = dataset_detail[\"subject_id\"].unique()\n",
    "    \n",
    "    #transfrom categorical data to binary form\n",
    "    dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop('gender')))\n",
    "    dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop(\"ethnicity_grouped\")))\n",
    "    dataset_detail = dataset_detail.join(pd.get_dummies(dataset_detail.pop('admission_type')))\n",
    "    dataset_detail = dataset_detail.drop(['subject_id', 'hadm_id'], axis=1)\n",
    "    # AL merge\n",
    "    X =  pd.merge(X, dataset_detail, on = [\"icustay_id\"], how= \"left\", copy = False) \n",
    "    numeric_feat.append('age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Cap features between 0.01 / 0.99 quantile and normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capping between the 0.01 and 0.99 quantile\n",
      "Normalizing in [0,1] with min-max normalization\n"
     ]
    }
   ],
   "source": [
    "if CAPPING:\n",
    "    X = cap_data(X)\n",
    "\n",
    "X = normalise_data(X, numeric_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter for the selected features\n"
     ]
    }
   ],
   "source": [
    "print(\"Filter for the selected features\")\n",
    "if SELECTED_FEATURE_SET:\n",
    "    X = X[selected_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 1452698, 1.0: 321817})\n"
     ]
    }
   ],
   "source": [
    "# approximate weights (just for information, preliminary)\n",
    "counter=collections.Counter(X['aki_stage'])\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    }
   ],
   "source": [
    "X = X.sort_values(by=['icustay_id', 'charttime'])\n",
    "seq_lengths = X.groupby(['icustay_id'],as_index=False).size().sort_values(ascending=False)\n",
    "sequence_length = seq_lengths.max() # the longest sequence per icustay-id\n",
    "print(sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AL re-write as try except to make it work as hadm_id is not used if only one csv file is used and none are merged\n",
    "try:\n",
    "    X.drop(['hadm_id'], axis=1, inplace = True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "icustay_id\n",
      "299999\n",
      "charttime\n",
      "2210-08-22 00:00:00\n",
      "aki_stage\n",
      "1.0\n",
      "aniongap_avg\n",
      "1.0\n",
      "bicarbonate_avg\n",
      "1.0\n",
      "bun_avg\n",
      "1.0\n",
      "chloride_avg\n",
      "1.0\n",
      "creat\n",
      "1.0\n",
      "diasbp_mean\n",
      "1.0\n",
      "glucose_avg\n",
      "1.0\n",
      "heartrate_mean\n",
      "1.0\n",
      "hematocrit_avg\n",
      "1.0\n",
      "hemoglobin_avg\n",
      "1.0\n",
      "potassium_avg\n",
      "1.0\n",
      "resprate_mean\n",
      "1.0\n",
      "sodium_avg\n",
      "1.0\n",
      "spo2_mean\n",
      "1.0\n",
      "sysbp_mean\n",
      "1.0\n",
      "uo_rt_12hr\n",
      "1.0\n",
      "uo_rt_24hr\n",
      "1.0\n",
      "uo_rt_6hr\n",
      "1.0\n",
      "wbc_avg\n",
      "1.0\n",
      "sedative\n",
      "1\n",
      "vasopressor\n",
      "1\n",
      "vent\n",
      "1\n",
      "age\n",
      "1.0\n",
      "F\n",
      "1\n",
      "M\n",
      "1\n",
      "asian\n",
      "1\n",
      "black\n",
      "1\n",
      "hispanic\n",
      "1\n",
      "native\n",
      "0\n",
      "other\n",
      "1\n",
      "unknown\n",
      "1\n",
      "white\n",
      "1\n",
      "ELECTIVE\n",
      "1\n",
      "EMERGENCY\n",
      "1\n",
      "URGENT\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#check\n",
    "for i in list(X.columns):\n",
    "    print(i)\n",
    "    print(X[i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = X.shape[1]-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "divide dataset into train, test and validation sets\n",
      "train is 38200\n",
      "val and test are 4776\n"
     ]
    }
   ],
   "source": [
    "print(\"divide dataset into train, test and validation sets\")\n",
    "id_train, id_test_val = train_test_split(id_list, test_size = SPLIT_SIZE, random_state = MAN_SEED) # train set is 80%)\n",
    "train = X[X.icustay_id.isin(id_train)].sort_values(by=['icustay_id'])\n",
    "print(\"train is %d\" % len(id_train))\n",
    "\n",
    "# remaining 20% split in halves as test and validation 10% and 10%\n",
    "id_valid, id_test = train_test_split(id_test_val, test_size = 0.5, random_state = MAN_SEED) # test 10% valid 10%\n",
    "print(\"val and test are %d\" %len(id_test))\n",
    "test = X[X.icustay_id.isin(id_test)].sort_values(by=['icustay_id'], ignore_index = True) \n",
    "validation = X[X.icustay_id.isin(id_valid)].sort_values(by=['icustay_id']) \n",
    "\n",
    "test = test.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "train = train.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "validation = validation.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9549718558\n"
     ]
    }
   ],
   "source": [
    "suma=0\n",
    "for i in id_train:\n",
    "    suma +=int(i)\n",
    "print (suma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remember 1 label per icu stay for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = test.copy(deep = True)\n",
    "test = test.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "id_test.sort()\n",
    "#last_charttime_list= []\n",
    "\n",
    "index_list = []\n",
    "label_list = []\n",
    "\n",
    "first_row_index = 0\n",
    "id_count = 0\n",
    "seq_length = Z.groupby(['icustay_id'],as_index=False).size().to_frame('size')\n",
    "\n",
    "for ID in id_test:\n",
    "    last_row_index = first_row_index + seq_length.iloc[id_count,0]-1\n",
    "    a = Z.loc[Z['icustay_id']==ID].aki_stage\n",
    "    if 1 not in a.values:\n",
    "        label_list.append(0)\n",
    "        #last_charttime_list.append(Z.iat[last_row_index, Z.columns.get_loc('charttime')]) \n",
    "        index_list.append(last_row_index)\n",
    "    elif 1 in a.values:\n",
    "        label_list.append(1)\n",
    "        row = first_row_index\n",
    "        while row != last_row_index+1:\n",
    "            if Z.iat[row, Z.columns.get_loc('aki_stage')]==0:\n",
    "                row +=1\n",
    "            elif Z.iat[row, Z.columns.get_loc('aki_stage')]==1:\n",
    "                #last_charttime_list.append(Z.iat[row, Z.columns.get_loc('charttime')])\n",
    "                index_list.append(row)\n",
    "                break\n",
    "    first_row_index = last_row_index+1\n",
    "    id_count +=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop(['charttime'], axis=1, inplace = True)\n",
    "train.drop(['charttime'], axis=1, inplace = True)\n",
    "validation.drop(['charttime'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(38200,)\n"
     ]
    }
   ],
   "source": [
    "#print(\"reshape 2D dataframe to 3D Array, group by icustay_id\")\n",
    "train = np.array(sorted(list(train.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)),key=len, reverse=True))\n",
    "test = np.array(list(test.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)))\n",
    "validation = np.array(list(validation.groupby(['icustay_id'],as_index=False).apply(pd.DataFrame.to_numpy)))\n",
    "\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4776,)\n"
     ]
    }
   ],
   "source": [
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4775,)\n"
     ]
    }
   ],
   "source": [
    "print(validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-12 13:45:27.207782\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on CPU\n"
     ]
    }
   ],
   "source": [
    "if (torch.cuda.is_available()):\n",
    "    print('Training on GPU')\n",
    "else:\n",
    "    print('Training on CPU') # On mac book GPU is not possible =() \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0.0: 1163757, 1.0: 257313})\n",
      "torch.Size([635075])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4775, 133, 35])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def batch(data, batch_size):\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    times = math.floor(data.shape[0]/batch_size)\n",
    "    remainder = data.shape[0]%times\n",
    "    a = 0\n",
    "    start = 0\n",
    "    end = start+batch_size\n",
    "    if remainder ==0:\n",
    "        a +=1\n",
    "    while a<times:\n",
    "        temp = pad(data[start:end,],0)\n",
    "        x = torch.from_numpy(temp[:,:,2:]).float() # without icustay_id and without aki_stage columns\n",
    "        y = torch.flatten(torch.from_numpy(temp[:, :,1].reshape(-1,1)).float())\n",
    "        X_batches.append(x)\n",
    "        y_batches.append(y)\n",
    "        start = end\n",
    "        end = start+batch_size\n",
    "        a +=1\n",
    "    temp = pad(data[start:data.shape[0]],0)\n",
    "    x = torch.from_numpy(temp[:,:,2:]).float()\n",
    "    y = torch.flatten(torch.from_numpy(temp[:, :,1].reshape(-1,1)).float()).long()\n",
    "    X_batches.append(x)\n",
    "    y_batches.append(y)\n",
    "    if len(X_batches) != len(y_batches):\n",
    "        print(\"length error\")\n",
    "    return X_batches, y_batches # arrays\n",
    "\n",
    "# batching\n",
    "X_train, y_train = batch(train, batch_size) # to count weights\n",
    "\n",
    "# counting balance of the classes\n",
    "y = []\n",
    "for i in y_train:\n",
    "    for element in i:\n",
    "        y.append(element.item())\n",
    "\n",
    "#  weights\n",
    "counter=collections.Counter(y)\n",
    "print(counter)\n",
    "zeroes = counter[0]\n",
    "ones = counter[1]\n",
    "\n",
    "X_test, y_test = batch(test, test.shape[0]) \n",
    "X_val, y_val = batch(validation, validation.shape[0])\n",
    "X_val = X_val[0]\n",
    "y_val = y_val[0]\n",
    "X_test = X_test[0]\n",
    "y_test = y_test[0]\n",
    "print(y_val.shape)\n",
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6364, 0.0000, 0.7406, 0.0000,\n",
      "        0.0000, 0.0000, 0.7321, 0.0000, 0.9600, 0.8257, 0.2147, 0.1512, 0.1854,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7330, 0.7277, 0.7892, 0.0000,\n",
      "        0.0000, 0.0000, 0.7958, 0.0000, 0.9733, 0.8558, 0.2339, 0.1786, 0.1820,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7538, 0.4946, 0.8738, 0.0000,\n",
      "        0.0000, 0.0000, 0.8117, 0.0000, 0.9867, 0.9201, 0.1724, 0.1839, 0.1709,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7973, 0.5655, 0.8936, 0.0000,\n",
      "        0.0000, 0.0000, 0.8952, 0.0000, 0.9762, 0.8950, 0.2066, 0.2034, 0.1925,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.7143, 0.9714, 1.0000, 0.8362, 0.7895, 0.7145, 0.3764, 0.7638, 0.6077,\n",
      "        0.5929, 0.8148, 0.8594, 0.9595, 0.9789, 0.8829, 0.2495, 0.2204, 0.2721,\n",
      "        0.7935, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6648, 0.6757, 0.7337, 0.0000,\n",
      "        0.0000, 0.0000, 0.8488, 0.0000, 0.9783, 0.8226, 0.2943, 0.2568, 0.2357,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8845, 0.6851, 0.7490, 0.0000,\n",
      "        0.0000, 0.0000, 0.8223, 0.0000, 0.9767, 1.0000, 0.2695, 0.2765, 0.2478,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8087, 0.4119, 0.7822, 0.0000,\n",
      "        0.0000, 0.0000, 0.8467, 0.0000, 0.9467, 1.0000, 0.3227, 0.3217, 0.3436,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.7619, 1.0000, 1.0000, 0.8448, 0.8070, 0.8087, 0.3107, 0.7181, 0.5811,\n",
      "        0.5857, 0.7407, 0.7851, 0.9797, 0.9557, 0.9994, 0.3203, 0.3051, 0.2562,\n",
      "        0.7854, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8598, 0.4643, 0.7642, 0.0000,\n",
      "        0.0000, 0.0000, 0.7056, 0.0000, 0.9517, 1.0000, 0.2857, 0.3155, 0.2532,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.8561, 0.0000, 0.7212, 0.0000,\n",
      "        0.0000, 0.0000, 0.8382, 0.0000, 0.9400, 1.0000, 0.3294, 0.3453, 0.3412,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.8095, 0.9429, 1.0000, 0.8534, 0.7193, 0.0000, 0.4571, 0.0000, 0.6223,\n",
      "        0.6357, 0.6481, 0.0000, 0.9797, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.7328, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.6667, 0.9143, 0.8495, 0.8879, 0.5965, 0.0000, 0.4107, 0.0000, 0.5496,\n",
      "        0.5571, 0.6852, 0.0000, 0.9797, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.7126, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.7143, 0.9714, 0.7419, 0.8707, 0.5789, 0.0000, 0.6143, 0.0000, 0.0000,\n",
      "        0.0000, 0.6481, 0.0000, 0.9865, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.7143, 0.9429, 0.7419, 0.9052, 0.5439, 0.0000, 0.4321, 0.0000, 0.7288,\n",
      "        0.7071, 0.5926, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.7045, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7215,\n",
      "        0.7214, 0.6111, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.6842, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.5238, 0.8857, 0.6237, 0.8707, 0.4386, 0.0000, 0.5214, 0.0000, 0.0000,\n",
      "        0.0000, 0.6111, 0.0000, 0.9459, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.7381, 0.8000, 0.5806, 0.8707, 0.4211, 0.0000, 0.3964, 0.0000, 0.6973,\n",
      "        0.6929, 0.6111, 0.0000, 0.9527, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.7733, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7222, 0.0000, 0.9459, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.6667, 0.8286, 0.4839, 0.8621, 0.3509, 0.0000, 0.3143, 0.0000, 0.6320,\n",
      "        0.6429, 0.5926, 0.0000, 0.9459, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.5587, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.7143, 0.7714, 0.4624, 0.8621, 0.3509, 0.0000, 0.4429, 0.0000, 0.0000,\n",
      "        0.0000, 0.5926, 0.0000, 0.9392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.8571, 0.7714, 0.4194, 0.8362, 0.3158, 0.0000, 0.7750, 0.0000, 0.0000,\n",
      "        0.0000, 0.6111, 0.0000, 0.9392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.7619, 0.7143, 0.3978, 0.8707, 0.2982, 0.0000, 0.5321, 0.0000, 0.5956,\n",
      "        0.6071, 0.7222, 0.0000, 0.9324, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.5919, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.6667, 0.8286, 0.3548, 0.8621, 0.2632, 0.0000, 0.2286, 0.0000, 0.6562,\n",
      "        0.6643, 0.5556, 0.0000, 0.9459, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.5283, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5666,\n",
      "        0.5357, 0.6852, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.4170, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.6190, 0.7143, 0.3118, 0.9224, 0.2281, 0.0000, 0.2964, 0.0000, 0.6707,\n",
      "        0.6643, 0.6481, 0.0000, 0.9527, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.3482, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6731,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6102,\n",
      "        0.0000, 0.6296, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.5714, 0.8000, 0.3118, 0.9052, 0.2281, 0.0000, 0.2393, 0.0000, 0.6973,\n",
      "        0.6929, 0.6111, 0.0000, 0.9595, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.3077, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7143,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.6190, 0.8286, 0.2903, 0.8879, 0.1930, 0.0000, 0.2893, 0.0000, 0.6852,\n",
      "        0.7143, 0.5926, 0.0000, 0.9595, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.3360, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6780,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.6190, 0.8000, 0.2903, 0.8879, 0.1930, 0.0000, 0.2857, 0.0000, 0.6659,\n",
      "        0.6857, 0.6296, 0.0000, 0.9527, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.3158, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.9048, 0.6857, 0.2796, 0.8621, 0.1930, 0.0000, 0.3214, 0.0000, 0.6877,\n",
      "        0.6929, 0.6667, 0.0000, 0.9392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.3522, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6780,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.6667, 0.7714, 0.3441, 0.8793, 0.1930, 0.0000, 0.3179, 0.0000, 0.6828,\n",
      "        0.6929, 0.5741, 0.0000, 0.9459, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.4251, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.4286, 0.8571, 0.3441, 0.9138, 0.1754, 0.0000, 0.2036, 0.0000, 0.6804,\n",
      "        0.6643, 0.5556, 0.0000, 0.9595, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.5951, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.6190, 0.7714, 0.3226, 0.8966, 0.1579, 0.0000, 0.1964, 0.0000, 0.6465,\n",
      "        0.6286, 0.5926, 0.0000, 0.9527, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.6761, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.8704, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.5714, 0.7429, 0.3226, 0.9138, 0.1579, 0.0000, 0.2964, 0.0000, 0.6416,\n",
      "        0.6429, 0.7407, 0.0000, 0.9459, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.7773, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.5714, 0.6857, 0.3441, 0.8966, 0.1404, 0.0000, 0.4607, 0.0000, 0.6344,\n",
      "        0.6357, 0.7037, 0.0000, 0.9189, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.9676, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.6667, 0.6857, 0.4301, 0.9310, 0.1754, 0.0000, 0.2857, 0.0000, 0.6416,\n",
      "        0.6357, 0.8333, 0.0000, 0.9527, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        1.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.7619, 0.6571, 0.3871, 0.8966, 0.1579, 0.0000, 0.2464, 0.0000, 0.6755,\n",
      "        0.6571, 0.7037, 0.0000, 0.9392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        1.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.6667, 0.6857, 0.3548, 0.8966, 0.1404, 0.0000, 0.2357, 0.0000, 0.6150,\n",
      "        0.6000, 0.6296, 0.0000, 0.9392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.8016, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6223,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.5238, 0.6857, 0.2581, 0.9224, 0.1228, 0.0000, 0.2500, 0.0000, 0.6029,\n",
      "        0.5929, 0.6296, 0.0000, 0.9392, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.7490, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.6190, 0.6857, 0.2473, 0.9397, 0.1228, 0.0000, 0.3464, 0.0000, 0.6586,\n",
      "        0.6500, 0.6481, 0.0000, 0.9595, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.6194, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.8095, 0.6000, 0.2473, 0.9310, 0.0175, 0.0000, 0.3643, 0.0000, 0.6901,\n",
      "        0.6643, 0.6852, 0.0000, 0.9595, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.6032, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.7143, 0.6857, 0.2796, 0.9052, 0.1404, 0.0000, 0.3036, 0.0000, 0.7191,\n",
      "        0.6929, 0.6481, 0.0000, 0.9459, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.6478, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6998,\n",
      "        0.6786, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.6640, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6901,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.6190, 0.6571, 0.2151, 0.9224, 0.1053, 0.0000, 0.4179, 0.0000, 0.6465,\n",
      "        0.6357, 0.5556, 0.0000, 0.9459, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.5101, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5981,\n",
      "        0.5857, 0.7778, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.5263, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6368,\n",
      "        0.6286, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.4777, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.5238, 0.7429, 0.1720, 0.9310, 0.1053, 0.0000, 0.2464, 0.0000, 0.6174,\n",
      "        0.6000, 0.5741, 0.0000, 0.9595, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.3887, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.7407, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.6190, 0.7143, 0.2043, 0.9397, 0.1053, 0.0000, 0.3357, 0.0000, 0.6174,\n",
      "        0.5929, 0.6296, 0.0000, 0.9730, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.3522, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.2151, 0.0000, 0.1053, 0.0000, 0.0000, 0.0000, 0.6683,\n",
      "        0.6429, 0.7037, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.3951, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6949,\n",
      "        0.6714, 0.6111, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.3563, 0.0000, 0.0000, 0.0000, 0.0806, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "m = X_train[0]\n",
    "\n",
    "for i in m[0]:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_label (model, label_list,X_test,index_list):\n",
    "    # evaluate on a test set\n",
    "    labels = np.array(label_list)\n",
    "    labels = labels.reshape(-1,1)\n",
    "    labels = labels.astype(int)\n",
    "    logits = nn_model(X_test)\n",
    "    pred = torch.nn.Sigmoid() (logits)\n",
    "    max_rows = pred.shape[1]\n",
    "    predictions = pred.detach().numpy()\n",
    "    predictions = predictions.reshape(-1,1) \n",
    "    # select 1 per icu stay id by index\n",
    "    prob_1_label = []\n",
    "    row = 0\n",
    "    prev = 0\n",
    "    for i in index_list:\n",
    "        prob_1_label.append(predictions[row+i-prev])\n",
    "        row += pred.shape[1]\n",
    "        prev = i\n",
    "    prob_1_label = np.array(prob_1_label).reshape(-1,1)\n",
    "    \n",
    "    return labels, prob_1_label\n",
    "\n",
    "def performance (model, y_test, pred_probabilities):\n",
    "    # performance\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    # compute roc auc\n",
    "    roc_auc = roc_auc_score(y_test, pred_probabilities, average = 'micro')\n",
    "    # compute Precision_Recall curves\n",
    "    precision, recall, _ = precision_recall_curve(y_test, pred_probabilities)\n",
    "    # compute PR_AUC\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "       \n",
    "    # I add confusion matrix\n",
    "    optimal_cut_off = round(thresholds[np.argmax(tpr - fpr)],4)\n",
    "    a = np.where(pred_probabilities > optimal_cut_off, 1, 0)\n",
    "    brier = round(metrics.brier_score_loss(y_test, pred_probabilities, sample_weight=None, pos_label=None),3)\n",
    "    predictions = np.where(pred_probabilities > optimal_cut_off, 1, 0)  \n",
    "    \n",
    "    print (\"Area Under ROC Curve: %0.2f\" % roc_auc  )\n",
    "    print (\"Area Under PR Curve(AP): %0.2f\" % pr_auc  ) \n",
    "    print(\"Brier score : {:.3f}\".format(brier))\n",
    "    print('Accuracy for Classifier : {:.2f}'.format(accuracy_score(y_test, predictions)))\n",
    "    print('Cut off: ' + str(optimal_cut_off))\n",
    "    matrix = metrics.confusion_matrix(y_test, a, labels=None, normalize=None)\n",
    "    print(str(matrix))\n",
    "    \n",
    "    f.write(\"\\n Area Under ROC Curve: \" +str(roc_auc))\n",
    "    f.write(\"\\n Area Under PR Curve(AP): \" + str(pr_auc))\n",
    "    f.write(\"\\n Brier score: \" +str(brier))\n",
    "    f.write('\\n Accuracy for Classifier '+str(round((accuracy_score(labels, predictions)),3)))\n",
    "    f.write(\"\\n Cut off: \" +str(optimal_cut_off))\n",
    "    f.write(str(matrix))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypercount: 1\n",
      "\n",
      "\n",
      "i-Onedir_1_lr_0.001_nodrop\n",
      "Net(\n",
      "  (fc1): Linear(in_features=35, out_features=35, bias=True)\n",
      "  (fc2): LSTM(35, 1, batch_first=True)\n",
      "  (combination_layer): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (projection): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (dropout_layer): Dropout(p=0, inplace=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      " Epoch [1] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [2] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [3] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [4] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [5] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [6] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [7] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [8] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [9] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [10] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [11] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "out of patience\n",
      "\n",
      " Finished Training\n",
      "starttime = 2021-01-12 13:45:38.926833\n",
      "endtime = 2021-01-12 13:55:25.606507\n",
      "\n",
      " Last model \n",
      "\n",
      "Area Under ROC Curve: 0.50\n",
      "Area Under PR Curve(AP): 0.79\n",
      "Brier score : 0.252\n",
      "Accuracy for Classifier : 0.42\n",
      "Cut off: 1.4878\n",
      "[[1999    0]\n",
      " [2777    0]]\n",
      "\n",
      " Best model \n",
      "\n",
      "Area Under ROC Curve: 0.50\n",
      "Area Under PR Curve(AP): 0.79\n",
      "Brier score : 0.252\n",
      "Accuracy for Classifier : 0.42\n",
      "Cut off: 1.4878\n",
      "[[1999    0]\n",
      " [2777    0]]\n",
      "hypercount: 2\n",
      "\n",
      "\n",
      "i-Onedir_1_lr_0.0001_nodrop\n",
      "Net(\n",
      "  (fc1): Linear(in_features=35, out_features=35, bias=True)\n",
      "  (fc2): LSTM(35, 1, batch_first=True)\n",
      "  (combination_layer): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (projection): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (dropout_layer): Dropout(p=0, inplace=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      " Epoch [1] out of 80\n",
      "Training loss: 1.237.. Validation loss: 0.782.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [2] out of 80\n",
      "Training loss: 1.206.. Validation loss: 0.833.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [3] out of 80\n",
      "Training loss: 1.199.. Validation loss: 0.857.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [4] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.867.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [5] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.871.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [6] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.873.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [7] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.873.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [8] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.873.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [9] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.873.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [10] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.873.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [11] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.873.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "out of patience\n",
      "\n",
      " Finished Training\n",
      "starttime = 2021-01-12 13:55:26.095950\n",
      "endtime = 2021-01-12 14:05:13.326962\n",
      "\n",
      " Last model \n",
      "\n",
      "Area Under ROC Curve: 0.50\n",
      "Area Under PR Curve(AP): 0.79\n",
      "Brier score : 0.246\n",
      "Accuracy for Classifier : 0.42\n",
      "Cut off: 1.5258\n",
      "[[1999    0]\n",
      " [2777    0]]\n",
      "\n",
      " Best model \n",
      "\n",
      "Area Under ROC Curve: 0.50\n",
      "Area Under PR Curve(AP): 0.79\n",
      "Brier score : 0.246\n",
      "Accuracy for Classifier : 0.42\n",
      "Cut off: 1.5258\n",
      "[[1999    0]\n",
      " [2777    0]]\n",
      "hypercount: 3\n",
      "\n",
      "\n",
      "i-Onedir_1_lr_0.001_drop0.2\n",
      "Net(\n",
      "  (fc1): Linear(in_features=35, out_features=35, bias=True)\n",
      "  (fc2): LSTM(35, 1, batch_first=True)\n",
      "  (combination_layer): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (projection): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      " Epoch [1] out of 80\n",
      "Training loss: 1.197.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [2] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [3] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [4] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [5] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [6] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [7] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [8] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [9] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [10] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [11] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "out of patience\n",
      "\n",
      " Finished Training\n",
      "starttime = 2021-01-12 14:05:13.799737\n",
      "endtime = 2021-01-12 14:14:41.464663\n",
      "\n",
      " Last model \n",
      "\n",
      "Area Under ROC Curve: 0.50\n",
      "Area Under PR Curve(AP): 0.79\n",
      "Brier score : 0.252\n",
      "Accuracy for Classifier : 0.42\n",
      "Cut off: 1.4878\n",
      "[[1999    0]\n",
      " [2777    0]]\n",
      "\n",
      " Best model \n",
      "\n",
      "Area Under ROC Curve: 0.50\n",
      "Area Under PR Curve(AP): 0.79\n",
      "Brier score : 0.252\n",
      "Accuracy for Classifier : 0.42\n",
      "Cut off: 1.4878\n",
      "[[1999    0]\n",
      " [2777    0]]\n",
      "hypercount: 4\n",
      "\n",
      "\n",
      "i-Onedir_1_lr_0.0001_drop0.2\n",
      "Net(\n",
      "  (fc1): Linear(in_features=35, out_features=35, bias=True)\n",
      "  (fc2): LSTM(35, 1, batch_first=True)\n",
      "  (combination_layer): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (projection): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      " Epoch [1] out of 80\n",
      "Training loss: 1.176.. Validation loss: 0.765.. \n",
      "AUC: 0.65 PR AUC: 0.34 \n",
      "\n",
      " Epoch [2] out of 80\n",
      "Training loss: 1.154.. Validation loss: 0.732.. \n",
      "AUC: 0.70 PR AUC: 0.37 \n",
      "\n",
      " Epoch [3] out of 80\n",
      "Training loss: 1.145.. Validation loss: 0.717.. \n",
      "AUC: 0.70 PR AUC: 0.37 \n",
      "\n",
      " Epoch [4] out of 80\n",
      "Training loss: 1.139.. Validation loss: 0.705.. \n",
      "AUC: 0.72 PR AUC: 0.38 \n",
      "\n",
      " Epoch [5] out of 80\n",
      "Training loss: 1.136.. Validation loss: 0.698.. \n",
      "AUC: 0.74 PR AUC: 0.39 \n",
      "\n",
      " Epoch [6] out of 80\n",
      "Training loss: 1.133.. Validation loss: 0.691.. \n",
      "AUC: 0.77 PR AUC: 0.40 \n",
      "\n",
      " Epoch [7] out of 80\n",
      "Training loss: 1.132.. Validation loss: 0.682.. \n",
      "AUC: 0.77 PR AUC: 0.40 \n",
      "\n",
      " Epoch [8] out of 80\n",
      "Training loss: 1.131.. Validation loss: 0.678.. \n",
      "AUC: 0.81 PR AUC: 0.41 \n",
      "\n",
      " Epoch [9] out of 80\n",
      "Training loss: 1.130.. Validation loss: 0.673.. \n",
      "AUC: 0.81 PR AUC: 0.41 \n",
      "\n",
      " Epoch [10] out of 80\n",
      "Training loss: 1.129.. Validation loss: 0.665.. \n",
      "AUC: 0.82 PR AUC: 0.42 \n",
      "\n",
      " Epoch [11] out of 80\n",
      "Training loss: 1.128.. Validation loss: 0.660.. \n",
      "AUC: 0.84 PR AUC: 0.42 \n",
      "\n",
      " Epoch [12] out of 80\n",
      "Training loss: 1.127.. Validation loss: 0.654.. \n",
      "AUC: 0.83 PR AUC: 0.42 \n",
      "\n",
      " Epoch [13] out of 80\n",
      "Training loss: 1.126.. Validation loss: 0.650.. \n",
      "AUC: 0.84 PR AUC: 0.41 \n",
      "\n",
      " Epoch [14] out of 80\n",
      "Training loss: 1.125.. Validation loss: 0.645.. \n",
      "AUC: 0.83 PR AUC: 0.40 \n",
      "\n",
      " Epoch [15] out of 80\n",
      "Training loss: 1.126.. Validation loss: 0.641.. \n",
      "AUC: 0.85 PR AUC: 0.42 \n",
      "\n",
      " Epoch [16] out of 80\n",
      "Training loss: 1.125.. Validation loss: 0.633.. \n",
      "AUC: 0.84 PR AUC: 0.41 \n",
      "\n",
      " Epoch [17] out of 80\n",
      "Training loss: 1.124.. Validation loss: 0.630.. \n",
      "AUC: 0.83 PR AUC: 0.40 \n",
      "\n",
      " Epoch [18] out of 80\n",
      "Training loss: 1.124.. Validation loss: 0.624.. \n",
      "AUC: 0.85 PR AUC: 0.42 \n",
      "\n",
      " Epoch [19] out of 80\n",
      "Training loss: 1.122.. Validation loss: 0.621.. \n",
      "AUC: 0.85 PR AUC: 0.42 \n",
      "\n",
      " Epoch [20] out of 80\n",
      "Training loss: 1.123.. Validation loss: 0.623.. \n",
      "AUC: 0.87 PR AUC: 0.43 \n",
      "\n",
      " Epoch [21] out of 80\n",
      "Training loss: 1.123.. Validation loss: 0.619.. \n",
      "AUC: 0.86 PR AUC: 0.42 \n",
      "\n",
      " Epoch [22] out of 80\n",
      "Training loss: 1.121.. Validation loss: 0.612.. \n",
      "AUC: 0.84 PR AUC: 0.41 \n",
      "\n",
      " Epoch [23] out of 80\n",
      "Training loss: 1.122.. Validation loss: 0.613.. \n",
      "AUC: 0.86 PR AUC: 0.42 \n",
      "\n",
      " Epoch [24] out of 80\n",
      "Training loss: 1.120.. Validation loss: 0.605.. \n",
      "AUC: 0.86 PR AUC: 0.42 \n",
      "\n",
      " Epoch [25] out of 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.120.. Validation loss: 0.606.. \n",
      "AUC: 0.86 PR AUC: 0.42 \n",
      "\n",
      " Epoch [26] out of 80\n",
      "Training loss: 1.119.. Validation loss: 0.603.. \n",
      "AUC: 0.86 PR AUC: 0.42 \n",
      "\n",
      " Epoch [27] out of 80\n",
      "Training loss: 1.120.. Validation loss: 0.601.. \n",
      "AUC: 0.86 PR AUC: 0.42 \n",
      "\n",
      " Epoch [28] out of 80\n",
      "Training loss: 1.119.. Validation loss: 0.603.. \n",
      "AUC: 0.87 PR AUC: 0.43 \n",
      "\n",
      " Epoch [29] out of 80\n",
      "Training loss: 1.119.. Validation loss: 0.602.. \n",
      "AUC: 0.88 PR AUC: 0.44 \n",
      "\n",
      " Epoch [30] out of 80\n",
      "Training loss: 1.117.. Validation loss: 0.598.. \n",
      "AUC: 0.86 PR AUC: 0.43 \n",
      "\n",
      " Epoch [31] out of 80\n",
      "Training loss: 1.118.. Validation loss: 0.602.. \n",
      "AUC: 0.87 PR AUC: 0.43 \n",
      "\n",
      " Epoch [32] out of 80\n",
      "Training loss: 1.118.. Validation loss: 0.597.. \n",
      "AUC: 0.85 PR AUC: 0.42 \n",
      "\n",
      " Epoch [33] out of 80\n",
      "Training loss: 1.118.. Validation loss: 0.596.. \n",
      "AUC: 0.86 PR AUC: 0.42 \n",
      "\n",
      " Epoch [34] out of 80\n",
      "Training loss: 1.117.. Validation loss: 0.599.. \n",
      "AUC: 0.89 PR AUC: 0.45 \n",
      "\n",
      " Epoch [35] out of 80\n",
      "Training loss: 1.116.. Validation loss: 0.594.. \n",
      "AUC: 0.86 PR AUC: 0.42 \n",
      "\n",
      " Epoch [36] out of 80\n",
      "Training loss: 1.116.. Validation loss: 0.593.. \n",
      "AUC: 0.86 PR AUC: 0.42 \n",
      "\n",
      " Epoch [37] out of 80\n",
      "Training loss: 1.117.. Validation loss: 0.597.. \n",
      "AUC: 0.89 PR AUC: 0.45 \n",
      "\n",
      " Epoch [38] out of 80\n",
      "Training loss: 1.116.. Validation loss: 0.590.. \n",
      "AUC: 0.85 PR AUC: 0.42 \n",
      "\n",
      " Epoch [39] out of 80\n",
      "Training loss: 1.115.. Validation loss: 0.589.. \n",
      "AUC: 0.87 PR AUC: 0.44 \n",
      "\n",
      " Epoch [40] out of 80\n",
      "Training loss: 1.115.. Validation loss: 0.590.. \n",
      "AUC: 0.86 PR AUC: 0.43 \n",
      "\n",
      " Epoch [41] out of 80\n",
      "Training loss: 1.116.. Validation loss: 0.592.. \n",
      "AUC: 0.87 PR AUC: 0.43 \n",
      "\n",
      " Epoch [42] out of 80\n",
      "Training loss: 1.116.. Validation loss: 0.588.. \n",
      "AUC: 0.86 PR AUC: 0.43 \n",
      "\n",
      " Epoch [43] out of 80\n",
      "Training loss: 1.115.. Validation loss: 0.592.. \n",
      "AUC: 0.87 PR AUC: 0.43 \n",
      "\n",
      " Epoch [44] out of 80\n",
      "Training loss: 1.116.. Validation loss: 0.598.. \n",
      "AUC: 0.89 PR AUC: 0.45 \n",
      "\n",
      " Epoch [45] out of 80\n",
      "Training loss: 1.115.. Validation loss: 0.597.. \n",
      "AUC: 0.87 PR AUC: 0.44 \n",
      "\n",
      " Epoch [46] out of 80\n",
      "Training loss: 1.116.. Validation loss: 0.594.. \n",
      "AUC: 0.85 PR AUC: 0.42 \n",
      "\n",
      " Epoch [47] out of 80\n",
      "Training loss: 1.116.. Validation loss: 0.598.. \n",
      "AUC: 0.87 PR AUC: 0.44 \n",
      "\n",
      " Epoch [48] out of 80\n",
      "Training loss: 1.115.. Validation loss: 0.597.. \n",
      "AUC: 0.86 PR AUC: 0.43 \n",
      "\n",
      " Epoch [49] out of 80\n",
      "Training loss: 1.114.. Validation loss: 0.597.. \n",
      "AUC: 0.86 PR AUC: 0.43 \n",
      "\n",
      " Epoch [50] out of 80\n",
      "Training loss: 1.114.. Validation loss: 0.603.. \n",
      "AUC: 0.86 PR AUC: 0.44 \n",
      "\n",
      " Epoch [51] out of 80\n",
      "Training loss: 1.115.. Validation loss: 0.605.. \n",
      "AUC: 0.86 PR AUC: 0.44 \n",
      "\n",
      " Epoch [52] out of 80\n",
      "Training loss: 1.114.. Validation loss: 0.607.. \n",
      "AUC: 0.86 PR AUC: 0.44 \n",
      "\n",
      " Epoch [53] out of 80\n",
      "Training loss: 1.114.. Validation loss: 0.609.. \n",
      "AUC: 0.85 PR AUC: 0.43 \n",
      "\n",
      " Epoch [54] out of 80\n",
      "Training loss: 1.113.. Validation loss: 0.608.. \n",
      "AUC: 0.85 PR AUC: 0.44 \n",
      "\n",
      " Epoch [55] out of 80\n",
      "Training loss: 1.113.. Validation loss: 0.615.. \n",
      "AUC: 0.86 PR AUC: 0.44 \n",
      "\n",
      " Epoch [56] out of 80\n",
      "Training loss: 1.113.. Validation loss: 0.614.. \n",
      "AUC: 0.87 PR AUC: 0.45 \n",
      "\n",
      " Epoch [57] out of 80\n",
      "Training loss: 1.113.. Validation loss: 0.613.. \n",
      "AUC: 0.85 PR AUC: 0.44 \n",
      "\n",
      " Epoch [58] out of 80\n",
      "Training loss: 1.113.. Validation loss: 0.616.. \n",
      "AUC: 0.84 PR AUC: 0.43 \n",
      "\n",
      " Epoch [59] out of 80\n",
      "Training loss: 1.113.. Validation loss: 0.617.. \n",
      "AUC: 0.84 PR AUC: 0.44 \n",
      "\n",
      " Epoch [60] out of 80\n",
      "Training loss: 1.113.. Validation loss: 0.625.. \n",
      "AUC: 0.85 PR AUC: 0.44 \n",
      "\n",
      " Epoch [61] out of 80\n",
      "Training loss: 1.113.. Validation loss: 0.622.. \n",
      "AUC: 0.85 PR AUC: 0.44 \n",
      "\n",
      " Epoch [62] out of 80\n",
      "Training loss: 1.113.. Validation loss: 0.626.. \n",
      "AUC: 0.85 PR AUC: 0.44 \n",
      "\n",
      " Epoch [63] out of 80\n",
      "Training loss: 1.112.. Validation loss: 0.627.. \n",
      "AUC: 0.84 PR AUC: 0.43 \n",
      "\n",
      " Epoch [64] out of 80\n",
      "Training loss: 1.113.. Validation loss: 0.627.. \n",
      "AUC: 0.79 PR AUC: 0.41 \n",
      "\n",
      " Epoch [65] out of 80\n",
      "Training loss: 1.112.. Validation loss: 0.627.. \n",
      "AUC: 0.82 PR AUC: 0.43 \n",
      "\n",
      " Epoch [66] out of 80\n",
      "Training loss: 1.112.. Validation loss: 0.635.. \n",
      "AUC: 0.85 PR AUC: 0.44 \n",
      "\n",
      " Epoch [67] out of 80\n",
      "Training loss: 1.112.. Validation loss: 0.632.. \n",
      "AUC: 0.83 PR AUC: 0.43 \n",
      "\n",
      " Epoch [68] out of 80\n",
      "Training loss: 1.112.. Validation loss: 0.635.. \n",
      "AUC: 0.82 PR AUC: 0.42 \n",
      "\n",
      " Epoch [69] out of 80\n",
      "Training loss: 1.113.. Validation loss: 0.637.. \n",
      "AUC: 0.80 PR AUC: 0.42 \n",
      "\n",
      " Epoch [70] out of 80\n",
      "Training loss: 1.112.. Validation loss: 0.645.. \n",
      "AUC: 0.83 PR AUC: 0.43 \n",
      "\n",
      " Epoch [71] out of 80\n",
      "Training loss: 1.111.. Validation loss: 0.644.. \n",
      "AUC: 0.83 PR AUC: 0.43 \n",
      "\n",
      " Epoch [72] out of 80\n",
      "Training loss: 1.111.. Validation loss: 0.648.. \n",
      "AUC: 0.82 PR AUC: 0.43 \n",
      "\n",
      " Epoch [73] out of 80\n",
      "Training loss: 1.112.. Validation loss: 0.651.. \n",
      "AUC: 0.82 PR AUC: 0.43 \n",
      "\n",
      " Epoch [74] out of 80\n",
      "Training loss: 1.112.. Validation loss: 0.651.. \n",
      "AUC: 0.81 PR AUC: 0.43 \n",
      "\n",
      " Epoch [75] out of 80\n",
      "Training loss: 1.111.. Validation loss: 0.657.. \n",
      "AUC: 0.80 PR AUC: 0.42 \n",
      "\n",
      " Epoch [76] out of 80\n",
      "Training loss: 1.111.. Validation loss: 0.655.. \n",
      "AUC: 0.79 PR AUC: 0.42 \n",
      "\n",
      " Epoch [77] out of 80\n",
      "Training loss: 1.111.. Validation loss: 0.659.. \n",
      "AUC: 0.77 PR AUC: 0.41 \n",
      "\n",
      " Epoch [78] out of 80\n",
      "Training loss: 1.111.. Validation loss: 0.662.. \n",
      "AUC: 0.79 PR AUC: 0.42 \n",
      "\n",
      " Epoch [79] out of 80\n",
      "Training loss: 1.112.. Validation loss: 0.668.. \n",
      "AUC: 0.80 PR AUC: 0.42 \n",
      "\n",
      " Epoch [80] out of 80\n",
      "Training loss: 1.111.. Validation loss: 0.666.. \n",
      "AUC: 0.78 PR AUC: 0.42 \n",
      "\n",
      " Finished Training\n",
      "starttime = 2021-01-12 14:14:41.942074\n",
      "endtime = 2021-01-12 15:27:12.925188\n",
      "\n",
      " Last model \n",
      "\n",
      "Area Under ROC Curve: 0.64\n",
      "Area Under PR Curve(AP): 0.73\n",
      "Brier score : 0.254\n",
      "Accuracy for Classifier : 0.65\n",
      "Cut off: 0.3319\n",
      "[[1281  718]\n",
      " [ 942 1835]]\n",
      "\n",
      " Best model \n",
      "\n",
      "Area Under ROC Curve: 0.64\n",
      "Area Under PR Curve(AP): 0.73\n",
      "Brier score : 0.253\n",
      "Accuracy for Classifier : 0.66\n",
      "Cut off: 0.3319\n",
      "[[1297  702]\n",
      " [ 943 1834]]\n",
      "hypercount: 5\n",
      "\n",
      "\n",
      "i-Onedir_2_lr_0.001_nodrop\n",
      "Net(\n",
      "  (fc1): Linear(in_features=35, out_features=35, bias=True)\n",
      "  (fc2): LSTM(35, 1, num_layers=2, batch_first=True)\n",
      "  (combination_layer): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (projection): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (dropout_layer): Dropout(p=0, inplace=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      " Epoch [1] out of 80\n",
      "Training loss: 1.205.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [2] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [3] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [4] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [5] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [6] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [7] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [8] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [9] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [10] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [11] out of 80\n",
      "Training loss: 1.195.. Validation loss: 0.820.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "out of patience\n",
      "\n",
      " Finished Training\n",
      "starttime = 2021-01-12 15:27:13.479257\n",
      "endtime = 2021-01-12 15:45:26.289242\n",
      "\n",
      " Last model \n",
      "\n",
      "Area Under ROC Curve: 0.50\n",
      "Area Under PR Curve(AP): 0.79\n",
      "Brier score : 0.252\n",
      "Accuracy for Classifier : 0.42\n",
      "Cut off: 1.4878\n",
      "[[1999    0]\n",
      " [2777    0]]\n",
      "\n",
      " Best model \n",
      "\n",
      "Area Under ROC Curve: 0.50\n",
      "Area Under PR Curve(AP): 0.79\n",
      "Brier score : 0.252\n",
      "Accuracy for Classifier : 0.42\n",
      "Cut off: 1.4878\n",
      "[[1999    0]\n",
      " [2777    0]]\n",
      "hypercount: 6\n",
      "\n",
      "\n",
      "i-Onedir_2_lr_0.0001_nodrop\n",
      "Net(\n",
      "  (fc1): Linear(in_features=35, out_features=35, bias=True)\n",
      "  (fc2): LSTM(35, 1, num_layers=2, batch_first=True)\n",
      "  (combination_layer): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (projection): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (dropout_layer): Dropout(p=0, inplace=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      " Epoch [1] out of 80\n",
      "Training loss: 1.179.. Validation loss: 0.845.. \n",
      "AUC: 0.68 PR AUC: 0.22 \n",
      "\n",
      " Epoch [2] out of 80\n",
      "Training loss: 1.140.. Validation loss: 0.793.. \n",
      "AUC: 0.71 PR AUC: 0.24 \n",
      "\n",
      " Epoch [3] out of 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.120.. Validation loss: 0.752.. \n",
      "AUC: 0.73 PR AUC: 0.27 \n",
      "\n",
      " Epoch [4] out of 80\n",
      "Training loss: 1.108.. Validation loss: 0.721.. \n",
      "AUC: 0.75 PR AUC: 0.28 \n",
      "\n",
      " Epoch [5] out of 80\n",
      "Training loss: 1.100.. Validation loss: 0.698.. \n",
      "AUC: 0.77 PR AUC: 0.28 \n",
      "\n",
      " Epoch [6] out of 80\n",
      "Training loss: 1.094.. Validation loss: 0.690.. \n",
      "AUC: 0.65 PR AUC: 0.26 \n",
      "\n",
      " Epoch [7] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.688.. \n",
      "AUC: 0.66 PR AUC: 0.26 \n",
      "\n",
      " Epoch [8] out of 80\n",
      "Training loss: 1.086.. Validation loss: 0.690.. \n",
      "AUC: 0.66 PR AUC: 0.26 \n",
      "\n",
      " Epoch [9] out of 80\n",
      "Training loss: 1.083.. Validation loss: 0.694.. \n",
      "AUC: 0.66 PR AUC: 0.26 \n",
      "\n",
      " Epoch [10] out of 80\n",
      "Training loss: 1.080.. Validation loss: 0.702.. \n",
      "AUC: 0.67 PR AUC: 0.26 \n",
      "\n",
      " Epoch [11] out of 80\n",
      "Training loss: 1.077.. Validation loss: 0.711.. \n",
      "AUC: 0.67 PR AUC: 0.26 \n",
      "\n",
      " Epoch [12] out of 80\n",
      "Training loss: 1.075.. Validation loss: 0.720.. \n",
      "AUC: 0.67 PR AUC: 0.27 \n",
      "\n",
      " Epoch [13] out of 80\n",
      "Training loss: 1.072.. Validation loss: 0.732.. \n",
      "AUC: 0.68 PR AUC: 0.27 \n",
      "\n",
      " Epoch [14] out of 80\n",
      "Training loss: 1.070.. Validation loss: 0.740.. \n",
      "AUC: 0.68 PR AUC: 0.27 \n",
      "\n",
      " Epoch [15] out of 80\n",
      "Training loss: 1.068.. Validation loss: 0.745.. \n",
      "AUC: 0.68 PR AUC: 0.28 \n",
      "\n",
      " Epoch [16] out of 80\n",
      "Training loss: 1.067.. Validation loss: 0.749.. \n",
      "AUC: 0.68 PR AUC: 0.28 \n",
      "\n",
      " Epoch [17] out of 80\n",
      "Training loss: 1.065.. Validation loss: 0.757.. \n",
      "AUC: 0.68 PR AUC: 0.28 \n",
      "\n",
      " Epoch [18] out of 80\n",
      "Training loss: 1.063.. Validation loss: 0.760.. \n",
      "AUC: 0.69 PR AUC: 0.29 \n",
      "\n",
      " Epoch [19] out of 80\n",
      "Training loss: 1.062.. Validation loss: 0.767.. \n",
      "AUC: 0.68 PR AUC: 0.29 \n",
      "\n",
      " Epoch [20] out of 80\n",
      "Training loss: 1.061.. Validation loss: 0.771.. \n",
      "AUC: 0.68 PR AUC: 0.29 \n",
      "\n",
      " Epoch [21] out of 80\n",
      "Training loss: 1.060.. Validation loss: 0.771.. \n",
      "AUC: 0.69 PR AUC: 0.29 \n",
      "\n",
      " Epoch [22] out of 80\n",
      "Training loss: 1.059.. Validation loss: 0.778.. \n",
      "AUC: 0.68 PR AUC: 0.29 \n",
      "\n",
      " Epoch [23] out of 80\n",
      "Training loss: 1.058.. Validation loss: 0.783.. \n",
      "AUC: 0.68 PR AUC: 0.29 \n",
      "\n",
      " Epoch [24] out of 80\n",
      "Training loss: 1.057.. Validation loss: 0.787.. \n",
      "AUC: 0.68 PR AUC: 0.30 \n",
      "\n",
      " Epoch [25] out of 80\n",
      "Training loss: 1.056.. Validation loss: 0.794.. \n",
      "AUC: 0.68 PR AUC: 0.30 \n",
      "\n",
      " Epoch [26] out of 80\n",
      "Training loss: 1.055.. Validation loss: 0.800.. \n",
      "AUC: 0.67 PR AUC: 0.30 \n",
      "\n",
      " Epoch [27] out of 80\n",
      "Training loss: 1.054.. Validation loss: 0.804.. \n",
      "AUC: 0.67 PR AUC: 0.30 \n",
      "\n",
      " Epoch [28] out of 80\n",
      "Training loss: 1.053.. Validation loss: 0.810.. \n",
      "AUC: 0.67 PR AUC: 0.30 \n",
      "\n",
      " Epoch [29] out of 80\n",
      "Training loss: 1.053.. Validation loss: 0.813.. \n",
      "AUC: 0.67 PR AUC: 0.30 \n",
      "\n",
      " Epoch [30] out of 80\n",
      "Training loss: 1.052.. Validation loss: 0.817.. \n",
      "AUC: 0.66 PR AUC: 0.30 \n",
      "\n",
      " Epoch [31] out of 80\n",
      "Training loss: 1.051.. Validation loss: 0.821.. \n",
      "AUC: 0.66 PR AUC: 0.30 \n",
      "\n",
      " Epoch [32] out of 80\n",
      "Training loss: 1.051.. Validation loss: 0.825.. \n",
      "AUC: 0.66 PR AUC: 0.30 \n",
      "\n",
      " Epoch [33] out of 80\n",
      "Training loss: 1.050.. Validation loss: 0.829.. \n",
      "AUC: 0.65 PR AUC: 0.31 \n",
      "\n",
      " Epoch [34] out of 80\n",
      "Training loss: 1.050.. Validation loss: 0.834.. \n",
      "AUC: 0.65 PR AUC: 0.31 \n",
      "\n",
      " Epoch [35] out of 80\n",
      "Training loss: 1.050.. Validation loss: 0.837.. \n",
      "AUC: 0.65 PR AUC: 0.31 \n",
      "\n",
      " Epoch [36] out of 80\n",
      "Training loss: 1.049.. Validation loss: 0.845.. \n",
      "AUC: 0.64 PR AUC: 0.31 \n",
      "\n",
      " Epoch [37] out of 80\n",
      "Training loss: 1.049.. Validation loss: 0.850.. \n",
      "AUC: 0.64 PR AUC: 0.31 \n",
      "\n",
      " Epoch [38] out of 80\n",
      "Training loss: 1.048.. Validation loss: 0.854.. \n",
      "AUC: 0.64 PR AUC: 0.31 \n",
      "\n",
      " Epoch [39] out of 80\n",
      "Training loss: 1.048.. Validation loss: 0.861.. \n",
      "AUC: 0.63 PR AUC: 0.31 \n",
      "\n",
      " Epoch [40] out of 80\n",
      "Training loss: 1.047.. Validation loss: 0.869.. \n",
      "AUC: 0.63 PR AUC: 0.30 \n",
      "\n",
      " Epoch [41] out of 80\n",
      "Training loss: 1.047.. Validation loss: 0.880.. \n",
      "AUC: 0.62 PR AUC: 0.30 \n",
      "\n",
      " Epoch [42] out of 80\n",
      "Training loss: 1.047.. Validation loss: 0.892.. \n",
      "AUC: 0.61 PR AUC: 0.30 \n",
      "\n",
      " Epoch [43] out of 80\n",
      "Training loss: 1.046.. Validation loss: 0.901.. \n",
      "AUC: 0.61 PR AUC: 0.30 \n",
      "\n",
      " Epoch [44] out of 80\n",
      "Training loss: 1.046.. Validation loss: 0.912.. \n",
      "AUC: 0.60 PR AUC: 0.30 \n",
      "\n",
      " Epoch [45] out of 80\n",
      "Training loss: 1.045.. Validation loss: 0.919.. \n",
      "AUC: 0.60 PR AUC: 0.30 \n",
      "\n",
      " Epoch [46] out of 80\n",
      "Training loss: 1.045.. Validation loss: 0.929.. \n",
      "AUC: 0.59 PR AUC: 0.30 \n",
      "\n",
      " Epoch [47] out of 80\n",
      "Training loss: 1.045.. Validation loss: 0.940.. \n",
      "AUC: 0.59 PR AUC: 0.30 \n",
      "\n",
      " Epoch [48] out of 80\n",
      "Training loss: 1.044.. Validation loss: 0.945.. \n",
      "AUC: 0.59 PR AUC: 0.30 \n",
      "\n",
      " Epoch [49] out of 80\n",
      "Training loss: 1.044.. Validation loss: 0.953.. \n",
      "AUC: 0.58 PR AUC: 0.29 \n",
      "\n",
      " Epoch [50] out of 80\n",
      "Training loss: 1.044.. Validation loss: 0.960.. \n",
      "AUC: 0.58 PR AUC: 0.29 \n",
      "\n",
      " Epoch [51] out of 80\n",
      "Training loss: 1.043.. Validation loss: 0.972.. \n",
      "AUC: 0.57 PR AUC: 0.29 \n",
      "\n",
      " Epoch [52] out of 80\n",
      "Training loss: 1.043.. Validation loss: 0.981.. \n",
      "AUC: 0.57 PR AUC: 0.29 \n",
      "\n",
      " Epoch [53] out of 80\n",
      "Training loss: 1.043.. Validation loss: 0.992.. \n",
      "AUC: 0.56 PR AUC: 0.29 \n",
      "\n",
      " Epoch [54] out of 80\n",
      "Training loss: 1.042.. Validation loss: 1.004.. \n",
      "AUC: 0.56 PR AUC: 0.29 \n",
      "\n",
      " Epoch [55] out of 80\n",
      "Training loss: 1.042.. Validation loss: 1.017.. \n",
      "AUC: 0.55 PR AUC: 0.28 \n",
      "\n",
      " Epoch [56] out of 80\n",
      "Training loss: 1.042.. Validation loss: 1.026.. \n",
      "AUC: 0.55 PR AUC: 0.28 \n",
      "\n",
      " Epoch [57] out of 80\n",
      "Training loss: 1.042.. Validation loss: 1.032.. \n",
      "AUC: 0.54 PR AUC: 0.28 \n",
      "\n",
      " Epoch [58] out of 80\n",
      "Training loss: 1.041.. Validation loss: 1.042.. \n",
      "AUC: 0.54 PR AUC: 0.28 \n",
      "\n",
      " Epoch [59] out of 80\n",
      "Training loss: 1.041.. Validation loss: 1.053.. \n",
      "AUC: 0.54 PR AUC: 0.28 \n",
      "\n",
      " Epoch [60] out of 80\n",
      "Training loss: 1.041.. Validation loss: 1.059.. \n",
      "AUC: 0.53 PR AUC: 0.28 \n",
      "\n",
      " Epoch [61] out of 80\n",
      "Training loss: 1.041.. Validation loss: 1.067.. \n",
      "AUC: 0.53 PR AUC: 0.28 \n",
      "\n",
      " Epoch [62] out of 80\n",
      "Training loss: 1.040.. Validation loss: 1.075.. \n",
      "AUC: 0.53 PR AUC: 0.28 \n",
      "\n",
      " Epoch [63] out of 80\n",
      "Training loss: 1.040.. Validation loss: 1.085.. \n",
      "AUC: 0.53 PR AUC: 0.27 \n",
      "\n",
      " Epoch [64] out of 80\n",
      "Training loss: 1.040.. Validation loss: 1.092.. \n",
      "AUC: 0.52 PR AUC: 0.27 \n",
      "\n",
      " Epoch [65] out of 80\n",
      "Training loss: 1.040.. Validation loss: 1.099.. \n",
      "AUC: 0.52 PR AUC: 0.27 \n",
      "\n",
      " Epoch [66] out of 80\n",
      "Training loss: 1.040.. Validation loss: 1.106.. \n",
      "AUC: 0.52 PR AUC: 0.27 \n",
      "\n",
      " Epoch [67] out of 80\n",
      "Training loss: 1.040.. Validation loss: 1.113.. \n",
      "AUC: 0.51 PR AUC: 0.27 \n",
      "\n",
      " Epoch [68] out of 80\n",
      "Training loss: 1.039.. Validation loss: 1.121.. \n",
      "AUC: 0.51 PR AUC: 0.27 \n",
      "\n",
      " Epoch [69] out of 80\n",
      "Training loss: 1.039.. Validation loss: 1.132.. \n",
      "AUC: 0.51 PR AUC: 0.27 \n",
      "\n",
      " Epoch [70] out of 80\n",
      "Training loss: 1.039.. Validation loss: 1.139.. \n",
      "AUC: 0.50 PR AUC: 0.27 \n",
      "\n",
      " Epoch [71] out of 80\n",
      "Training loss: 1.039.. Validation loss: 1.148.. \n",
      "AUC: 0.50 PR AUC: 0.26 \n",
      "\n",
      " Epoch [72] out of 80\n",
      "Training loss: 1.039.. Validation loss: 1.158.. \n",
      "AUC: 0.50 PR AUC: 0.26 \n",
      "\n",
      " Epoch [73] out of 80\n",
      "Training loss: 1.038.. Validation loss: 1.166.. \n",
      "AUC: 0.49 PR AUC: 0.26 \n",
      "\n",
      " Epoch [74] out of 80\n",
      "Training loss: 1.038.. Validation loss: 1.175.. \n",
      "AUC: 0.49 PR AUC: 0.26 \n",
      "\n",
      " Epoch [75] out of 80\n",
      "Training loss: 1.038.. Validation loss: 1.183.. \n",
      "AUC: 0.49 PR AUC: 0.26 \n",
      "\n",
      " Epoch [76] out of 80\n",
      "Training loss: 1.038.. Validation loss: 1.192.. \n",
      "AUC: 0.49 PR AUC: 0.26 \n",
      "\n",
      " Epoch [77] out of 80\n",
      "Training loss: 1.038.. Validation loss: 1.198.. \n",
      "AUC: 0.48 PR AUC: 0.26 \n",
      "\n",
      " Epoch [78] out of 80\n",
      "Training loss: 1.038.. Validation loss: 1.206.. \n",
      "AUC: 0.48 PR AUC: 0.25 \n",
      "\n",
      " Epoch [79] out of 80\n",
      "Training loss: 1.038.. Validation loss: 1.214.. \n",
      "AUC: 0.48 PR AUC: 0.25 \n",
      "\n",
      " Epoch [80] out of 80\n",
      "Training loss: 1.038.. Validation loss: 1.220.. \n",
      "AUC: 0.48 PR AUC: 0.25 \n",
      "\n",
      " Finished Training\n",
      "starttime = 2021-01-12 15:45:26.998422\n",
      "endtime = 2021-01-12 18:01:51.488574\n",
      "\n",
      " Last model \n",
      "\n",
      "Area Under ROC Curve: 0.54\n",
      "Area Under PR Curve(AP): 0.66\n",
      "Brier score : 0.274\n",
      "Accuracy for Classifier : 0.49\n",
      "Cut off: 0.7527\n",
      "[[1972   27]\n",
      " [2389  388]]\n",
      "\n",
      " Best model \n",
      "\n",
      "Area Under ROC Curve: 0.54\n",
      "Area Under PR Curve(AP): 0.66\n",
      "Brier score : 0.274\n",
      "Accuracy for Classifier : 0.49\n",
      "Cut off: 0.7527\n",
      "[[1972   27]\n",
      " [2389  388]]\n",
      "hypercount: 7\n",
      "\n",
      "\n",
      "i-Onedir_2_lr_0.001_drop0.2\n",
      "Net(\n",
      "  (fc1): Linear(in_features=35, out_features=35, bias=True)\n",
      "  (fc2): LSTM(35, 1, num_layers=2, batch_first=True)\n",
      "  (combination_layer): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (projection): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      " Epoch [1] out of 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.141.. Validation loss: 0.709.. \n",
      "AUC: 0.65 PR AUC: 0.26 \n",
      "\n",
      " Epoch [2] out of 80\n",
      "Training loss: 1.119.. Validation loss: 0.691.. \n",
      "AUC: 0.68 PR AUC: 0.26 \n",
      "\n",
      " Epoch [3] out of 80\n",
      "Training loss: 1.113.. Validation loss: 0.700.. \n",
      "AUC: 0.70 PR AUC: 0.28 \n",
      "\n",
      " Epoch [4] out of 80\n",
      "Training loss: 1.110.. Validation loss: 0.718.. \n",
      "AUC: 0.70 PR AUC: 0.29 \n",
      "\n",
      " Epoch [5] out of 80\n",
      "Training loss: 1.107.. Validation loss: 0.737.. \n",
      "AUC: 0.61 PR AUC: 0.25 \n",
      "\n",
      " Epoch [6] out of 80\n",
      "Training loss: 1.105.. Validation loss: 0.722.. \n",
      "AUC: 0.65 PR AUC: 0.29 \n",
      "\n",
      " Epoch [7] out of 80\n",
      "Training loss: 1.103.. Validation loss: 0.746.. \n",
      "AUC: 0.64 PR AUC: 0.29 \n",
      "\n",
      " Epoch [8] out of 80\n",
      "Training loss: 1.102.. Validation loss: 0.741.. \n",
      "AUC: 0.61 PR AUC: 0.27 \n",
      "\n",
      " Epoch [9] out of 80\n",
      "Training loss: 1.101.. Validation loss: 0.736.. \n",
      "AUC: 0.60 PR AUC: 0.28 \n",
      "\n",
      " Epoch [10] out of 80\n",
      "Training loss: 1.100.. Validation loss: 0.739.. \n",
      "AUC: 0.63 PR AUC: 0.30 \n",
      "\n",
      " Epoch [11] out of 80\n",
      "Training loss: 1.097.. Validation loss: 0.758.. \n",
      "AUC: 0.65 PR AUC: 0.31 \n",
      "\n",
      " Epoch [12] out of 80\n",
      "Training loss: 1.097.. Validation loss: 0.778.. \n",
      "AUC: 0.65 PR AUC: 0.29 \n",
      "\n",
      " Epoch [13] out of 80\n",
      "Training loss: 1.097.. Validation loss: 0.745.. \n",
      "AUC: 0.63 PR AUC: 0.30 \n",
      "\n",
      " Epoch [14] out of 80\n",
      "Training loss: 1.097.. Validation loss: 0.764.. \n",
      "AUC: 0.61 PR AUC: 0.29 \n",
      "\n",
      " Epoch [15] out of 80\n",
      "Training loss: 1.096.. Validation loss: 0.763.. \n",
      "AUC: 0.62 PR AUC: 0.30 \n",
      "\n",
      " Epoch [16] out of 80\n",
      "Training loss: 1.095.. Validation loss: 0.771.. \n",
      "AUC: 0.62 PR AUC: 0.30 \n",
      "\n",
      " Epoch [17] out of 80\n",
      "Training loss: 1.096.. Validation loss: 0.747.. \n",
      "AUC: 0.62 PR AUC: 0.30 \n",
      "\n",
      " Epoch [18] out of 80\n",
      "Training loss: 1.095.. Validation loss: 0.807.. \n",
      "AUC: 0.68 PR AUC: 0.31 \n",
      "\n",
      " Epoch [19] out of 80\n",
      "Training loss: 1.096.. Validation loss: 0.760.. \n",
      "AUC: 0.62 PR AUC: 0.31 \n",
      "\n",
      " Epoch [20] out of 80\n",
      "Training loss: 1.093.. Validation loss: 0.785.. \n",
      "AUC: 0.64 PR AUC: 0.32 \n",
      "\n",
      " Epoch [21] out of 80\n",
      "Training loss: 1.093.. Validation loss: 0.816.. \n",
      "AUC: 0.62 PR AUC: 0.31 \n",
      "\n",
      " Epoch [22] out of 80\n",
      "Training loss: 1.094.. Validation loss: 0.789.. \n",
      "AUC: 0.57 PR AUC: 0.30 \n",
      "\n",
      " Epoch [23] out of 80\n",
      "Training loss: 1.095.. Validation loss: 0.783.. \n",
      "AUC: 0.60 PR AUC: 0.31 \n",
      "\n",
      " Epoch [24] out of 80\n",
      "Training loss: 1.093.. Validation loss: 0.779.. \n",
      "AUC: 0.70 PR AUC: 0.32 \n",
      "\n",
      " Epoch [25] out of 80\n",
      "Training loss: 1.092.. Validation loss: 0.824.. \n",
      "AUC: 0.61 PR AUC: 0.31 \n",
      "\n",
      " Epoch [26] out of 80\n",
      "Training loss: 1.093.. Validation loss: 0.812.. \n",
      "AUC: 0.63 PR AUC: 0.32 \n",
      "\n",
      " Epoch [27] out of 80\n",
      "Training loss: 1.093.. Validation loss: 0.819.. \n",
      "AUC: 0.59 PR AUC: 0.31 \n",
      "\n",
      " Epoch [28] out of 80\n",
      "Training loss: 1.093.. Validation loss: 0.859.. \n",
      "AUC: 0.66 PR AUC: 0.33 \n",
      "\n",
      " Epoch [29] out of 80\n",
      "Training loss: 1.092.. Validation loss: 0.861.. \n",
      "AUC: 0.59 PR AUC: 0.29 \n",
      "\n",
      " Epoch [30] out of 80\n",
      "Training loss: 1.093.. Validation loss: 0.868.. \n",
      "AUC: 0.62 PR AUC: 0.31 \n",
      "\n",
      " Epoch [31] out of 80\n",
      "Training loss: 1.092.. Validation loss: 0.877.. \n",
      "AUC: 0.63 PR AUC: 0.31 \n",
      "\n",
      " Epoch [32] out of 80\n",
      "Training loss: 1.093.. Validation loss: 0.864.. \n",
      "AUC: 0.58 PR AUC: 0.31 \n",
      "\n",
      " Epoch [33] out of 80\n",
      "Training loss: 1.092.. Validation loss: 0.861.. \n",
      "AUC: 0.54 PR AUC: 0.27 \n",
      "\n",
      " Epoch [34] out of 80\n",
      "Training loss: 1.092.. Validation loss: 0.851.. \n",
      "AUC: 0.59 PR AUC: 0.31 \n",
      "\n",
      " Epoch [35] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.864.. \n",
      "AUC: 0.64 PR AUC: 0.32 \n",
      "\n",
      " Epoch [36] out of 80\n",
      "Training loss: 1.093.. Validation loss: 0.873.. \n",
      "AUC: 0.56 PR AUC: 0.31 \n",
      "\n",
      " Epoch [37] out of 80\n",
      "Training loss: 1.092.. Validation loss: 0.867.. \n",
      "AUC: 0.60 PR AUC: 0.32 \n",
      "\n",
      " Epoch [38] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.858.. \n",
      "AUC: 0.57 PR AUC: 0.30 \n",
      "\n",
      " Epoch [39] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.900.. \n",
      "AUC: 0.57 PR AUC: 0.31 \n",
      "\n",
      " Epoch [40] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.875.. \n",
      "AUC: 0.59 PR AUC: 0.31 \n",
      "\n",
      " Epoch [41] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.846.. \n",
      "AUC: 0.59 PR AUC: 0.31 \n",
      "\n",
      " Epoch [42] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.837.. \n",
      "AUC: 0.53 PR AUC: 0.28 \n",
      "\n",
      " Epoch [43] out of 80\n",
      "Training loss: 1.092.. Validation loss: 0.848.. \n",
      "AUC: 0.60 PR AUC: 0.31 \n",
      "\n",
      " Epoch [44] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.900.. \n",
      "AUC: 0.57 PR AUC: 0.32 \n",
      "\n",
      " Epoch [45] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.875.. \n",
      "AUC: 0.62 PR AUC: 0.33 \n",
      "\n",
      " Epoch [46] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.881.. \n",
      "AUC: 0.59 PR AUC: 0.31 \n",
      "\n",
      " Epoch [47] out of 80\n",
      "Training loss: 1.092.. Validation loss: 0.878.. \n",
      "AUC: 0.55 PR AUC: 0.28 \n",
      "\n",
      " Epoch [48] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.836.. \n",
      "AUC: 0.53 PR AUC: 0.29 \n",
      "\n",
      " Epoch [49] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.846.. \n",
      "AUC: 0.50 PR AUC: 0.28 \n",
      "\n",
      " Epoch [50] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.862.. \n",
      "AUC: 0.61 PR AUC: 0.31 \n",
      "\n",
      " Epoch [51] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.849.. \n",
      "AUC: 0.59 PR AUC: 0.32 \n",
      "\n",
      " Epoch [52] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.845.. \n",
      "AUC: 0.62 PR AUC: 0.33 \n",
      "\n",
      " Epoch [53] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.838.. \n",
      "AUC: 0.55 PR AUC: 0.30 \n",
      "\n",
      " Epoch [54] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.809.. \n",
      "AUC: 0.56 PR AUC: 0.29 \n",
      "\n",
      " Epoch [55] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.813.. \n",
      "AUC: 0.56 PR AUC: 0.30 \n",
      "\n",
      " Epoch [56] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.859.. \n",
      "AUC: 0.63 PR AUC: 0.33 \n",
      "\n",
      " Epoch [57] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.816.. \n",
      "AUC: 0.54 PR AUC: 0.30 \n",
      "\n",
      " Epoch [58] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.804.. \n",
      "AUC: 0.56 PR AUC: 0.31 \n",
      "\n",
      " Epoch [59] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.816.. \n",
      "AUC: 0.61 PR AUC: 0.29 \n",
      "\n",
      " Epoch [60] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.809.. \n",
      "AUC: 0.56 PR AUC: 0.30 \n",
      "\n",
      " Epoch [61] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.816.. \n",
      "AUC: 0.54 PR AUC: 0.27 \n",
      "\n",
      " Epoch [62] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.823.. \n",
      "AUC: 0.58 PR AUC: 0.31 \n",
      "\n",
      " Epoch [63] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.830.. \n",
      "AUC: 0.58 PR AUC: 0.31 \n",
      "\n",
      " Epoch [64] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.843.. \n",
      "AUC: 0.63 PR AUC: 0.32 \n",
      "\n",
      " Epoch [65] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.820.. \n",
      "AUC: 0.54 PR AUC: 0.28 \n",
      "\n",
      " Epoch [66] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.860.. \n",
      "AUC: 0.59 PR AUC: 0.31 \n",
      "\n",
      " Epoch [67] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.854.. \n",
      "AUC: 0.59 PR AUC: 0.31 \n",
      "\n",
      " Epoch [68] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.829.. \n",
      "AUC: 0.61 PR AUC: 0.31 \n",
      "\n",
      " Epoch [69] out of 80\n",
      "Training loss: 1.087.. Validation loss: 0.873.. \n",
      "AUC: 0.67 PR AUC: 0.33 \n",
      "\n",
      " Epoch [70] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.839.. \n",
      "AUC: 0.52 PR AUC: 0.29 \n",
      "\n",
      " Epoch [71] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.824.. \n",
      "AUC: 0.60 PR AUC: 0.30 \n",
      "\n",
      " Epoch [72] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.844.. \n",
      "AUC: 0.58 PR AUC: 0.31 \n",
      "\n",
      " Epoch [73] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.858.. \n",
      "AUC: 0.57 PR AUC: 0.31 \n",
      "\n",
      " Epoch [74] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.861.. \n",
      "AUC: 0.63 PR AUC: 0.33 \n",
      "\n",
      " Epoch [75] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.846.. \n",
      "AUC: 0.62 PR AUC: 0.31 \n",
      "\n",
      " Epoch [76] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.826.. \n",
      "AUC: 0.62 PR AUC: 0.32 \n",
      "\n",
      " Epoch [77] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.857.. \n",
      "AUC: 0.61 PR AUC: 0.31 \n",
      "\n",
      " Epoch [78] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.887.. \n",
      "AUC: 0.49 PR AUC: 0.29 \n",
      "\n",
      " Epoch [79] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.851.. \n",
      "AUC: 0.59 PR AUC: 0.30 \n",
      "\n",
      " Epoch [80] out of 80\n",
      "Training loss: 1.087.. Validation loss: 0.846.. \n",
      "AUC: 0.63 PR AUC: 0.31 \n",
      "\n",
      " Finished Training\n",
      "starttime = 2021-01-12 18:01:52.085158\n",
      "endtime = 2021-01-12 20:10:44.716215\n",
      "\n",
      " Last model \n",
      "\n",
      "Area Under ROC Curve: 0.57\n",
      "Area Under PR Curve(AP): 0.71\n",
      "Brier score : 0.248\n",
      "Accuracy for Classifier : 0.56\n",
      "Cut off: 0.5703\n",
      "[[1847  152]\n",
      " [1961  816]]\n",
      "\n",
      " Best model \n",
      "\n",
      "Area Under ROC Curve: 0.56\n",
      "Area Under PR Curve(AP): 0.70\n",
      "Brier score : 0.250\n",
      "Accuracy for Classifier : 0.55\n",
      "Cut off: 0.5703\n",
      "[[1840  159]\n",
      " [1977  800]]\n",
      "hypercount: 8\n",
      "\n",
      "\n",
      "i-Onedir_2_lr_0.0001_drop0.2\n",
      "Net(\n",
      "  (fc1): Linear(in_features=35, out_features=35, bias=True)\n",
      "  (fc2): LSTM(35, 1, num_layers=2, batch_first=True)\n",
      "  (combination_layer): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (projection): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      " Epoch [1] out of 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.333.. Validation loss: 0.708.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [2] out of 80\n",
      "Training loss: 1.235.. Validation loss: 0.784.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [3] out of 80\n",
      "Training loss: 1.205.. Validation loss: 0.834.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [4] out of 80\n",
      "Training loss: 1.199.. Validation loss: 0.858.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [5] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.867.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [6] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.871.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [7] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.873.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [8] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.873.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [9] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.873.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [10] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.873.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [11] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.873.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "out of patience\n",
      "\n",
      " Finished Training\n",
      "starttime = 2021-01-12 20:10:45.316875\n",
      "endtime = 2021-01-12 20:28:21.175776\n",
      "\n",
      " Last model \n",
      "\n",
      "Area Under ROC Curve: 0.50\n",
      "Area Under PR Curve(AP): 0.79\n",
      "Brier score : 0.246\n",
      "Accuracy for Classifier : 0.42\n",
      "Cut off: 1.5258\n",
      "[[1999    0]\n",
      " [2777    0]]\n",
      "\n",
      " Best model \n",
      "\n",
      "Area Under ROC Curve: 0.50\n",
      "Area Under PR Curve(AP): 0.79\n",
      "Brier score : 0.246\n",
      "Accuracy for Classifier : 0.42\n",
      "Cut off: 1.5258\n",
      "[[1999    0]\n",
      " [2777    0]]\n",
      "hypercount: 9\n",
      "\n",
      "\n",
      "i-Onedir_3_lr_0.001_nodrop\n",
      "Net(\n",
      "  (fc1): Linear(in_features=35, out_features=35, bias=True)\n",
      "  (fc2): LSTM(35, 1, num_layers=3, batch_first=True)\n",
      "  (combination_layer): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (projection): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (dropout_layer): Dropout(p=0, inplace=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      " Epoch [1] out of 80\n",
      "Training loss: 1.134.. Validation loss: 0.515.. \n",
      "AUC: 0.86 PR AUC: 0.42 \n",
      "\n",
      " Epoch [2] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.505.. \n",
      "AUC: 0.85 PR AUC: 0.38 \n",
      "\n",
      " Epoch [3] out of 80\n",
      "Training loss: 1.077.. Validation loss: 0.495.. \n",
      "AUC: 0.86 PR AUC: 0.38 \n",
      "\n",
      " Epoch [4] out of 80\n",
      "Training loss: 1.068.. Validation loss: 0.507.. \n",
      "AUC: 0.85 PR AUC: 0.36 \n",
      "\n",
      " Epoch [5] out of 80\n",
      "Training loss: 1.059.. Validation loss: 0.499.. \n",
      "AUC: 0.86 PR AUC: 0.39 \n",
      "\n",
      " Epoch [6] out of 80\n",
      "Training loss: 1.054.. Validation loss: 0.518.. \n",
      "AUC: 0.84 PR AUC: 0.38 \n",
      "\n",
      " Epoch [7] out of 80\n",
      "Training loss: 1.050.. Validation loss: 0.520.. \n",
      "AUC: 0.84 PR AUC: 0.38 \n",
      "\n",
      " Epoch [8] out of 80\n",
      "Training loss: 1.047.. Validation loss: 0.509.. \n",
      "AUC: 0.85 PR AUC: 0.38 \n",
      "\n",
      " Epoch [9] out of 80\n",
      "Training loss: 1.045.. Validation loss: 0.513.. \n",
      "AUC: 0.85 PR AUC: 0.39 \n",
      "\n",
      " Epoch [10] out of 80\n",
      "Training loss: 1.043.. Validation loss: 0.505.. \n",
      "AUC: 0.84 PR AUC: 0.38 \n",
      "\n",
      " Epoch [11] out of 80\n",
      "Training loss: 1.042.. Validation loss: 0.522.. \n",
      "AUC: 0.83 PR AUC: 0.37 \n",
      "\n",
      " Epoch [12] out of 80\n",
      "Training loss: 1.041.. Validation loss: 0.530.. \n",
      "AUC: 0.82 PR AUC: 0.37 \n",
      "\n",
      " Epoch [13] out of 80\n",
      "Training loss: 1.040.. Validation loss: 0.542.. \n",
      "AUC: 0.81 PR AUC: 0.37 \n",
      "\n",
      " Epoch [14] out of 80\n",
      "Training loss: 1.039.. Validation loss: 0.548.. \n",
      "AUC: 0.80 PR AUC: 0.36 \n",
      "\n",
      " Epoch [15] out of 80\n",
      "Training loss: 1.039.. Validation loss: 0.557.. \n",
      "AUC: 0.78 PR AUC: 0.36 \n",
      "\n",
      " Epoch [16] out of 80\n",
      "Training loss: 1.038.. Validation loss: 0.555.. \n",
      "AUC: 0.79 PR AUC: 0.36 \n",
      "\n",
      " Epoch [17] out of 80\n",
      "Training loss: 1.037.. Validation loss: 0.556.. \n",
      "AUC: 0.79 PR AUC: 0.36 \n",
      "\n",
      " Epoch [18] out of 80\n",
      "Training loss: 1.037.. Validation loss: 0.565.. \n",
      "AUC: 0.79 PR AUC: 0.36 \n",
      "\n",
      " Epoch [19] out of 80\n",
      "Training loss: 1.036.. Validation loss: 0.585.. \n",
      "AUC: 0.77 PR AUC: 0.35 \n",
      "\n",
      " Epoch [20] out of 80\n",
      "Training loss: 1.036.. Validation loss: 0.581.. \n",
      "AUC: 0.77 PR AUC: 0.35 \n",
      "\n",
      " Epoch [21] out of 80\n",
      "Training loss: 1.035.. Validation loss: 0.585.. \n",
      "AUC: 0.76 PR AUC: 0.35 \n",
      "\n",
      " Epoch [22] out of 80\n",
      "Training loss: 1.035.. Validation loss: 0.582.. \n",
      "AUC: 0.77 PR AUC: 0.35 \n",
      "\n",
      " Epoch [23] out of 80\n",
      "Training loss: 1.035.. Validation loss: 0.595.. \n",
      "AUC: 0.77 PR AUC: 0.35 \n",
      "\n",
      " Epoch [24] out of 80\n",
      "Training loss: 1.035.. Validation loss: 0.597.. \n",
      "AUC: 0.77 PR AUC: 0.35 \n",
      "\n",
      " Epoch [25] out of 80\n",
      "Training loss: 1.034.. Validation loss: 0.592.. \n",
      "AUC: 0.77 PR AUC: 0.34 \n",
      "\n",
      " Epoch [26] out of 80\n",
      "Training loss: 1.034.. Validation loss: 0.593.. \n",
      "AUC: 0.77 PR AUC: 0.35 \n",
      "\n",
      " Epoch [27] out of 80\n",
      "Training loss: 1.034.. Validation loss: 0.600.. \n",
      "AUC: 0.75 PR AUC: 0.34 \n",
      "\n",
      " Epoch [28] out of 80\n",
      "Training loss: 1.033.. Validation loss: 0.590.. \n",
      "AUC: 0.75 PR AUC: 0.34 \n",
      "\n",
      " Epoch [29] out of 80\n",
      "Training loss: 1.033.. Validation loss: 0.594.. \n",
      "AUC: 0.75 PR AUC: 0.34 \n",
      "\n",
      " Epoch [30] out of 80\n",
      "Training loss: 1.032.. Validation loss: 0.611.. \n",
      "AUC: 0.75 PR AUC: 0.34 \n",
      "\n",
      " Epoch [31] out of 80\n",
      "Training loss: 1.032.. Validation loss: 0.614.. \n",
      "AUC: 0.75 PR AUC: 0.34 \n",
      "\n",
      " Epoch [32] out of 80\n",
      "Training loss: 1.031.. Validation loss: 0.633.. \n",
      "AUC: 0.75 PR AUC: 0.34 \n",
      "\n",
      " Epoch [33] out of 80\n",
      "Training loss: 1.031.. Validation loss: 0.651.. \n",
      "AUC: 0.73 PR AUC: 0.34 \n",
      "\n",
      " Epoch [34] out of 80\n",
      "Training loss: 1.031.. Validation loss: 0.669.. \n",
      "AUC: 0.72 PR AUC: 0.33 \n",
      "\n",
      " Epoch [35] out of 80\n",
      "Training loss: 1.030.. Validation loss: 0.672.. \n",
      "AUC: 0.72 PR AUC: 0.33 \n",
      "\n",
      " Epoch [36] out of 80\n",
      "Training loss: 1.030.. Validation loss: 0.677.. \n",
      "AUC: 0.71 PR AUC: 0.33 \n",
      "\n",
      " Epoch [37] out of 80\n",
      "Training loss: 1.030.. Validation loss: 0.696.. \n",
      "AUC: 0.70 PR AUC: 0.32 \n",
      "\n",
      " Epoch [38] out of 80\n",
      "Training loss: 1.030.. Validation loss: 0.695.. \n",
      "AUC: 0.70 PR AUC: 0.32 \n",
      "\n",
      " Epoch [39] out of 80\n",
      "Training loss: 1.030.. Validation loss: 0.709.. \n",
      "AUC: 0.69 PR AUC: 0.32 \n",
      "\n",
      " Epoch [40] out of 80\n",
      "Training loss: 1.030.. Validation loss: 0.759.. \n",
      "AUC: 0.68 PR AUC: 0.32 \n",
      "\n",
      " Epoch [41] out of 80\n",
      "Training loss: 1.029.. Validation loss: 0.735.. \n",
      "AUC: 0.69 PR AUC: 0.32 \n",
      "\n",
      " Epoch [42] out of 80\n",
      "Training loss: 1.029.. Validation loss: 0.776.. \n",
      "AUC: 0.67 PR AUC: 0.32 \n",
      "\n",
      " Epoch [43] out of 80\n",
      "Training loss: 1.029.. Validation loss: 0.776.. \n",
      "AUC: 0.66 PR AUC: 0.32 \n",
      "\n",
      " Epoch [44] out of 80\n",
      "Training loss: 1.029.. Validation loss: 0.764.. \n",
      "AUC: 0.67 PR AUC: 0.32 \n",
      "\n",
      " Epoch [45] out of 80\n",
      "Training loss: 1.029.. Validation loss: 0.763.. \n",
      "AUC: 0.67 PR AUC: 0.32 \n",
      "\n",
      " Epoch [46] out of 80\n",
      "Training loss: 1.028.. Validation loss: 0.733.. \n",
      "AUC: 0.68 PR AUC: 0.32 \n",
      "\n",
      " Epoch [47] out of 80\n",
      "Training loss: 1.028.. Validation loss: 0.734.. \n",
      "AUC: 0.69 PR AUC: 0.32 \n",
      "\n",
      " Epoch [48] out of 80\n",
      "Training loss: 1.029.. Validation loss: 0.688.. \n",
      "AUC: 0.70 PR AUC: 0.32 \n",
      "\n",
      " Epoch [49] out of 80\n",
      "Training loss: 1.028.. Validation loss: 0.706.. \n",
      "AUC: 0.70 PR AUC: 0.33 \n",
      "\n",
      " Epoch [50] out of 80\n",
      "Training loss: 1.028.. Validation loss: 0.715.. \n",
      "AUC: 0.70 PR AUC: 0.32 \n",
      "\n",
      " Epoch [51] out of 80\n",
      "Training loss: 1.028.. Validation loss: 0.681.. \n",
      "AUC: 0.71 PR AUC: 0.33 \n",
      "\n",
      " Epoch [52] out of 80\n",
      "Training loss: 1.028.. Validation loss: 0.690.. \n",
      "AUC: 0.71 PR AUC: 0.33 \n",
      "\n",
      " Epoch [53] out of 80\n",
      "Training loss: 1.027.. Validation loss: 0.668.. \n",
      "AUC: 0.73 PR AUC: 0.33 \n",
      "\n",
      " Epoch [54] out of 80\n",
      "Training loss: 1.027.. Validation loss: 0.682.. \n",
      "AUC: 0.72 PR AUC: 0.33 \n",
      "\n",
      " Epoch [55] out of 80\n",
      "Training loss: 1.027.. Validation loss: 0.706.. \n",
      "AUC: 0.71 PR AUC: 0.33 \n",
      "\n",
      " Epoch [56] out of 80\n",
      "Training loss: 1.027.. Validation loss: 0.725.. \n",
      "AUC: 0.70 PR AUC: 0.33 \n",
      "\n",
      " Epoch [57] out of 80\n",
      "Training loss: 1.026.. Validation loss: 0.712.. \n",
      "AUC: 0.71 PR AUC: 0.32 \n",
      "\n",
      " Epoch [58] out of 80\n",
      "Training loss: 1.026.. Validation loss: 0.717.. \n",
      "AUC: 0.70 PR AUC: 0.32 \n",
      "\n",
      " Epoch [59] out of 80\n",
      "Training loss: 1.026.. Validation loss: 0.712.. \n",
      "AUC: 0.71 PR AUC: 0.33 \n",
      "\n",
      " Epoch [60] out of 80\n",
      "Training loss: 1.026.. Validation loss: 0.715.. \n",
      "AUC: 0.70 PR AUC: 0.32 \n",
      "\n",
      " Epoch [61] out of 80\n",
      "Training loss: 1.026.. Validation loss: 0.730.. \n",
      "AUC: 0.69 PR AUC: 0.32 \n",
      "\n",
      " Epoch [62] out of 80\n",
      "Training loss: 1.026.. Validation loss: 0.691.. \n",
      "AUC: 0.71 PR AUC: 0.32 \n",
      "\n",
      " Epoch [63] out of 80\n",
      "Training loss: 1.026.. Validation loss: 0.710.. \n",
      "AUC: 0.71 PR AUC: 0.32 \n",
      "\n",
      " Epoch [64] out of 80\n",
      "Training loss: 1.026.. Validation loss: 0.678.. \n",
      "AUC: 0.72 PR AUC: 0.33 \n",
      "\n",
      " Epoch [65] out of 80\n",
      "Training loss: 1.025.. Validation loss: 0.689.. \n",
      "AUC: 0.72 PR AUC: 0.33 \n",
      "\n",
      " Epoch [66] out of 80\n",
      "Training loss: 1.025.. Validation loss: 0.703.. \n",
      "AUC: 0.70 PR AUC: 0.32 \n",
      "\n",
      " Epoch [67] out of 80\n",
      "Training loss: 1.025.. Validation loss: 0.712.. \n",
      "AUC: 0.71 PR AUC: 0.32 \n",
      "\n",
      " Epoch [68] out of 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.025.. Validation loss: 0.712.. \n",
      "AUC: 0.71 PR AUC: 0.32 \n",
      "\n",
      " Epoch [69] out of 80\n",
      "Training loss: 1.025.. Validation loss: 0.722.. \n",
      "AUC: 0.69 PR AUC: 0.32 \n",
      "\n",
      " Epoch [70] out of 80\n",
      "Training loss: 1.025.. Validation loss: 0.724.. \n",
      "AUC: 0.69 PR AUC: 0.32 \n",
      "\n",
      " Epoch [71] out of 80\n",
      "Training loss: 1.025.. Validation loss: 0.757.. \n",
      "AUC: 0.67 PR AUC: 0.32 \n",
      "\n",
      " Epoch [72] out of 80\n",
      "Training loss: 1.025.. Validation loss: 0.766.. \n",
      "AUC: 0.66 PR AUC: 0.32 \n",
      "\n",
      " Epoch [73] out of 80\n",
      "Training loss: 1.024.. Validation loss: 0.805.. \n",
      "AUC: 0.65 PR AUC: 0.32 \n",
      "\n",
      " Epoch [74] out of 80\n",
      "Training loss: 1.024.. Validation loss: 0.745.. \n",
      "AUC: 0.66 PR AUC: 0.32 \n",
      "\n",
      " Epoch [75] out of 80\n",
      "Training loss: 1.024.. Validation loss: 0.778.. \n",
      "AUC: 0.65 PR AUC: 0.31 \n",
      "\n",
      " Epoch [76] out of 80\n",
      "Training loss: 1.024.. Validation loss: 0.827.. \n",
      "AUC: 0.64 PR AUC: 0.31 \n",
      "\n",
      " Epoch [77] out of 80\n",
      "Training loss: 1.024.. Validation loss: 0.808.. \n",
      "AUC: 0.64 PR AUC: 0.31 \n",
      "\n",
      " Epoch [78] out of 80\n",
      "Training loss: 1.024.. Validation loss: 0.806.. \n",
      "AUC: 0.64 PR AUC: 0.31 \n",
      "\n",
      " Epoch [79] out of 80\n",
      "Training loss: 1.024.. Validation loss: 0.859.. \n",
      "AUC: 0.61 PR AUC: 0.31 \n",
      "\n",
      " Epoch [80] out of 80\n",
      "Training loss: 1.024.. Validation loss: 0.876.. \n",
      "AUC: 0.60 PR AUC: 0.30 \n",
      "\n",
      " Finished Training\n",
      "starttime = 2021-01-12 20:28:21.872473\n",
      "endtime = 2021-01-12 23:37:56.866792\n",
      "\n",
      " Last model \n",
      "\n",
      "Area Under ROC Curve: 0.60\n",
      "Area Under PR Curve(AP): 0.72\n",
      "Brier score : 0.257\n",
      "Accuracy for Classifier : 0.55\n",
      "Cut off: 0.5578\n",
      "[[1473  526]\n",
      " [1633 1144]]\n",
      "\n",
      " Best model \n",
      "\n",
      "Area Under ROC Curve: 0.60\n",
      "Area Under PR Curve(AP): 0.72\n",
      "Brier score : 0.257\n",
      "Accuracy for Classifier : 0.55\n",
      "Cut off: 0.5578\n",
      "[[1473  526]\n",
      " [1633 1144]]\n",
      "hypercount: 10\n",
      "\n",
      "\n",
      "i-Onedir_3_lr_0.0001_nodrop\n",
      "Net(\n",
      "  (fc1): Linear(in_features=35, out_features=35, bias=True)\n",
      "  (fc2): LSTM(35, 1, num_layers=3, batch_first=True)\n",
      "  (combination_layer): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (projection): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (dropout_layer): Dropout(p=0, inplace=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      " Epoch [1] out of 80\n",
      "Training loss: 1.222.. Validation loss: 0.905.. \n",
      "AUC: 0.13 PR AUC: 0.03 \n",
      "\n",
      " Epoch [2] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.867.. \n",
      "AUC: 0.47 PR AUC: 0.07 \n",
      "\n",
      " Epoch [3] out of 80\n",
      "Training loss: 1.197.. Validation loss: 0.863.. \n",
      "AUC: 0.44 PR AUC: 0.07 \n",
      "\n",
      " Epoch [4] out of 80\n",
      "Training loss: 1.197.. Validation loss: 0.859.. \n",
      "AUC: 0.65 PR AUC: 0.10 \n",
      "\n",
      " Epoch [5] out of 80\n",
      "Training loss: 1.196.. Validation loss: 0.854.. \n",
      "AUC: 0.77 PR AUC: 0.12 \n",
      "\n",
      " Epoch [6] out of 80\n",
      "Training loss: 1.192.. Validation loss: 0.875.. \n",
      "AUC: 0.57 PR AUC: 0.12 \n",
      "\n",
      " Epoch [7] out of 80\n",
      "Training loss: 1.176.. Validation loss: 0.755.. \n",
      "AUC: 0.84 PR AUC: 0.31 \n",
      "\n",
      " Epoch [8] out of 80\n",
      "Training loss: 1.157.. Validation loss: 0.690.. \n",
      "AUC: 0.86 PR AUC: 0.32 \n",
      "\n",
      " Epoch [9] out of 80\n",
      "Training loss: 1.144.. Validation loss: 0.649.. \n",
      "AUC: 0.86 PR AUC: 0.32 \n",
      "\n",
      " Epoch [10] out of 80\n",
      "Training loss: 1.135.. Validation loss: 0.626.. \n",
      "AUC: 0.69 PR AUC: 0.28 \n",
      "\n",
      " Epoch [11] out of 80\n",
      "Training loss: 1.128.. Validation loss: 0.613.. \n",
      "AUC: 0.65 PR AUC: 0.27 \n",
      "\n",
      " Epoch [12] out of 80\n",
      "Training loss: 1.123.. Validation loss: 0.609.. \n",
      "AUC: 0.63 PR AUC: 0.27 \n",
      "\n",
      " Epoch [13] out of 80\n",
      "Training loss: 1.119.. Validation loss: 0.610.. \n",
      "AUC: 0.62 PR AUC: 0.26 \n",
      "\n",
      " Epoch [14] out of 80\n",
      "Training loss: 1.115.. Validation loss: 0.619.. \n",
      "AUC: 0.61 PR AUC: 0.26 \n",
      "\n",
      " Epoch [15] out of 80\n",
      "Training loss: 1.112.. Validation loss: 0.633.. \n",
      "AUC: 0.59 PR AUC: 0.25 \n",
      "\n",
      " Epoch [16] out of 80\n",
      "Training loss: 1.109.. Validation loss: 0.652.. \n",
      "AUC: 0.58 PR AUC: 0.25 \n",
      "\n",
      " Epoch [17] out of 80\n",
      "Training loss: 1.107.. Validation loss: 0.677.. \n",
      "AUC: 0.57 PR AUC: 0.25 \n",
      "\n",
      " Epoch [18] out of 80\n",
      "Training loss: 1.104.. Validation loss: 0.703.. \n",
      "AUC: 0.56 PR AUC: 0.24 \n",
      "\n",
      " Epoch [19] out of 80\n",
      "Training loss: 1.101.. Validation loss: 0.725.. \n",
      "AUC: 0.54 PR AUC: 0.24 \n",
      "\n",
      " Epoch [20] out of 80\n",
      "Training loss: 1.098.. Validation loss: 0.752.. \n",
      "AUC: 0.52 PR AUC: 0.24 \n",
      "\n",
      " Epoch [21] out of 80\n",
      "Training loss: 1.095.. Validation loss: 0.771.. \n",
      "AUC: 0.51 PR AUC: 0.24 \n",
      "\n",
      " Epoch [22] out of 80\n",
      "Training loss: 1.093.. Validation loss: 0.789.. \n",
      "AUC: 0.50 PR AUC: 0.23 \n",
      "\n",
      " Epoch [23] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.810.. \n",
      "AUC: 0.49 PR AUC: 0.23 \n",
      "\n",
      " Epoch [24] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.832.. \n",
      "AUC: 0.48 PR AUC: 0.23 \n",
      "\n",
      " Epoch [25] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.857.. \n",
      "AUC: 0.46 PR AUC: 0.22 \n",
      "\n",
      " Epoch [26] out of 80\n",
      "Training loss: 1.087.. Validation loss: 0.886.. \n",
      "AUC: 0.45 PR AUC: 0.21 \n",
      "\n",
      " Epoch [27] out of 80\n",
      "Training loss: 1.085.. Validation loss: 0.925.. \n",
      "AUC: 0.44 PR AUC: 0.21 \n",
      "\n",
      " Epoch [28] out of 80\n",
      "Training loss: 1.084.. Validation loss: 0.976.. \n",
      "AUC: 0.42 PR AUC: 0.20 \n",
      "\n",
      " Epoch [29] out of 80\n",
      "Training loss: 1.082.. Validation loss: 1.025.. \n",
      "AUC: 0.41 PR AUC: 0.19 \n",
      "\n",
      " Epoch [30] out of 80\n",
      "Training loss: 1.081.. Validation loss: 1.064.. \n",
      "AUC: 0.40 PR AUC: 0.18 \n",
      "\n",
      " Epoch [31] out of 80\n",
      "Training loss: 1.079.. Validation loss: 1.094.. \n",
      "AUC: 0.39 PR AUC: 0.18 \n",
      "\n",
      " Epoch [32] out of 80\n",
      "Training loss: 1.078.. Validation loss: 1.115.. \n",
      "AUC: 0.39 PR AUC: 0.18 \n",
      "\n",
      " Epoch [33] out of 80\n",
      "Training loss: 1.076.. Validation loss: 1.131.. \n",
      "AUC: 0.39 PR AUC: 0.18 \n",
      "\n",
      " Epoch [34] out of 80\n",
      "Training loss: 1.075.. Validation loss: 1.145.. \n",
      "AUC: 0.38 PR AUC: 0.18 \n",
      "\n",
      " Epoch [35] out of 80\n",
      "Training loss: 1.074.. Validation loss: 1.156.. \n",
      "AUC: 0.38 PR AUC: 0.18 \n",
      "\n",
      " Epoch [36] out of 80\n",
      "Training loss: 1.073.. Validation loss: 1.164.. \n",
      "AUC: 0.38 PR AUC: 0.18 \n",
      "\n",
      " Epoch [37] out of 80\n",
      "Training loss: 1.072.. Validation loss: 1.168.. \n",
      "AUC: 0.39 PR AUC: 0.18 \n",
      "\n",
      " Epoch [38] out of 80\n",
      "Training loss: 1.071.. Validation loss: 1.172.. \n",
      "AUC: 0.39 PR AUC: 0.18 \n",
      "\n",
      " Epoch [39] out of 80\n",
      "Training loss: 1.071.. Validation loss: 1.175.. \n",
      "AUC: 0.39 PR AUC: 0.18 \n",
      "\n",
      " Epoch [40] out of 80\n",
      "Training loss: 1.070.. Validation loss: 1.176.. \n",
      "AUC: 0.39 PR AUC: 0.18 \n",
      "\n",
      " Epoch [41] out of 80\n",
      "Training loss: 1.069.. Validation loss: 1.179.. \n",
      "AUC: 0.39 PR AUC: 0.18 \n",
      "\n",
      " Epoch [42] out of 80\n",
      "Training loss: 1.068.. Validation loss: 1.182.. \n",
      "AUC: 0.39 PR AUC: 0.18 \n",
      "\n",
      " Epoch [43] out of 80\n",
      "Training loss: 1.068.. Validation loss: 1.186.. \n",
      "AUC: 0.39 PR AUC: 0.18 \n",
      "\n",
      " Epoch [44] out of 80\n",
      "Training loss: 1.067.. Validation loss: 1.188.. \n",
      "AUC: 0.39 PR AUC: 0.18 \n",
      "\n",
      " Epoch [45] out of 80\n",
      "Training loss: 1.066.. Validation loss: 1.188.. \n",
      "AUC: 0.40 PR AUC: 0.19 \n",
      "\n",
      " Epoch [46] out of 80\n",
      "Training loss: 1.066.. Validation loss: 1.190.. \n",
      "AUC: 0.40 PR AUC: 0.19 \n",
      "\n",
      " Epoch [47] out of 80\n",
      "Training loss: 1.065.. Validation loss: 1.192.. \n",
      "AUC: 0.40 PR AUC: 0.19 \n",
      "\n",
      " Epoch [48] out of 80\n",
      "Training loss: 1.064.. Validation loss: 1.192.. \n",
      "AUC: 0.40 PR AUC: 0.19 \n",
      "\n",
      " Epoch [49] out of 80\n",
      "Training loss: 1.064.. Validation loss: 1.193.. \n",
      "AUC: 0.40 PR AUC: 0.19 \n",
      "\n",
      " Epoch [50] out of 80\n",
      "Training loss: 1.063.. Validation loss: 1.193.. \n",
      "AUC: 0.41 PR AUC: 0.19 \n",
      "\n",
      " Epoch [51] out of 80\n",
      "Training loss: 1.062.. Validation loss: 1.195.. \n",
      "AUC: 0.41 PR AUC: 0.19 \n",
      "\n",
      " Epoch [52] out of 80\n",
      "Training loss: 1.062.. Validation loss: 1.197.. \n",
      "AUC: 0.41 PR AUC: 0.19 \n",
      "\n",
      " Epoch [53] out of 80\n",
      "Training loss: 1.061.. Validation loss: 1.200.. \n",
      "AUC: 0.41 PR AUC: 0.19 \n",
      "\n",
      " Epoch [54] out of 80\n",
      "Training loss: 1.060.. Validation loss: 1.202.. \n",
      "AUC: 0.41 PR AUC: 0.19 \n",
      "\n",
      " Epoch [55] out of 80\n",
      "Training loss: 1.060.. Validation loss: 1.206.. \n",
      "AUC: 0.41 PR AUC: 0.19 \n",
      "\n",
      " Epoch [56] out of 80\n",
      "Training loss: 1.059.. Validation loss: 1.211.. \n",
      "AUC: 0.41 PR AUC: 0.19 \n",
      "\n",
      " Epoch [57] out of 80\n",
      "Training loss: 1.058.. Validation loss: 1.215.. \n",
      "AUC: 0.41 PR AUC: 0.19 \n",
      "\n",
      " Epoch [58] out of 80\n",
      "Training loss: 1.058.. Validation loss: 1.218.. \n",
      "AUC: 0.41 PR AUC: 0.20 \n",
      "\n",
      " Epoch [59] out of 80\n",
      "Training loss: 1.057.. Validation loss: 1.223.. \n",
      "AUC: 0.41 PR AUC: 0.20 \n",
      "\n",
      " Epoch [60] out of 80\n",
      "Training loss: 1.056.. Validation loss: 1.228.. \n",
      "AUC: 0.41 PR AUC: 0.20 \n",
      "\n",
      " Epoch [61] out of 80\n",
      "Training loss: 1.056.. Validation loss: 1.233.. \n",
      "AUC: 0.41 PR AUC: 0.20 \n",
      "\n",
      " Epoch [62] out of 80\n",
      "Training loss: 1.055.. Validation loss: 1.236.. \n",
      "AUC: 0.42 PR AUC: 0.20 \n",
      "\n",
      " Epoch [63] out of 80\n",
      "Training loss: 1.055.. Validation loss: 1.240.. \n",
      "AUC: 0.42 PR AUC: 0.20 \n",
      "\n",
      " Epoch [64] out of 80\n",
      "Training loss: 1.054.. Validation loss: 1.243.. \n",
      "AUC: 0.42 PR AUC: 0.20 \n",
      "\n",
      " Epoch [65] out of 80\n",
      "Training loss: 1.053.. Validation loss: 1.249.. \n",
      "AUC: 0.42 PR AUC: 0.20 \n",
      "\n",
      " Epoch [66] out of 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.053.. Validation loss: 1.254.. \n",
      "AUC: 0.42 PR AUC: 0.20 \n",
      "\n",
      " Epoch [67] out of 80\n",
      "Training loss: 1.052.. Validation loss: 1.259.. \n",
      "AUC: 0.42 PR AUC: 0.20 \n",
      "\n",
      " Epoch [68] out of 80\n",
      "Training loss: 1.052.. Validation loss: 1.265.. \n",
      "AUC: 0.42 PR AUC: 0.20 \n",
      "\n",
      " Epoch [69] out of 80\n",
      "Training loss: 1.051.. Validation loss: 1.269.. \n",
      "AUC: 0.42 PR AUC: 0.20 \n",
      "\n",
      " Epoch [70] out of 80\n",
      "Training loss: 1.051.. Validation loss: 1.275.. \n",
      "AUC: 0.42 PR AUC: 0.20 \n",
      "\n",
      " Epoch [71] out of 80\n",
      "Training loss: 1.050.. Validation loss: 1.280.. \n",
      "AUC: 0.42 PR AUC: 0.20 \n",
      "\n",
      " Epoch [72] out of 80\n",
      "Training loss: 1.050.. Validation loss: 1.284.. \n",
      "AUC: 0.42 PR AUC: 0.20 \n",
      "\n",
      " Epoch [73] out of 80\n",
      "Training loss: 1.049.. Validation loss: 1.288.. \n",
      "AUC: 0.42 PR AUC: 0.20 \n",
      "\n",
      " Epoch [74] out of 80\n",
      "Training loss: 1.049.. Validation loss: 1.294.. \n",
      "AUC: 0.42 PR AUC: 0.20 \n",
      "\n",
      " Epoch [75] out of 80\n",
      "Training loss: 1.049.. Validation loss: 1.302.. \n",
      "AUC: 0.42 PR AUC: 0.21 \n",
      "\n",
      " Epoch [76] out of 80\n",
      "Training loss: 1.048.. Validation loss: 1.308.. \n",
      "AUC: 0.42 PR AUC: 0.21 \n",
      "\n",
      " Epoch [77] out of 80\n",
      "Training loss: 1.048.. Validation loss: 1.314.. \n",
      "AUC: 0.42 PR AUC: 0.21 \n",
      "\n",
      " Epoch [78] out of 80\n",
      "Training loss: 1.047.. Validation loss: 1.320.. \n",
      "AUC: 0.42 PR AUC: 0.21 \n",
      "\n",
      " Epoch [79] out of 80\n",
      "Training loss: 1.047.. Validation loss: 1.325.. \n",
      "AUC: 0.42 PR AUC: 0.21 \n",
      "\n",
      " Epoch [80] out of 80\n",
      "Training loss: 1.047.. Validation loss: 1.331.. \n",
      "AUC: 0.42 PR AUC: 0.21 \n",
      "\n",
      " Finished Training\n",
      "starttime = 2021-01-12 23:37:57.565250\n",
      "endtime = 2021-01-13 02:47:15.476344\n",
      "\n",
      " Last model \n",
      "\n",
      "Area Under ROC Curve: 0.49\n",
      "Area Under PR Curve(AP): 0.62\n",
      "Brier score : 0.288\n",
      "Accuracy for Classifier : 0.48\n",
      "Cut off: 0.7786\n",
      "[[1982   17]\n",
      " [2485  292]]\n",
      "\n",
      " Best model \n",
      "\n",
      "Area Under ROC Curve: 0.49\n",
      "Area Under PR Curve(AP): 0.62\n",
      "Brier score : 0.288\n",
      "Accuracy for Classifier : 0.48\n",
      "Cut off: 0.7786\n",
      "[[1982   17]\n",
      " [2485  292]]\n",
      "hypercount: 11\n",
      "\n",
      "\n",
      "i-Onedir_3_lr_0.001_drop0.2\n",
      "Net(\n",
      "  (fc1): Linear(in_features=35, out_features=35, bias=True)\n",
      "  (fc2): LSTM(35, 1, num_layers=3, batch_first=True)\n",
      "  (combination_layer): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (projection): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      " Epoch [1] out of 80\n",
      "Training loss: 1.151.. Validation loss: 0.701.. \n",
      "AUC: 0.72 PR AUC: 0.26 \n",
      "\n",
      " Epoch [2] out of 80\n",
      "Training loss: 1.123.. Validation loss: 0.691.. \n",
      "AUC: 0.72 PR AUC: 0.28 \n",
      "\n",
      " Epoch [3] out of 80\n",
      "Training loss: 1.117.. Validation loss: 0.678.. \n",
      "AUC: 0.79 PR AUC: 0.32 \n",
      "\n",
      " Epoch [4] out of 80\n",
      "Training loss: 1.113.. Validation loss: 0.681.. \n",
      "AUC: 0.78 PR AUC: 0.31 \n",
      "\n",
      " Epoch [5] out of 80\n",
      "Training loss: 1.110.. Validation loss: 0.669.. \n",
      "AUC: 0.80 PR AUC: 0.33 \n",
      "\n",
      " Epoch [6] out of 80\n",
      "Training loss: 1.107.. Validation loss: 0.665.. \n",
      "AUC: 0.84 PR AUC: 0.35 \n",
      "\n",
      " Epoch [7] out of 80\n",
      "Training loss: 1.104.. Validation loss: 0.664.. \n",
      "AUC: 0.80 PR AUC: 0.35 \n",
      "\n",
      " Epoch [8] out of 80\n",
      "Training loss: 1.103.. Validation loss: 0.662.. \n",
      "AUC: 0.83 PR AUC: 0.35 \n",
      "\n",
      " Epoch [9] out of 80\n",
      "Training loss: 1.102.. Validation loss: 0.659.. \n",
      "AUC: 0.84 PR AUC: 0.36 \n",
      "\n",
      " Epoch [10] out of 80\n",
      "Training loss: 1.101.. Validation loss: 0.661.. \n",
      "AUC: 0.82 PR AUC: 0.34 \n",
      "\n",
      " Epoch [11] out of 80\n",
      "Training loss: 1.100.. Validation loss: 0.665.. \n",
      "AUC: 0.77 PR AUC: 0.31 \n",
      "\n",
      " Epoch [12] out of 80\n",
      "Training loss: 1.099.. Validation loss: 0.662.. \n",
      "AUC: 0.81 PR AUC: 0.31 \n",
      "\n",
      " Epoch [13] out of 80\n",
      "Training loss: 1.098.. Validation loss: 0.662.. \n",
      "AUC: 0.69 PR AUC: 0.30 \n",
      "\n",
      " Epoch [14] out of 80\n",
      "Training loss: 1.097.. Validation loss: 0.665.. \n",
      "AUC: 0.72 PR AUC: 0.31 \n",
      "\n",
      " Epoch [15] out of 80\n",
      "Training loss: 1.096.. Validation loss: 0.662.. \n",
      "AUC: 0.69 PR AUC: 0.31 \n",
      "\n",
      " Epoch [16] out of 80\n",
      "Training loss: 1.096.. Validation loss: 0.672.. \n",
      "AUC: 0.71 PR AUC: 0.32 \n",
      "\n",
      " Epoch [17] out of 80\n",
      "Training loss: 1.096.. Validation loss: 0.673.. \n",
      "AUC: 0.67 PR AUC: 0.31 \n",
      "\n",
      " Epoch [18] out of 80\n",
      "Training loss: 1.094.. Validation loss: 0.704.. \n",
      "AUC: 0.66 PR AUC: 0.30 \n",
      "\n",
      " Epoch [19] out of 80\n",
      "Training loss: 1.096.. Validation loss: 0.663.. \n",
      "AUC: 0.80 PR AUC: 0.34 \n",
      "\n",
      " Epoch [20] out of 80\n",
      "Training loss: 1.094.. Validation loss: 0.654.. \n",
      "AUC: 0.81 PR AUC: 0.34 \n",
      "\n",
      " Epoch [21] out of 80\n",
      "Training loss: 1.092.. Validation loss: 0.661.. \n",
      "AUC: 0.67 PR AUC: 0.30 \n",
      "\n",
      " Epoch [22] out of 80\n",
      "Training loss: 1.094.. Validation loss: 0.665.. \n",
      "AUC: 0.72 PR AUC: 0.32 \n",
      "\n",
      " Epoch [23] out of 80\n",
      "Training loss: 1.094.. Validation loss: 0.660.. \n",
      "AUC: 0.70 PR AUC: 0.34 \n",
      "\n",
      " Epoch [24] out of 80\n",
      "Training loss: 1.093.. Validation loss: 0.668.. \n",
      "AUC: 0.70 PR AUC: 0.31 \n",
      "\n",
      " Epoch [25] out of 80\n",
      "Training loss: 1.093.. Validation loss: 0.652.. \n",
      "AUC: 0.71 PR AUC: 0.32 \n",
      "\n",
      " Epoch [26] out of 80\n",
      "Training loss: 1.093.. Validation loss: 0.655.. \n",
      "AUC: 0.80 PR AUC: 0.35 \n",
      "\n",
      " Epoch [27] out of 80\n",
      "Training loss: 1.092.. Validation loss: 0.673.. \n",
      "AUC: 0.69 PR AUC: 0.34 \n",
      "\n",
      " Epoch [28] out of 80\n",
      "Training loss: 1.093.. Validation loss: 0.659.. \n",
      "AUC: 0.73 PR AUC: 0.35 \n",
      "\n",
      " Epoch [29] out of 80\n",
      "Training loss: 1.092.. Validation loss: 0.660.. \n",
      "AUC: 0.69 PR AUC: 0.34 \n",
      "\n",
      " Epoch [30] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.651.. \n",
      "AUC: 0.68 PR AUC: 0.33 \n",
      "\n",
      " Epoch [31] out of 80\n",
      "Training loss: 1.092.. Validation loss: 0.652.. \n",
      "AUC: 0.82 PR AUC: 0.37 \n",
      "\n",
      " Epoch [32] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.658.. \n",
      "AUC: 0.71 PR AUC: 0.33 \n",
      "\n",
      " Epoch [33] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.648.. \n",
      "AUC: 0.82 PR AUC: 0.37 \n",
      "\n",
      " Epoch [34] out of 80\n",
      "Training loss: 1.092.. Validation loss: 0.651.. \n",
      "AUC: 0.81 PR AUC: 0.38 \n",
      "\n",
      " Epoch [35] out of 80\n",
      "Training loss: 1.092.. Validation loss: 0.651.. \n",
      "AUC: 0.81 PR AUC: 0.37 \n",
      "\n",
      " Epoch [36] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.665.. \n",
      "AUC: 0.71 PR AUC: 0.34 \n",
      "\n",
      " Epoch [37] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.663.. \n",
      "AUC: 0.70 PR AUC: 0.33 \n",
      "\n",
      " Epoch [38] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.667.. \n",
      "AUC: 0.70 PR AUC: 0.33 \n",
      "\n",
      " Epoch [39] out of 80\n",
      "Training loss: 1.092.. Validation loss: 0.651.. \n",
      "AUC: 0.81 PR AUC: 0.38 \n",
      "\n",
      " Epoch [40] out of 80\n",
      "Training loss: 1.092.. Validation loss: 0.656.. \n",
      "AUC: 0.69 PR AUC: 0.35 \n",
      "\n",
      " Epoch [41] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.660.. \n",
      "AUC: 0.70 PR AUC: 0.34 \n",
      "\n",
      " Epoch [42] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.654.. \n",
      "AUC: 0.70 PR AUC: 0.35 \n",
      "\n",
      " Epoch [43] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.685.. \n",
      "AUC: 0.69 PR AUC: 0.35 \n",
      "\n",
      " Epoch [44] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.659.. \n",
      "AUC: 0.69 PR AUC: 0.34 \n",
      "\n",
      " Epoch [45] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.661.. \n",
      "AUC: 0.69 PR AUC: 0.34 \n",
      "\n",
      " Epoch [46] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.728.. \n",
      "AUC: 0.64 PR AUC: 0.32 \n",
      "\n",
      " Epoch [47] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.671.. \n",
      "AUC: 0.69 PR AUC: 0.34 \n",
      "\n",
      " Epoch [48] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.699.. \n",
      "AUC: 0.70 PR AUC: 0.33 \n",
      "\n",
      " Epoch [49] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.673.. \n",
      "AUC: 0.68 PR AUC: 0.33 \n",
      "\n",
      " Epoch [50] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.724.. \n",
      "AUC: 0.66 PR AUC: 0.32 \n",
      "\n",
      " Epoch [51] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.733.. \n",
      "AUC: 0.65 PR AUC: 0.32 \n",
      "\n",
      " Epoch [52] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.737.. \n",
      "AUC: 0.66 PR AUC: 0.33 \n",
      "\n",
      " Epoch [53] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.735.. \n",
      "AUC: 0.64 PR AUC: 0.31 \n",
      "\n",
      " Epoch [54] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.745.. \n",
      "AUC: 0.66 PR AUC: 0.32 \n",
      "\n",
      " Epoch [55] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.775.. \n",
      "AUC: 0.64 PR AUC: 0.32 \n",
      "\n",
      " Epoch [56] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.724.. \n",
      "AUC: 0.66 PR AUC: 0.31 \n",
      "\n",
      " Epoch [57] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.706.. \n",
      "AUC: 0.63 PR AUC: 0.30 \n",
      "\n",
      " Epoch [58] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.665.. \n",
      "AUC: 0.67 PR AUC: 0.33 \n",
      "\n",
      " Epoch [59] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.775.. \n",
      "AUC: 0.65 PR AUC: 0.31 \n",
      "\n",
      " Epoch [60] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.818.. \n",
      "AUC: 0.61 PR AUC: 0.30 \n",
      "\n",
      " Epoch [61] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.798.. \n",
      "AUC: 0.62 PR AUC: 0.30 \n",
      "\n",
      " Epoch [62] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.771.. \n",
      "AUC: 0.63 PR AUC: 0.31 \n",
      "\n",
      " Epoch [63] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.800.. \n",
      "AUC: 0.60 PR AUC: 0.30 \n",
      "\n",
      " Epoch [64] out of 80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.089.. Validation loss: 0.815.. \n",
      "AUC: 0.59 PR AUC: 0.28 \n",
      "\n",
      " Epoch [65] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.787.. \n",
      "AUC: 0.61 PR AUC: 0.30 \n",
      "\n",
      " Epoch [66] out of 80\n",
      "Training loss: 1.091.. Validation loss: 0.778.. \n",
      "AUC: 0.63 PR AUC: 0.30 \n",
      "\n",
      " Epoch [67] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.717.. \n",
      "AUC: 0.65 PR AUC: 0.30 \n",
      "\n",
      " Epoch [68] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.796.. \n",
      "AUC: 0.59 PR AUC: 0.29 \n",
      "\n",
      " Epoch [69] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.772.. \n",
      "AUC: 0.64 PR AUC: 0.31 \n",
      "\n",
      " Epoch [70] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.836.. \n",
      "AUC: 0.60 PR AUC: 0.30 \n",
      "\n",
      " Epoch [71] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.807.. \n",
      "AUC: 0.62 PR AUC: 0.30 \n",
      "\n",
      " Epoch [72] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.817.. \n",
      "AUC: 0.63 PR AUC: 0.31 \n",
      "\n",
      " Epoch [73] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.794.. \n",
      "AUC: 0.62 PR AUC: 0.30 \n",
      "\n",
      " Epoch [74] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.908.. \n",
      "AUC: 0.61 PR AUC: 0.30 \n",
      "\n",
      " Epoch [75] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.906.. \n",
      "AUC: 0.61 PR AUC: 0.30 \n",
      "\n",
      " Epoch [76] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.840.. \n",
      "AUC: 0.61 PR AUC: 0.30 \n",
      "\n",
      " Epoch [77] out of 80\n",
      "Training loss: 1.090.. Validation loss: 0.829.. \n",
      "AUC: 0.61 PR AUC: 0.30 \n",
      "\n",
      " Epoch [78] out of 80\n",
      "Training loss: 1.088.. Validation loss: 0.876.. \n",
      "AUC: 0.60 PR AUC: 0.30 \n",
      "\n",
      " Epoch [79] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.890.. \n",
      "AUC: 0.60 PR AUC: 0.30 \n",
      "\n",
      " Epoch [80] out of 80\n",
      "Training loss: 1.089.. Validation loss: 0.814.. \n",
      "AUC: 0.59 PR AUC: 0.29 \n",
      "\n",
      " Finished Training\n",
      "starttime = 2021-01-13 02:47:16.204134\n",
      "endtime = 2021-01-13 05:55:46.125346\n",
      "\n",
      " Last model \n",
      "\n",
      "Area Under ROC Curve: 0.55\n",
      "Area Under PR Curve(AP): 0.69\n",
      "Brier score : 0.251\n",
      "Accuracy for Classifier : 0.54\n",
      "Cut off: 0.5408\n",
      "[[1704  295]\n",
      " [1906  871]]\n",
      "\n",
      " Best model \n",
      "\n",
      "Area Under ROC Curve: 0.56\n",
      "Area Under PR Curve(AP): 0.70\n",
      "Brier score : 0.249\n",
      "Accuracy for Classifier : 0.54\n",
      "Cut off: 0.5408\n",
      "[[1688  311]\n",
      " [1877  900]]\n",
      "hypercount: 12\n",
      "\n",
      "\n",
      "i-Onedir_3_lr_0.0001_drop0.2\n",
      "Net(\n",
      "  (fc1): Linear(in_features=35, out_features=35, bias=True)\n",
      "  (fc2): LSTM(35, 1, num_layers=3, batch_first=True)\n",
      "  (combination_layer): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (projection): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      " Epoch [1] out of 80\n",
      "Training loss: 1.279.. Validation loss: 1.039.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [2] out of 80\n",
      "Training loss: 1.211.. Validation loss: 0.929.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [3] out of 80\n",
      "Training loss: 1.199.. Validation loss: 0.893.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [4] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.881.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [5] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.876.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [6] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.874.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [7] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.874.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [8] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.874.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [9] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.873.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [10] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.873.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "\n",
      " Epoch [11] out of 80\n",
      "Training loss: 1.198.. Validation loss: 0.873.. \n",
      "AUC: 0.50 PR AUC: 0.53 \n",
      "out of patience\n",
      "\n",
      " Finished Training\n",
      "starttime = 2021-01-13 05:55:46.843204\n",
      "endtime = 2021-01-13 06:21:41.497955\n",
      "\n",
      " Last model \n",
      "\n",
      "Area Under ROC Curve: 0.50\n",
      "Area Under PR Curve(AP): 0.79\n",
      "Brier score : 0.246\n",
      "Accuracy for Classifier : 0.42\n",
      "Cut off: 1.5258\n",
      "[[1999    0]\n",
      " [2777    0]]\n",
      "\n",
      " Best model \n",
      "\n",
      "Area Under ROC Curve: 0.50\n",
      "Area Under PR Curve(AP): 0.79\n",
      "Brier score : 0.246\n",
      "Accuracy for Classifier : 0.42\n",
      "Cut off: 1.5258\n",
      "[[1999    0]\n",
      " [2777    0]]\n"
     ]
    }
   ],
   "source": [
    "# search grid uni\n",
    "layers = [1,2,3]\n",
    "l_rate = [0.001, 0.0001]\n",
    "drop = [0,0.2]\n",
    "bidirectionality = [False]\n",
    "#loops count\n",
    "hypercount = 0\n",
    "# static parameters\n",
    "n_epochs = 80\n",
    "emb_size = round(features/1)\n",
    "input_size = features\n",
    "output_size = 1\n",
    "###############################\n",
    "\n",
    "f = open('lstm_no_imp_uni.txt', 'w+')\n",
    "\n",
    "for q1 in bidirectionality:\n",
    "    for q2 in layers:\n",
    "        for q3 in drop:\n",
    "            for q4 in l_rate:\n",
    "                hypercount +=1\n",
    "                name = \"i-Bidir_\" if q1 else \"i-Onedir_\"\n",
    "                name = name+str(q2) + \"_lr_\"+str(q4)\n",
    "                name = name+\"_drop\"+str(q3) if q3 == 0.2 else name+\"_nodrop\"\n",
    "                #set parameters\n",
    "                bi_directional = q1\n",
    "                lr = q4\n",
    "                number_layers = q2\n",
    "                dropout = q3 # dropout\n",
    "                print('hypercount: %d' % hypercount)\n",
    "                print('\\n')\n",
    "                print(name)\n",
    "                f.write('\\n\\n' + str(name)+ '\\n\\n')\n",
    "                    \n",
    "                # create the NN\n",
    "                class Net(nn.Module):\n",
    "                    def __init__(self, input_size, emb_size, output_size, bi_directional, number_layers, dropout):\n",
    "                        super(Net, self).__init__()\n",
    "                        self.input_size = input_size\n",
    "                        self.emb_size = emb_size \n",
    "                        self.output_size = output_size\n",
    "                        self.number_layers = number_layers\n",
    "                        self.fc1 = nn.Linear(self.input_size, self.emb_size, bias = True) # I can have a few (IV) within this line - documentation        \n",
    "                        self.fc2 = nn.LSTM(self.emb_size, self.output_size,num_layers=self.number_layers, batch_first = True, bidirectional = bi_directional) \n",
    "                        # in bidirectional encoder we have  forward and backward hidden states\n",
    "                        self.encoding_size = self.output_size * 2 if bi_directional else self.output_size\n",
    "                        self.combination_layer = nn.Linear(self.encoding_size, self.encoding_size)\n",
    "                        # Create affine layer to project to the classes \n",
    "                        self.projection = nn.Linear(self.encoding_size, self.output_size)\n",
    "                        #dropout layer for regularizetion of a sequence\n",
    "                        self.dropout_layer = nn.Dropout(p = dropout)  \n",
    "                        self.relu = nn.ReLU()\n",
    "\n",
    "                    def forward(self, x):\n",
    "                        h = self.relu(self.fc1(x))\n",
    "                        h, _ = self.fc2(h) # h, _ : as I have 2outputs (tuple), only take the real output [0]. \n",
    "                        #print(type(h)) # Underscore throughs away the rest, _ \"I do not care\" variable notation in python\n",
    "                        h = self.relu(self.combination_layer(h))\n",
    "                        h = self.dropout_layer(h)\n",
    "                        h = self.projection(h) \n",
    "                        return h\n",
    "\n",
    "                #create a network \n",
    "                nn_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "                print(nn_model)\n",
    "                #print(list(nn_model.parameters()))\n",
    "                \n",
    "                # BCE Loss and optimizer\n",
    "                criterion = nn.BCEWithLogitsLoss(pos_weight = torch.tensor(round(zeroes/ones,0))) # class imbalance\n",
    "                #print(round(zeroes/ones,0))\n",
    "                optimizer = optim.Adam(nn_model.parameters(), lr=lr) \n",
    "    \n",
    "    \n",
    "                # TRAINING LOOP \n",
    "                epochs = n_epochs\n",
    "                starttime = datetime.now() # datetime object containing current date and time\n",
    "                train_losses, validation_losses = [], []\n",
    "                best = 0\n",
    "                patience = 0\n",
    "                old_auc = 0\n",
    "                old_pr = 0\n",
    "\n",
    "                for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "                    print (\"\\n Epoch [%d] out of %d\" % (epoch + 1, epochs))\n",
    "                    running_loss = 0.0\n",
    "                    validation_loss = 0.0\n",
    "                    roc_auc = 0.0\n",
    "                    pr_auc = 0.0\n",
    "                    m = 0\n",
    "                    \n",
    "                    #train\n",
    "                    #print(list(nn_model.parameters())[0])\n",
    "                    for i in X_train:\n",
    "                        # zero the parameter gradients\n",
    "                        optimizer.zero_grad() # zero the gradient buffers not to consider gradients of previous iterations\n",
    "                        X_batch = X_train[m]\n",
    "                        y_batch = y_train[m]\n",
    "                        # forward + backward + optimize\n",
    "                        outputs = nn_model(X_batch)\n",
    "                        outputs = torch.flatten(outputs)\n",
    "                        y_batch = y_batch.type_as(outputs)\n",
    "                        loss = criterion(outputs, y_batch)\n",
    "                        loss.backward()\n",
    "                        optimizer.step() # Does the update\n",
    "                        running_loss += loss.item()\n",
    "                        m +=1\n",
    "                    #validation \n",
    "                    nn_model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        v_out = nn_model(X_val) \n",
    "                        v_out = torch.flatten(v_out) \n",
    "                        y_val = y_val.type_as(v_out)\n",
    "                        v_loss = criterion(v_out, y_val)\n",
    "                        validation_loss = v_loss.item()\n",
    "                        # auc and pr auc\n",
    "                        val_prob = torch.nn.Sigmoid() (v_out)\n",
    "                        precision, recall, thresholds = precision_recall_curve(y_val, val_prob)\n",
    "                        pr_auc = auc(recall, precision)\n",
    "                        roc_auc = roc_auc_score(y_val,val_prob) \n",
    "\n",
    "                    validation_losses.append(validation_loss) \n",
    "                    train_losses.append(running_loss/len(X_train)) \n",
    "                    print(f\"Training loss: {running_loss/len(X_train):.3f}.. \" f\"Validation loss: {validation_loss:.3f}.. \")\n",
    "                    print(f\"AUC: {roc_auc:.2f} \" f\"PR AUC: {pr_auc:.2f} \")  \n",
    "                    nn_model.train()\n",
    "\n",
    "                    \n",
    "                    if roc_auc > best:\n",
    "                        best = roc_auc\n",
    "                        PATH1 = './'+str(name)+'best.pth' \n",
    "                        torch.save(nn_model.state_dict(), PATH1) # save the model\n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                    if roc_auc == old_auc and pr_auc==old_pr:\n",
    "                        patience +=1\n",
    "                    old_auc = roc_auc\n",
    "                    old_pr = pr_auc\n",
    "                    if patience ==10:\n",
    "                        print(\"out of patience\")\n",
    "                        break\n",
    "\n",
    "                print('\\n Finished Training')\n",
    "                print(\"starttime =\", starttime)\n",
    "                now = datetime.now()\n",
    "                print(\"endtime =\", now)\n",
    "                # end of training loop\n",
    "                \n",
    "                PATH2 = './'+str(name)+'last.pth' \n",
    "                torch.save(nn_model.state_dict(), PATH2) # save the model\n",
    "                print('\\n Last model \\n')\n",
    "                labels, probs = to_one_label(nn_model,label_list,X_test,index_list)\n",
    "                performance (nn_model, labels, probs)\n",
    "                \n",
    "                #load the best model\n",
    "                best_model = Net(input_size, emb_size, output_size,bi_directional, number_layers, dropout)\n",
    "                best_model.load_state_dict(torch.load(PATH1))\n",
    "                print('\\n Best model \\n')\n",
    "                labels, probs = to_one_label(best_model,label_list,X_test,index_list)\n",
    "                performance (best_model, labels, probs)\n",
    "f.close() \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypercount: 1\n",
      "\n",
      "\n",
      "i+Onedir_3_lr_0.0001_drop0.2\n",
      "Net(\n",
      "  (fc1): Linear(in_features=35, out_features=35, bias=True)\n",
      "  (fc2): LSTM(35, 1, num_layers=3, batch_first=True)\n",
      "  (combination_layer): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (projection): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "  (relu): ReLU()\n",
      ")\n",
      "\n",
      " Epoch [1] out of 80\n",
      "Training loss: 1.223.. Validation loss: 0.810.. \n",
      "AUC: 0.14 PR AUC: 0.03 \n",
      "\n",
      " Epoch [2] out of 80\n",
      "Training loss: 1.213.. Validation loss: 0.815.. \n",
      "AUC: 0.39 PR AUC: 0.11 \n",
      "\n",
      " Epoch [3] out of 80\n",
      "Training loss: 1.204.. Validation loss: 0.803.. \n",
      "AUC: 0.46 PR AUC: 0.13 \n",
      "\n",
      " Epoch [4] out of 80\n",
      "Training loss: 1.188.. Validation loss: 0.782.. \n",
      "AUC: 0.33 PR AUC: 0.10 \n",
      "\n",
      " Epoch [5] out of 80\n",
      "Training loss: 1.181.. Validation loss: 0.772.. \n",
      "AUC: 0.38 PR AUC: 0.12 \n",
      "\n",
      " Epoch [6] out of 80\n",
      "Training loss: 1.178.. Validation loss: 0.769.. \n",
      "AUC: 0.42 PR AUC: 0.13 \n",
      "\n",
      " Epoch [7] out of 80\n",
      "Training loss: 1.177.. Validation loss: 0.771.. \n",
      "AUC: 0.44 PR AUC: 0.13 \n",
      "\n",
      " Epoch [8] out of 80\n",
      "Training loss: 1.176.. Validation loss: 0.773.. \n",
      "AUC: 0.47 PR AUC: 0.14 \n",
      "\n",
      " Epoch [9] out of 80\n",
      "Training loss: 1.175.. Validation loss: 0.774.. \n",
      "AUC: 0.44 PR AUC: 0.14 \n",
      "\n",
      " Epoch [10] out of 80\n",
      "Training loss: 1.174.. Validation loss: 0.778.. \n",
      "AUC: 0.50 PR AUC: 0.16 \n",
      "\n",
      " Epoch [11] out of 80\n",
      "Training loss: 1.173.. Validation loss: 0.781.. \n",
      "AUC: 0.49 PR AUC: 0.16 \n",
      "\n",
      " Epoch [12] out of 80\n",
      "Training loss: 1.172.. Validation loss: 0.778.. \n",
      "AUC: 0.52 PR AUC: 0.17 \n",
      "\n",
      " Epoch [13] out of 80\n",
      "Training loss: 1.171.. Validation loss: 0.778.. \n",
      "AUC: 0.52 PR AUC: 0.18 \n",
      "\n",
      " Epoch [14] out of 80\n",
      "Training loss: 1.170.. Validation loss: 0.770.. \n",
      "AUC: 0.56 PR AUC: 0.19 \n",
      "\n",
      " Epoch [15] out of 80\n",
      "Training loss: 1.169.. Validation loss: 0.768.. \n",
      "AUC: 0.72 PR AUC: 0.24 \n",
      "\n",
      " Epoch [16] out of 80\n",
      "Training loss: 1.169.. Validation loss: 0.774.. \n",
      "AUC: 0.66 PR AUC: 0.25 \n",
      "\n",
      " Epoch [17] out of 80\n",
      "Training loss: 1.167.. Validation loss: 0.772.. \n",
      "AUC: 0.71 PR AUC: 0.26 \n",
      "\n",
      " Epoch [18] out of 80\n",
      "Training loss: 1.165.. Validation loss: 0.775.. \n",
      "AUC: 0.67 PR AUC: 0.25 \n",
      "\n",
      " Epoch [19] out of 80\n",
      "Training loss: 1.166.. Validation loss: 0.771.. \n",
      "AUC: 0.74 PR AUC: 0.25 \n",
      "\n",
      " Epoch [20] out of 80\n",
      "Training loss: 1.166.. Validation loss: 0.774.. \n",
      "AUC: 0.71 PR AUC: 0.26 \n",
      "\n",
      " Epoch [21] out of 80\n",
      "Training loss: 1.165.. Validation loss: 0.772.. \n",
      "AUC: 0.76 PR AUC: 0.25 \n",
      "\n",
      " Epoch [22] out of 80\n",
      "Training loss: 1.165.. Validation loss: 0.773.. \n",
      "AUC: 0.76 PR AUC: 0.25 \n",
      "\n",
      " Epoch [23] out of 80\n",
      "Training loss: 1.165.. Validation loss: 0.774.. \n",
      "AUC: 0.74 PR AUC: 0.25 \n",
      "\n",
      " Epoch [24] out of 80\n",
      "Training loss: 1.165.. Validation loss: 0.774.. \n",
      "AUC: 0.76 PR AUC: 0.25 \n",
      "\n",
      " Epoch [25] out of 80\n",
      "Training loss: 1.165.. Validation loss: 0.774.. \n",
      "AUC: 0.75 PR AUC: 0.25 \n",
      "\n",
      " Epoch [26] out of 80\n",
      "Training loss: 1.165.. Validation loss: 0.774.. \n",
      "AUC: 0.76 PR AUC: 0.25 \n",
      "\n",
      " Epoch [27] out of 80\n",
      "Training loss: 1.165.. Validation loss: 0.774.. \n",
      "AUC: 0.76 PR AUC: 0.25 \n",
      "\n",
      " Epoch [28] out of 80\n",
      "Training loss: 1.165.. Validation loss: 0.775.. \n",
      "AUC: 0.77 PR AUC: 0.25 \n",
      "\n",
      " Epoch [29] out of 80\n",
      "Training loss: 1.164.. Validation loss: 0.774.. \n",
      "AUC: 0.74 PR AUC: 0.25 \n",
      "\n",
      " Epoch [30] out of 80\n",
      "Training loss: 1.165.. Validation loss: 0.775.. \n",
      "AUC: 0.76 PR AUC: 0.25 \n",
      "\n",
      " Epoch [31] out of 80\n",
      "Training loss: 1.164.. Validation loss: 0.774.. \n",
      "AUC: 0.78 PR AUC: 0.25 \n",
      "\n",
      " Epoch [32] out of 80\n",
      "Training loss: 1.164.. Validation loss: 0.775.. \n",
      "AUC: 0.73 PR AUC: 0.26 \n",
      "\n",
      " Epoch [33] out of 80\n",
      "Training loss: 1.164.. Validation loss: 0.776.. \n",
      "AUC: 0.71 PR AUC: 0.26 \n",
      "\n",
      " Epoch [34] out of 80\n",
      "Training loss: 1.164.. Validation loss: 0.775.. \n",
      "AUC: 0.73 PR AUC: 0.26 \n",
      "\n",
      " Epoch [35] out of 80\n",
      "Training loss: 1.163.. Validation loss: 0.772.. \n",
      "AUC: 0.81 PR AUC: 0.25 \n",
      "\n",
      " Epoch [36] out of 80\n",
      "Training loss: 1.164.. Validation loss: 0.773.. \n",
      "AUC: 0.77 PR AUC: 0.25 \n",
      "\n",
      " Epoch [37] out of 80\n",
      "Training loss: 1.164.. Validation loss: 0.778.. \n",
      "AUC: 0.69 PR AUC: 0.26 \n",
      "\n",
      " Epoch [38] out of 80\n",
      "Training loss: 1.163.. Validation loss: 0.775.. \n",
      "AUC: 0.71 PR AUC: 0.26 \n",
      "\n",
      " Epoch [39] out of 80\n",
      "Training loss: 1.162.. Validation loss: 0.772.. \n",
      "AUC: 0.76 PR AUC: 0.25 \n",
      "\n",
      " Epoch [40] out of 80\n",
      "Training loss: 1.162.. Validation loss: 0.772.. \n",
      "AUC: 0.74 PR AUC: 0.24 \n",
      "\n",
      " Epoch [41] out of 80\n",
      "Training loss: 1.162.. Validation loss: 0.771.. \n",
      "AUC: 0.75 PR AUC: 0.25 \n",
      "\n",
      " Epoch [42] out of 80\n",
      "Training loss: 1.162.. Validation loss: 0.771.. \n",
      "AUC: 0.75 PR AUC: 0.25 \n",
      "\n",
      " Epoch [43] out of 80\n",
      "Training loss: 1.161.. Validation loss: 0.770.. \n",
      "AUC: 0.78 PR AUC: 0.25 \n",
      "\n",
      " Epoch [44] out of 80\n",
      "Training loss: 1.162.. Validation loss: 0.770.. \n",
      "AUC: 0.73 PR AUC: 0.26 \n",
      "\n",
      " Epoch [45] out of 80\n",
      "Training loss: 1.161.. Validation loss: 0.770.. \n",
      "AUC: 0.77 PR AUC: 0.25 \n",
      "\n",
      " Epoch [46] out of 80\n",
      "Training loss: 1.161.. Validation loss: 0.770.. \n",
      "AUC: 0.81 PR AUC: 0.25 \n",
      "\n",
      " Epoch [47] out of 80\n",
      "Training loss: 1.160.. Validation loss: 0.769.. \n",
      "AUC: 0.82 PR AUC: 0.25 \n",
      "\n",
      " Epoch [48] out of 80\n",
      "Training loss: 1.160.. Validation loss: 0.768.. \n",
      "AUC: 0.80 PR AUC: 0.25 \n",
      "\n",
      " Epoch [49] out of 80\n",
      "Training loss: 1.160.. Validation loss: 0.769.. \n",
      "AUC: 0.72 PR AUC: 0.27 \n",
      "\n",
      " Epoch [50] out of 80\n",
      "Training loss: 1.160.. Validation loss: 0.768.. \n",
      "AUC: 0.81 PR AUC: 0.25 \n",
      "\n",
      " Epoch [51] out of 80\n",
      "Training loss: 1.160.. Validation loss: 0.768.. \n",
      "AUC: 0.83 PR AUC: 0.26 \n",
      "\n",
      " Epoch [52] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.767.. \n",
      "AUC: 0.82 PR AUC: 0.25 \n",
      "\n",
      " Epoch [53] out of 80\n",
      "Training loss: 1.160.. Validation loss: 0.766.. \n",
      "AUC: 0.76 PR AUC: 0.27 \n",
      "\n",
      " Epoch [54] out of 80\n",
      "Training loss: 1.160.. Validation loss: 0.766.. \n",
      "AUC: 0.75 PR AUC: 0.27 \n",
      "\n",
      " Epoch [55] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.766.. \n",
      "AUC: 0.77 PR AUC: 0.27 \n",
      "\n",
      " Epoch [56] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.766.. \n",
      "AUC: 0.83 PR AUC: 0.26 \n",
      "\n",
      " Epoch [57] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.765.. \n",
      "AUC: 0.80 PR AUC: 0.25 \n",
      "\n",
      " Epoch [58] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.765.. \n",
      "AUC: 0.80 PR AUC: 0.25 \n",
      "\n",
      " Epoch [59] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.765.. \n",
      "AUC: 0.75 PR AUC: 0.27 \n",
      "\n",
      " Epoch [60] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.766.. \n",
      "AUC: 0.83 PR AUC: 0.26 \n",
      "\n",
      " Epoch [61] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.765.. \n",
      "AUC: 0.77 PR AUC: 0.27 \n",
      "\n",
      " Epoch [62] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.765.. \n",
      "AUC: 0.81 PR AUC: 0.25 \n",
      "\n",
      " Epoch [63] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.764.. \n",
      "AUC: 0.77 PR AUC: 0.27 \n",
      "\n",
      " Epoch [64] out of 80\n",
      "Training loss: 1.158.. Validation loss: 0.765.. \n",
      "AUC: 0.71 PR AUC: 0.27 \n",
      "\n",
      " Epoch [65] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.767.. \n",
      "AUC: 0.69 PR AUC: 0.26 \n",
      "\n",
      " Epoch [66] out of 80\n",
      "Training loss: 1.158.. Validation loss: 0.764.. \n",
      "AUC: 0.84 PR AUC: 0.26 \n",
      "\n",
      " Epoch [67] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.763.. \n",
      "AUC: 0.80 PR AUC: 0.25 \n",
      "\n",
      " Epoch [68] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.764.. \n",
      "AUC: 0.81 PR AUC: 0.25 \n",
      "\n",
      " Epoch [69] out of 80\n",
      "Training loss: 1.160.. Validation loss: 0.766.. \n",
      "AUC: 0.70 PR AUC: 0.27 \n",
      "\n",
      " Epoch [70] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.766.. \n",
      "AUC: 0.70 PR AUC: 0.27 \n",
      "\n",
      " Epoch [71] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.763.. \n",
      "AUC: 0.79 PR AUC: 0.25 \n",
      "\n",
      " Epoch [72] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.763.. \n",
      "AUC: 0.74 PR AUC: 0.27 \n",
      "\n",
      " Epoch [73] out of 80\n",
      "Training loss: 1.157.. Validation loss: 0.762.. \n",
      "AUC: 0.80 PR AUC: 0.26 \n",
      "\n",
      " Epoch [74] out of 80\n",
      "Training loss: 1.158.. Validation loss: 0.762.. \n",
      "AUC: 0.82 PR AUC: 0.25 \n",
      "\n",
      " Epoch [75] out of 80\n",
      "Training loss: 1.158.. Validation loss: 0.762.. \n",
      "AUC: 0.83 PR AUC: 0.25 \n",
      "\n",
      " Epoch [76] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.762.. \n",
      "AUC: 0.82 PR AUC: 0.25 \n",
      "\n",
      " Epoch [77] out of 80\n",
      "Training loss: 1.159.. Validation loss: 0.762.. \n",
      "AUC: 0.77 PR AUC: 0.27 \n",
      "\n",
      " Epoch [78] out of 80\n",
      "Training loss: 1.158.. Validation loss: 0.762.. \n",
      "AUC: 0.74 PR AUC: 0.26 \n",
      "\n",
      " Epoch [79] out of 80\n",
      "Training loss: 1.158.. Validation loss: 0.761.. \n",
      "AUC: 0.80 PR AUC: 0.25 \n",
      "\n",
      " Epoch [80] out of 80\n",
      "Training loss: 1.158.. Validation loss: 0.761.. \n",
      "AUC: 0.78 PR AUC: 0.27 \n",
      "Finished Training\n",
      "starttime = 2021-01-04 17:09:35.312748\n",
      "endtime = 2021-01-04 20:51:53.324512\n",
      "\n",
      " Last model \n",
      "\n",
      "Accuracy for Classifier 0.544\n",
      "Area Under ROC Curve: 0.639\n",
      "Area Under PR Curve(AP): 0.793\n",
      "[[1893  106]\n",
      " [2073  704]]\n",
      "Brier score: 0.236\n",
      "\n",
      " Best model \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Classifier 0.597\n",
      "Area Under ROC Curve: 0.666\n",
      "Area Under PR Curve(AP): 0.795\n",
      "[[1847  152]\n",
      " [1773 1004]]\n",
      "Brier score: 0.227\n"
     ]
    }
   ],
   "source": [
    "def build_graphs (y_test,pred_probabilities, classifier_name, plot_name):\n",
    "    \n",
    "    def bin_total(y_true, y_prob, n_bins):\n",
    "        bins = np.linspace(0., 1. + 1e-8, n_bins + 1)\n",
    "\n",
    "        # In sklearn.calibration.calibration_curve, the last value in the array is always 0.\n",
    "        binids = np.digitize(y_prob, bins) - 1\n",
    "\n",
    "        return np.bincount(binids, minlength=len(bins))\n",
    "\n",
    "    def missing_bin(bin_array):\n",
    "        midpoint = \" \"    \n",
    "        if bin_array[0]==0:\n",
    "            midpoint = \"5%, \"\n",
    "        if bin_array[1]==0:\n",
    "            midpoint = midpoint + \"15%, \"\n",
    "        if bin_array[2]==0:\n",
    "            midpoint = midpoint + \"25%, \"\n",
    "        if bin_array[3]==0:\n",
    "            midpoint = midpoint + \"35%, \" \n",
    "        if bin_array[4]==0:\n",
    "            midpoint = midpoint + \"45%, \"\n",
    "        if bin_array[5]==0:\n",
    "            midpoint = midpoint + \"55%, \"\n",
    "        if bin_array[6]==0:\n",
    "            midpoint = midpoint + \"65%, \"\n",
    "        if bin_array[7]==0:\n",
    "            midpoint = midpoint + \"75%, \"\n",
    "        if bin_array[8]==0:\n",
    "            midpoint = midpoint + \"85%, \"\n",
    "        if bin_array[9]==0:\n",
    "            midpoint = midpoint + \"95%, \"\n",
    "        return \"The missing bins have midpoint values of \"+ str(midpoint)\n",
    "    \n",
    "    # performance\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    # compute roc auc\n",
    "    roc_auc = roc_auc_score(y_test, pred_probabilities, average = 'micro')\n",
    "    # compute Precision_Recall curves\n",
    "    precision, recall, _ = precision_recall_curve(y_test, pred_probabilities)\n",
    "    # compute PR_AUC\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "\n",
    "    # compute calibration curve\n",
    "    print(\"compute calibration curve\")\n",
    "    LR_y, LR_x = calibration_curve(y_test, pred_probabilities, n_bins=10)\n",
    "    #find out which one are the missing bins\n",
    "    bin_array = bin_total(y_test, pred_probabilities , n_bins=10)\n",
    "    print(missing_bin(bin_array))\n",
    "\n",
    "    print(\"plot curves and save in one png file\")\n",
    "    #save three plots in one png file\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, figsize=(7, 24))\n",
    "    fig.subplots_adjust(wspace=0.3, hspace= 0.3)\n",
    "    fig.suptitle('Evaluation of '+ plot_name)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, pred_probabilities)\n",
    "    \n",
    "    # plot roc curve\n",
    "    ax1.plot(fpr, tpr, label=\"Classifier \" + str(classifier_name) + \", auc=\" +str(roc_auc))\n",
    "    ax1.title.set_text('ROC AUC')\n",
    "    ax1.set(xlabel='False Positive Rate', ylabel='True Positive Rate')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "\n",
    "    # plot PR curve\n",
    "    ax2.plot(recall, precision, label=\"Classifier \" + str(classifier_name) + \", auc=\"+str(pr_auc))\n",
    "    ax2.title.set_text('PR AUC')\n",
    "    ax2.set(xlabel='Recall', ylabel='Precision')\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "\n",
    "    # plot calibration curve\n",
    "    ax3.plot(LR_x, LR_y, marker='o', linewidth=1, label='LR')\n",
    "    line = mlines.Line2D([0, 1], [0, 1], color='black')\n",
    "    transform = ax3.transAxes\n",
    "    line.set_transform(transform)\n",
    "    ax3.add_line(line)\n",
    "    ax3.title.set_text('Calibration plot for '+str(plot_name))\n",
    "    ax3.set(xlabel= 'Predicted probability', ylabel= 'True probability in each bin')\n",
    "    ax3.legend(loc=\"lower right\")\n",
    "\n",
    "    plt.savefig(plot_name+\".png\")\n",
    "    plt.show()\n",
    "    \n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
