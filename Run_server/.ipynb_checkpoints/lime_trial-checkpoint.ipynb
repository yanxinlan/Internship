{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbf60d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "TESTING = False # Use 1% of data for testing. IF false - full dataset takes more time\n",
    "MAX_FEATURE_SET = True\n",
    "#DIAGNOSIS = False\n",
    "IMPUTE_EACH_ID = False # imputation within each icustay_id\n",
    "IMPUTE_COLUMN = False # imputation based on whole column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b65e2bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-25 09:25:27.320308\n"
     ]
    }
   ],
   "source": [
    "#data preprocessing\n",
    "from datetime import datetime\n",
    "print(datetime.now())\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "#import time\n",
    "from datetime import datetime\n",
    "\n",
    "#logistic regression model: train and evaluation and XGB \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, f1_score, roc_auc_score, auc, accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f689437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the full files pathes are here\n",
    "# time dependent\n",
    "DATA_PATH_stages=\"../data/kdigo_stages_measured.csv\" \n",
    "DATA_PATH_labs = \"../data/labs-kdigo_stages_measured.csv\" \n",
    "DATA_PATH_vitals = \"../data/vitals-kdigo_stages_measured.csv\" \n",
    "DATA_PATH_vents = \"../data/vents-vasopressor-sedatives-kdigo_stages_measured.csv\"\n",
    "#no time dependent\n",
    "DATA_PATH_detail=\"../data/icustay_detail-kdigo_stages_measured.csv\" #age constraint\n",
    "#DATA_PATH_icd = \"../data/diagnoses_icd_aki_measured.csv\" #AL was \"...measured 2.csv\"\n",
    "\n",
    "SEPARATOR=\";\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebafac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameter as constant \n",
    "\n",
    "#which classifier to use, only run one classifier at one time \n",
    "ALL_STAGES = False\n",
    "CLASS1 = True   #AnyAKI\n",
    "#CLASS2 = True    #ModerateSevereAKI\n",
    "#CLASS3 = False    #SevereAKI\n",
    "\n",
    "MAX_DAYS = 35\n",
    "\n",
    "NORMALIZATION = 'min-max' \n",
    "\n",
    "CAPPING_THRESHOLD_UPPER = 0.99\n",
    "CAPPING_THRESHOLD_LOWER = 0.01\n",
    "\n",
    "#Age constraints: adults\n",
    "ADULTS_MIN_AGE = 18\n",
    "ADULTS_MAX_AGE = -1\n",
    "\n",
    "\n",
    "SPLIT_SIZE = 0.2\n",
    "\n",
    "#Two options to deal with time series data\n",
    "FIRST_TURN_POS = True #True #False # the first charttime that turn pos as bechmark for Target\n",
    "LAST_CHARTTIME = False #the last charttime as bechmark for Target\n",
    "\n",
    "# resampling or not\n",
    "TIME_SAMPLING = True \n",
    "SAMPLING_INTERVAL = '6H'\n",
    "RESAMPLE_LIMIT = 16 # 4 days*6h interval\n",
    "MOST_COMMON = False #resampling with most common\n",
    "# if MOST_COMMON is not applied,sampling with different strategies per kind of variable, \n",
    "# numeric variables use mean value, categorical variables use max value\n",
    "\n",
    "IMPUTE_METHOD = 'most_frequent' \n",
    "FILL_VALUE = 0 #fill missing value and ragged part of 3d array\n",
    "\n",
    "# How much time the prediction should occur (hours)\n",
    "HOURS_AHEAD = 48\n",
    "\n",
    "# precentage of dataset to be used for testing model\n",
    "TEST_SIZE = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cce884df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set changable info corresponding to each classifier as variables\n",
    "\n",
    "min_set =  [\"icustay_id\", \"charttime\", \"creat\", \"uo_rt_6hr\", \"uo_rt_12hr\", \"uo_rt_24hr\", \"aki_stage\"]\n",
    "\n",
    "max_set = ['icustay_id', 'charttime', 'aki_stage', 'hadm_id','aniongap_avg', 'bicarbonate_avg', \n",
    "           'bun_avg','chloride_avg', 'creat', 'diasbp_mean', 'glucose_avg', 'heartrate_mean',\n",
    "           'hematocrit_avg', 'hemoglobin_avg', 'potassium_avg', 'resprate_mean','sodium_avg', 'spo2_mean', 'sysbp_mean', \n",
    "           'uo_rt_12hr', 'uo_rt_24hr','uo_rt_6hr', 'wbc_avg', 'sedative', 'vasopressor', 'vent', 'age', 'F','M', \n",
    "           'asian', 'black', 'hispanic', 'native', 'other', 'unknown','white', 'ELECTIVE', 'EMERGENCY', 'URGENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e014a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions used later\n",
    "\n",
    "\n",
    "def cap_data(df):\n",
    "    print(\"Capping between the {} and {} quantile\".format(CAPPING_THRESHOLD_LOWER, CAPPING_THRESHOLD_UPPER))\n",
    "    cap_mask = df.columns.difference(['icustay_id', 'charttime', 'aki_stage'])\n",
    "    df[cap_mask] = df[cap_mask].clip(df[cap_mask].quantile(CAPPING_THRESHOLD_LOWER),\n",
    "                                     df[cap_mask].quantile(CAPPING_THRESHOLD_UPPER),\n",
    "                                     axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# impute missing value in resampleing data with most common based on each id\n",
    "def fast_mode(df, key_cols, value_col):\n",
    "    \"\"\" Calculate a column mode, by group, ignoring null values. \n",
    "    \n",
    "    key_cols : list of str - Columns to groupby for calculation of mode.\n",
    "    value_col : str - Column for which to calculate the mode. \n",
    "\n",
    "    Return\n",
    "    pandas.DataFrame\n",
    "        One row for the mode of value_col per key_cols group. If ties, returns the one which is sorted first. \"\"\"\n",
    "    return (df.groupby(key_cols + [value_col]).size() \n",
    "              .to_frame('counts').reset_index() \n",
    "              .sort_values('counts', ascending=False) \n",
    "              .drop_duplicates(subset=key_cols)).drop('counts',axis=1)\n",
    "\n",
    "def normalise_data(df, norm_mask):\n",
    "    print(\"Normalizing in [0,1] with {} normalization\".format(NORMALIZATION))\n",
    "    \n",
    "    df[norm_mask] = (df[norm_mask] - df[norm_mask].min()) / (df[norm_mask].max() - df[norm_mask].min())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "#get max shape of 3d array\n",
    "def get_dimensions(array, level=0):   \n",
    "    yield level, len(array)\n",
    "    try:\n",
    "        for row in array:\n",
    "            yield from get_dimensions(row, level + 1)\n",
    "    except TypeError: #not an iterable\n",
    "        pass\n",
    "\n",
    "def get_max_shape(array):\n",
    "    dimensions = defaultdict(int)\n",
    "    for level, length in get_dimensions(array):\n",
    "        dimensions[level] = max(dimensions[level], length)\n",
    "    return [value for _, value in sorted(dimensions.items())]\n",
    "\n",
    "#pad the ragged 3d array to rectangular shape based on max size\n",
    "def iterate_nested_array(array, index=()):\n",
    "    try:\n",
    "        for idx, row in enumerate(array):\n",
    "            yield from iterate_nested_array(row, (*index, idx)) \n",
    "    except TypeError: # final level            \n",
    "        yield (*index, slice(len(array))), array # think of the types\n",
    "\n",
    "def pad(array, fill_value):\n",
    "    dimensions = get_max_shape(array)\n",
    "    result = np.full(dimensions, fill_value, dtype = np.float64)  \n",
    "    for index, value in iterate_nested_array(array):\n",
    "        result[index] = value \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fa00d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read csv files\n",
      "convert charttime to timestamp\n"
     ]
    }
   ],
   "source": [
    "print(\"read csv files\")\n",
    "#reading csv files\n",
    "X = pd.read_csv(DATA_PATH_stages, sep= SEPARATOR)\n",
    "X.drop([\"aki_stage_creat\", \"aki_stage_uo\"], axis = 1, inplace = True)\n",
    "#remove totally empty rows \n",
    "X = X.dropna(how = 'all', subset = ['creat','uo_rt_6hr','uo_rt_12hr','uo_rt_24hr','aki_stage'])\n",
    "print(\"convert charttime to timestamp\")\n",
    "X['charttime'] = pd.to_datetime(X['charttime'])\n",
    "#merge rows if they have exact timestamp within same icustay_id AL: t substitutes missing values with zero\n",
    "#X = X.groupby(['icustay_id', 'charttime']).sum().reset_index(['icustay_id', 'charttime'])\n",
    "\n",
    "dataset_detail = pd.read_csv(DATA_PATH_detail, sep= SEPARATOR)  #age constraint\n",
    "dataset_detail.drop(['dod', 'admittime','dischtime', 'los_hospital','ethnicity','hospital_expire_flag', 'hospstay_seq',\n",
    "       'first_hosp_stay', 'intime', 'outtime', 'los_icu', 'icustay_seq','first_icu_stay'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90b6141a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert charttime to timestamp\n"
     ]
    }
   ],
   "source": [
    "dataset_labs = pd.read_csv(DATA_PATH_labs, sep= SEPARATOR) # 'bands lactate platelet ptt inr pt\n",
    "dataset_labs.drop(['albumin_min', 'albumin_max','bilirubin_min', 'bilirubin_max','bands_min', 'bands_max',\n",
    "                   'lactate_min', 'lactate_max','platelet_min', 'platelet_max','ptt_min', 'ptt_max', \n",
    "                   'inr_min', 'inr_max', 'pt_min', 'pt_max'], axis = 1, inplace = True)\n",
    "dataset_labs = dataset_labs.dropna(subset=['charttime'])\n",
    "dataset_labs = dataset_labs.dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "dataset_labs['charttime'] = pd.to_datetime(dataset_labs['charttime'])\n",
    "dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'])\n",
    "\n",
    "if  MAX_FEATURE_SET:\n",
    "    dataset_vitals = pd.read_csv(DATA_PATH_vitals, sep= SEPARATOR)  \n",
    "    dataset_vents = pd.read_csv(DATA_PATH_vents , sep= SEPARATOR)\n",
    "    #dataset_icd = pd.read_csv(DATA_PATH_icd, sep= SEPARATOR)\n",
    "\n",
    "    dataset_vitals.drop([\"heartrate_min\", \"heartrate_max\",\"sysbp_min\", \"sysbp_max\",\"diasbp_min\", \"diasbp_max\",\n",
    "                        'meanbp_min','meanbp_max', 'meanbp_mean','tempc_min', 'tempc_max', 'tempc_mean',\n",
    "                        \"resprate_min\", \"resprate_max\", \"spo2_min\", \"spo2_max\", \"glucose_min\", \"glucose_max\"], axis = 1, inplace = True)\n",
    "          \n",
    "    print(\"convert charttime to timestamp\")\n",
    "    dataset_vitals['charttime'] = pd.to_datetime(dataset_vitals['charttime'])\n",
    "    dataset_vents['charttime'] = pd.to_datetime(dataset_vents['charttime'])\n",
    "    \n",
    "    dataset_vitals = dataset_vitals.sort_values(by=['icustay_id', 'charttime'])\n",
    "    dataset_vents = dataset_vents.sort_values(by=['icustay_id', 'charttime'])\n",
    "    \n",
    "    # AL drop those where all columns are nan\n",
    "    dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')   \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53dadc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labs file: instead of min and max their avg\n",
    "counter = 0\n",
    "col1 = 4\n",
    "col2 = 5\n",
    "null_l = [] # no null values in those that are different\n",
    "changed = 0 # 4316 records changed to avg\n",
    "\n",
    "while counter < 11:\n",
    "    row = 0\n",
    "# find where min and max are different and save their row indices \n",
    "    while row < len(dataset_labs):\n",
    "        a = dataset_labs.iloc[row,col1]\n",
    "        b = dataset_labs.iloc[row,col2]\n",
    "        if a==b or (np.isnan(a) and np.isnan(b)):\n",
    "            pass\n",
    "        elif a!=b:\n",
    "            changed +=1\n",
    "            avg = (a+b)/2\n",
    "            dataset_labs.iloc[row,col1] = avg\n",
    "            if (np.isnan(a) and ~np.isnan(b)) or (np.isnan(b) and ~np.isnan(a)):\n",
    "                null_l.append(row)\n",
    "        else:\n",
    "            print(a)\n",
    "            print(b)\n",
    "        row +=1       \n",
    "    # delete the redundant column max, update counters\n",
    "    dataset_labs.drop(dataset_labs.columns[col2], axis=1, inplace = True)\n",
    "    counter = counter+1\n",
    "    col1 = col1+1\n",
    "    col2 = col2+1\n",
    "\n",
    "dataset_labs.columns = ['subject_id','hadm_id', 'icustay_id', 'charttime', 'aniongap_avg', 'bicarbonate_avg', \n",
    "                        'creatinine_avg', 'chloride_avg', 'glucose_avg', 'hematocrit_avg','hemoglobin_avg',\n",
    "                        'potassium_avg', 'sodium_avg', 'bun_avg', 'wbc_avg']\n",
    "if len(null_l)>0:\n",
    "    print(\"null values encountered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40ba508d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge creatinine and glucose.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    }
   ],
   "source": [
    "print(\"Merge creatinine and glucose.\")\n",
    "# merge creatinine from labs and set with labels\n",
    "creat_l = dataset_labs[['icustay_id','charttime','creatinine_avg']].copy()\n",
    "creat_l = creat_l.dropna(subset=['creatinine_avg'])\n",
    "creat = X[['icustay_id','charttime', 'creat']].copy()\n",
    "creat = creat.dropna(subset=['creat'])\n",
    "creat_l = creat_l.rename(columns={\"creatinine_avg\": \"creat\"})\n",
    "creat = creat.append(creat_l, ignore_index=True)\n",
    "creat.drop_duplicates(inplace = True)\n",
    "#delete old columns\n",
    "dataset_labs.drop([\"creatinine_avg\"], axis = 1, inplace = True)\n",
    "dataset_labs = dataset_labs.dropna(subset=dataset_labs.columns[4:], how='all')\n",
    "X.drop([\"creat\"], axis = 1, inplace = True)\n",
    "#merge new column\n",
    "X = pd.merge(X, creat, on = [\"icustay_id\", \"charttime\"], sort = True, how= \"outer\", copy = False)\n",
    "\n",
    "if MAX_FEATURE_SET:\n",
    "    # merge glucose from vitals and labs\n",
    "    glucose_v = dataset_vitals[['subject_id','hadm_id','icustay_id','charttime', 'glucose_mean']].copy()\n",
    "    glucose_v = glucose_v.dropna(subset=['glucose_mean'])\n",
    "    glucose = dataset_labs[['subject_id','hadm_id','icustay_id','charttime', 'glucose_avg']].copy()\n",
    "    glucose = glucose.dropna(subset=['glucose_avg'])\n",
    "    glucose_v = glucose_v.rename(columns={\"glucose_mean\": \"glucose_avg\"})\n",
    "    glucose = glucose.append(glucose_v, ignore_index=True)\n",
    "    glucose.drop_duplicates(inplace = True)\n",
    "    #delete old columns\n",
    "    dataset_labs.drop([\"glucose_avg\"], axis = 1, inplace = True)\n",
    "    dataset_vitals.drop([\"glucose_mean\"], axis = 1, inplace = True)\n",
    "    dataset_vitals = dataset_vitals.dropna(subset=dataset_vitals.columns[4:], how='all')\n",
    "    #merge new column\n",
    "    dataset_labs = pd.merge(dataset_labs, glucose, on = ['subject_id','hadm_id','icustay_id','charttime',], sort = True, how= \"outer\", copy = False)\n",
    "    \n",
    "dataset_labs = dataset_labs.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a3b86e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging labs, vitals and vents files\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging labs, vitals and vents files\")\n",
    "#merge files with time-dependent data, based on icustay_id and charttime\n",
    "if MAX_FEATURE_SET:\n",
    "    X = pd.merge(X, dataset_labs, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False)\n",
    "    X = pd.merge(X, dataset_vitals, on = [\"icustay_id\", \"charttime\",\"subject_id\", \"hadm_id\"], how= \"outer\", copy = False)\n",
    "    X = pd.merge(X, dataset_vents, on = [\"icustay_id\", \"charttime\"], how= \"outer\", copy = False) \n",
    "    X.drop([\"subject_id\"], axis = 1, inplace = True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35a5eb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start preprocessing time dependent data\n",
      "Removing patients under the min age\n"
     ]
    }
   ],
   "source": [
    "print(\"start preprocessing time dependent data\") \n",
    "print(\"Removing patients under the min age\")\n",
    "dataset_detail = dataset_detail.loc[dataset_detail['age'] >= ADULTS_MIN_AGE]\n",
    "adults_icustay_id_list = dataset_detail['icustay_id'].unique()\n",
    "X = X[X.icustay_id.isin(adults_icustay_id_list)].sort_values(by=['icustay_id'], ignore_index = True)\n",
    "X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "adults_icustay_id_list = np.sort(adults_icustay_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8de60ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop icustay_id with time span less than 48hrs\n",
      "2302 long stays\n",
      "there are 5214 id-s shorter than 48 hours\n"
     ]
    }
   ],
   "source": [
    "print(\"drop icustay_id with time span less than 48hrs\")\n",
    "def more_than_HOURS_ahead(adults_icustay_id_list, X):\n",
    "    drop_list = []\n",
    "    los_list = [] # calculating LOS ICU based on charttime\n",
    "    long_stays_id = [] # LOS longer than MAX DAYS days\n",
    "    last_charttime_list = []\n",
    "    seq_length = X.groupby(['icustay_id'],as_index=False).size()\n",
    "    id_count = 0\n",
    "    first_row_index = 0\n",
    "\n",
    "    while id_count < len(adults_icustay_id_list):\n",
    "        icustay_id = adults_icustay_id_list[id_count]\n",
    "        last_row_index = first_row_index + seq_length.iloc[id_count,1]-1\n",
    "        first_time = X.iat[first_row_index, X.columns.get_loc('charttime')]\n",
    "        last_time = X.iat[last_row_index, X.columns.get_loc('charttime')]\n",
    "        los = round(float((last_time - first_time).total_seconds()/60/60/24),4) # in days\n",
    "        if los < HOURS_AHEAD/24:\n",
    "            drop_list.append(icustay_id)\n",
    "        else:\n",
    "            los_list.append(los)\n",
    "            if los > MAX_DAYS:\n",
    "                long_stays_id.append(icustay_id)\n",
    "                last_charttime_list.append(last_time)\n",
    "        # udpate for the next icustay_id\n",
    "        first_row_index = last_row_index+1\n",
    "        id_count +=1\n",
    "    if len(long_stays_id) != len(last_charttime_list):\n",
    "        print('ERROR')\n",
    "    print(\"%d long stays\" % len(long_stays_id))\n",
    "    # drop all the rows with the saved icustay_id\n",
    "    print(\"there are %d id-s shorter than 48 hours\" % len(drop_list))\n",
    "    X = X[~X.icustay_id.isin(drop_list)]\n",
    "    id_list = X['icustay_id'].unique()\n",
    "    X = X.sort_values(by=['icustay_id', 'charttime'], ignore_index = True)\n",
    "    \n",
    "    return id_list, X, long_stays_id,last_charttime_list\n",
    "\n",
    "id_list, X, long_stays_id,last_charttime_list  = more_than_HOURS_ahead(adults_icustay_id_list, X)\n",
    "\n",
    "\n",
    "long = pd.DataFrame()\n",
    "long['icustay_id']  = long_stays_id\n",
    "long['last_time']  = last_charttime_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e62c26fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 long stays\n",
      "there are 1 id-s shorter than 48 hours\n"
     ]
    }
   ],
   "source": [
    "# deleting rows that are not within MAX_DAYS (35) period\n",
    "i = 0 # long df index\n",
    "drop_long_time = []\n",
    "    \n",
    "while i < len(long_stays_id):\n",
    "    j = 0\n",
    "    all_rows = X.index[X['icustay_id'] == long.loc[i,'icustay_id']].tolist()\n",
    "    while j < len(all_rows):\n",
    "        time = X.iat[all_rows[j], X.columns.get_loc('charttime')]\n",
    "        # if keep last MAX_DAYS \n",
    "        if (long.loc[i,'last_time'] - time).total_seconds() > MAX_DAYS*24*60*60:\n",
    "            drop_long_time.append(all_rows[j])\n",
    "            j +=1\n",
    "        else:\n",
    "            break\n",
    "    i +=1       \n",
    "X.drop(X.index[drop_long_time], inplace=True) \n",
    "\n",
    "# checking for 48h min length again\n",
    "id_list, X, long_stays_id,last_charttime_list  = more_than_HOURS_ahead(id_list, X)\n",
    "dataset_detail = dataset_detail[dataset_detail.icustay_id.isin(id_list)].sort_values(by=['icustay_id'], ignore_index = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54af63bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing purpose, use small amount of data first\n",
    "if TESTING:\n",
    "    rest, id_list = train_test_split(id_list, test_size= 0.01, random_state=42)\n",
    "    X = X[X.icustay_id.isin(id_list)].sort_values(by=['icustay_id'])\n",
    "    dataset_detail = dataset_detail[dataset_detail.icustay_id.isin(id_list)].sort_values(by=['icustay_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d03ee977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resampling: MEAN & ZERO\n"
     ]
    }
   ],
   "source": [
    "if TIME_SAMPLING:\n",
    "    print(\"resampling: MEAN & ZERO\")\n",
    "    # Sampling with different strategies per kind of variable\n",
    "    label = ['aki_stage']\n",
    "    skip = ['icustay_id', 'charttime', 'aki_stage']\n",
    "    if MAX_FEATURE_SET:\n",
    "        discrete_feat = ['sedative', 'vasopressor', 'vent', 'hadm_id']\n",
    "        skip.extend(discrete_feat)    \n",
    "    # all features that are not in skip are numeric\n",
    "    numeric_feat = list(X.columns.difference(skip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c8c5b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = X # without resample or imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ff0897d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging sampled features\n",
      "(2156523, 26)\n"
     ]
    }
   ],
   "source": [
    "# Applying aggregation to features depending on their type\n",
    "X = X.set_index('charttime').groupby('icustay_id').resample(SAMPLING_INTERVAL)\n",
    "if MAX_FEATURE_SET:\n",
    "    X_discrete = X[discrete_feat].max().fillna(FILL_VALUE).astype(np.int64)\n",
    "X_numeric = X[numeric_feat].mean() \n",
    "X_label = X['aki_stage'].max()\n",
    "print(\"Merging sampled features\")\n",
    "try:\n",
    "    X = pd.concat([X_numeric, X_discrete,X_label], axis=1).reset_index()\n",
    "except:\n",
    "    X = pd.concat([X_numeric,X_label], axis=1).reset_index()\n",
    "print(X.shape)\n",
    "#Label forward fill\n",
    "X['aki_stage'] = X['aki_stage'].ffill(limit=RESAMPLE_LIMIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3487b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = X # after resample but without imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ce91583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation.\n"
     ]
    }
   ],
   "source": [
    "print(\"Imputation.\")\n",
    "# do imputation of label with zero if there are still missing values\n",
    "X['aki_stage'] = X['aki_stage'].fillna(0)\n",
    "# using most common within each icustay_id\n",
    "if IMPUTE_EACH_ID:\n",
    "    # set a new variable so won't change the orginial X\n",
    "    column_name = list(X.columns)\n",
    "    column_name.remove(column_name[0]) \n",
    "    for feature in column_name:\n",
    "        X.loc[X[feature].isnull(), feature] = X.icustay_id.map(fast_mode(X, ['icustay_id'], feature).set_index('icustay_id')[feature])       \n",
    "\n",
    "# imputation based on whole column\n",
    "if IMPUTE_COLUMN:\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy= IMPUTE_METHOD)\n",
    "    cols = list(X.columns)\n",
    "    cols = cols[2:23]\n",
    "    X[cols]=imp.fit_transform(X[cols])  \n",
    "\n",
    "# If no imputation method selected or only impute each id, for the remaining nan impute direclty with FILL_VALUE\n",
    "X = X.fillna(FILL_VALUE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d28ac816",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3 = X # with both resamaple and imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63512f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputation.\n"
     ]
    }
   ],
   "source": [
    "print(\"Imputation.\")\n",
    "# do imputation of label with zero if there are still missing values\n",
    "X_4 = X_1\n",
    "X_4['aki_stage'] = X_4['aki_stage'].fillna(0)\n",
    "# using most common within each icustay_id\n",
    "if IMPUTE_EACH_ID:\n",
    "    # set a new variable so won't change the orginial X\n",
    "    column_name = list(X_4.columns)\n",
    "    column_name.remove(column_name[0]) \n",
    "    for feature in column_name:\n",
    "        X_4.loc[X_4[feature].isnull(), feature] = X_4.icustay_id.map(fast_mode(X, ['icustay_id'], feature).set_index('icustay_id')[feature])       \n",
    "\n",
    "# imputation based on whole column\n",
    "if IMPUTE_COLUMN:\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy= IMPUTE_METHOD)\n",
    "    cols = list(X_4.columns)\n",
    "    cols = cols[2:23]\n",
    "    X_4[cols]=imp.fit_transform(X_4[cols])  \n",
    "\n",
    "# If no imputation method selected or only impute each id, for the remaining nan impute direclty with FILL_VALUE\n",
    "X_4 = X_4.fillna(FILL_VALUE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1d075ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icustay_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>uo_rt_6hr</th>\n",
       "      <th>uo_rt_12hr</th>\n",
       "      <th>uo_rt_24hr</th>\n",
       "      <th>aki_stage</th>\n",
       "      <th>creat</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>aniongap_avg</th>\n",
       "      <th>bicarbonate_avg</th>\n",
       "      <th>...</th>\n",
       "      <th>wbc_avg</th>\n",
       "      <th>glucose_avg</th>\n",
       "      <th>heartrate_mean</th>\n",
       "      <th>sysbp_mean</th>\n",
       "      <th>diasbp_mean</th>\n",
       "      <th>resprate_mean</th>\n",
       "      <th>spo2_mean</th>\n",
       "      <th>vent</th>\n",
       "      <th>vasopressor</th>\n",
       "      <th>sedative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200001</td>\n",
       "      <td>2181-11-18 11:10:00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>152234.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.6</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>200001</td>\n",
       "      <td>2181-11-18 11:20:00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200001</td>\n",
       "      <td>2181-11-18 15:29:00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200001</td>\n",
       "      <td>2181-11-18 23:40:00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>200001</td>\n",
       "      <td>2181-11-19 06:00:00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.7</td>\n",
       "      <td>152234.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.7</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10378676</th>\n",
       "      <td>299999</td>\n",
       "      <td>2117-09-01 16:00:00</td>\n",
       "      <td>0.2354</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>1.9528</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10378677</th>\n",
       "      <td>299999</td>\n",
       "      <td>2117-09-02 05:20:00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>129161.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10378678</th>\n",
       "      <td>299999</td>\n",
       "      <td>2117-09-03 05:28:00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>129161.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10378679</th>\n",
       "      <td>299999</td>\n",
       "      <td>2117-09-03 21:36:00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>129161.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10378680</th>\n",
       "      <td>299999</td>\n",
       "      <td>2117-09-04 04:10:00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>129161.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10378681 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          icustay_id           charttime  uo_rt_6hr  uo_rt_12hr  uo_rt_24hr  \\\n",
       "0             200001 2181-11-18 11:10:00     0.0000      0.0000      0.0000   \n",
       "1             200001 2181-11-18 11:20:00     0.0000      0.0000      0.0000   \n",
       "2             200001 2181-11-18 15:29:00     0.0000      0.0000      0.0000   \n",
       "3             200001 2181-11-18 23:40:00     0.0000      0.0000      0.0000   \n",
       "4             200001 2181-11-19 06:00:00     0.0000      0.0000      0.0000   \n",
       "...              ...                 ...        ...         ...         ...   \n",
       "10378676      299999 2117-09-01 16:00:00     0.2354      0.4426      1.9528   \n",
       "10378677      299999 2117-09-02 05:20:00     0.0000      0.0000      0.0000   \n",
       "10378678      299999 2117-09-03 05:28:00     0.0000      0.0000      0.0000   \n",
       "10378679      299999 2117-09-03 21:36:00     0.0000      0.0000      0.0000   \n",
       "10378680      299999 2117-09-04 04:10:00     0.0000      0.0000      0.0000   \n",
       "\n",
       "          aki_stage  creat   hadm_id  aniongap_avg  bicarbonate_avg  ...  \\\n",
       "0               0.0    2.5  152234.0          11.0             30.0  ...   \n",
       "1               0.0    0.0       0.0           0.0              0.0  ...   \n",
       "2               0.0    0.0       0.0           0.0              0.0  ...   \n",
       "3               0.0    0.0       0.0           0.0              0.0  ...   \n",
       "4               0.0    2.7  152234.0          14.0             27.0  ...   \n",
       "...             ...    ...       ...           ...              ...  ...   \n",
       "10378676        1.0    0.0       0.0           0.0              0.0  ...   \n",
       "10378677        0.0    1.0  129161.0          12.0             28.0  ...   \n",
       "10378678        0.0    0.8  129161.0           0.0              0.0  ...   \n",
       "10378679        0.0    0.9  129161.0          13.0             27.0  ...   \n",
       "10378680        0.0    0.8  129161.0           0.0              0.0  ...   \n",
       "\n",
       "          wbc_avg  glucose_avg  heartrate_mean  sysbp_mean  diasbp_mean  \\\n",
       "0             2.6         90.0             0.0         0.0          0.0   \n",
       "1             0.0          0.0             0.0         0.0          0.0   \n",
       "2             0.0          0.0             0.0         0.0          0.0   \n",
       "3             0.0          0.0             0.0         0.0          0.0   \n",
       "4             2.7         74.0             0.0         0.0          0.0   \n",
       "...           ...          ...             ...         ...          ...   \n",
       "10378676      0.0          0.0             0.0         0.0          0.0   \n",
       "10378677     10.0        104.0             0.0         0.0          0.0   \n",
       "10378678     10.6          0.0             0.0         0.0          0.0   \n",
       "10378679      0.0         88.0             0.0         0.0          0.0   \n",
       "10378680      0.0          0.0             0.0         0.0          0.0   \n",
       "\n",
       "          resprate_mean  spo2_mean  vent  vasopressor  sedative  \n",
       "0                   0.0        0.0     0            0         0  \n",
       "1                   0.0        0.0     0            0         0  \n",
       "2                   0.0        0.0     0            0         0  \n",
       "3                   0.0        0.0     0            0         0  \n",
       "4                   0.0        0.0     0            0         0  \n",
       "...                 ...        ...   ...          ...       ...  \n",
       "10378676            0.0        0.0     0            0         0  \n",
       "10378677            0.0        0.0     0            0         0  \n",
       "10378678            0.0        0.0     0            0         0  \n",
       "10378679            0.0        0.0     0            0         0  \n",
       "10378680            0.0        0.0     0            0         0  \n",
       "\n",
       "[10378681 rows x 26 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_4 # imputation but without resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b583aaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "# more comfortable to review in this order\n",
    "def change_order(X):\n",
    "    try:\n",
    "        cols = ['icustay_id', 'charttime','aki_stage','hadm_id','aniongap_avg','bicarbonate_avg', 'bun_avg','chloride_avg',\n",
    "                'creat','diasbp_mean', 'glucose_avg', 'heartrate_mean', 'hematocrit_avg','hemoglobin_avg', \n",
    "                'potassium_avg', 'resprate_mean', 'sodium_avg','spo2_mean', 'sysbp_mean', 'uo_rt_12hr', \n",
    "                'uo_rt_24hr', 'uo_rt_6hr','wbc_avg', 'sedative', 'vasopressor', 'vent' ]\n",
    "        X = X[cols]\n",
    "        print(\"success\")\n",
    "    except:\n",
    "        try:\n",
    "            cols = ['icustay_id', 'charttime','aki_stage','creat','uo_rt_12hr', 'uo_rt_24hr', 'uo_rt_6hr']\n",
    "            X = X[cols]\n",
    "        except:\n",
    "            print(\"error\")\n",
    "            \n",
    "change_order(X)\n",
    "change_order(X_1)\n",
    "change_order(X_2)\n",
    "change_order(X_3)\n",
    "change_order(X_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a170103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binarise labels\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n",
      "success\n"
     ]
    }
   ],
   "source": [
    "print(\"binarise labels\")\n",
    "def binarise_lable(X):\n",
    "    if ALL_STAGES:\n",
    "        pass\n",
    "    elif CLASS1:\n",
    "        X.loc[X['aki_stage'] > 1, 'aki_stage'] = 1\n",
    "    elif CLASS2:\n",
    "        X.loc[X['aki_stage'] < 2, 'aki_stage'] = 0\n",
    "        X.loc[X['aki_stage'] > 1, 'aki_stage'] = 1\n",
    "    elif CLASS3:\n",
    "        X.loc[X['aki_stage'] < 3, 'aki_stage'] = 0\n",
    "        X.loc[X['aki_stage'] > 2, 'aki_stage'] = 1\n",
    "    print('success')\n",
    "        \n",
    "binarise_lable(X)\n",
    "binarise_lable(X_1)\n",
    "binarise_lable(X_2)\n",
    "binarise_lable(X_3)\n",
    "binarise_lable(X_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d5ae90e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1774515, 26)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(\"Shifting the labels 48 h\") # by 8 position : 6h sampling*8=48h and ffil 8 newly empty ones\n",
    "# group by\n",
    "X['aki_stage'] = X.groupby('icustay_id')['aki_stage'].shift(-(HOURS_AHEAD // int(SAMPLING_INTERVAL[:-1])))\n",
    "X = X.dropna(subset=['aki_stage'])\n",
    "X['icustay_id'].unique()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7440fd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     icustay_id           charttime  uo_rt_6hr  uo_rt_12hr  uo_rt_24hr  \\\n",
      "0        200001 2181-11-18 11:10:00        NaN         NaN         NaN   \n",
      "1        200001 2181-11-18 11:20:00        NaN         NaN         NaN   \n",
      "2        200001 2181-11-18 15:29:00        NaN         NaN         NaN   \n",
      "3        200001 2181-11-18 23:40:00        NaN         NaN         NaN   \n",
      "4        200001 2181-11-19 06:00:00        NaN         NaN         NaN   \n",
      "..          ...                 ...        ...         ...         ...   \n",
      "150      200001 2181-12-01 13:20:00        NaN         NaN         NaN   \n",
      "151      200001 2181-12-01 21:00:00        NaN         NaN         NaN   \n",
      "152      200001 2181-12-02 05:30:00        NaN         NaN         NaN   \n",
      "153      200001 2181-12-03 06:10:00        NaN         NaN         NaN   \n",
      "154      200001 2181-12-04 05:58:00        NaN         NaN         NaN   \n",
      "\n",
      "     aki_stage  creat   hadm_id  aniongap_avg  bicarbonate_avg  ...  wbc_avg  \\\n",
      "0          0.0    2.5  152234.0          11.0             30.0  ...      2.6   \n",
      "1          0.0    NaN       NaN           NaN              NaN  ...      NaN   \n",
      "2          0.0    NaN       NaN           NaN              NaN  ...      NaN   \n",
      "3          0.0    NaN       NaN           NaN              NaN  ...      NaN   \n",
      "4          0.0    2.7  152234.0          14.0             27.0  ...      2.7   \n",
      "..         ...    ...       ...           ...              ...  ...      ...   \n",
      "150        0.0    NaN  152234.0           NaN              NaN  ...      NaN   \n",
      "151        0.0    NaN  152234.0           NaN              NaN  ...      NaN   \n",
      "152        1.0    3.9  152234.0          12.0             30.0  ...      2.9   \n",
      "153        0.0    4.0  152234.0          14.0             28.0  ...      3.1   \n",
      "154        0.0    3.5  152234.0          13.0             31.0  ...      3.1   \n",
      "\n",
      "     glucose_avg  heartrate_mean  sysbp_mean  diasbp_mean  resprate_mean  \\\n",
      "0           90.0             NaN         NaN          NaN            NaN   \n",
      "1            NaN             NaN         NaN          NaN            NaN   \n",
      "2            NaN             NaN         NaN          NaN            NaN   \n",
      "3            NaN             NaN         NaN          NaN            NaN   \n",
      "4           74.0             NaN         NaN          NaN            NaN   \n",
      "..           ...             ...         ...          ...            ...   \n",
      "150          NaN             NaN         NaN          NaN            NaN   \n",
      "151          NaN             NaN         NaN          NaN            NaN   \n",
      "152         82.0             NaN         NaN          NaN            NaN   \n",
      "153         90.0             NaN         NaN          NaN            NaN   \n",
      "154         72.0             NaN         NaN          NaN            NaN   \n",
      "\n",
      "     spo2_mean  vent  vasopressor  sedative  \n",
      "0          NaN     0            0         0  \n",
      "1          NaN     0            0         0  \n",
      "2          NaN     0            0         0  \n",
      "3          NaN     0            0         0  \n",
      "4          NaN     0            0         0  \n",
      "..         ...   ...          ...       ...  \n",
      "150        NaN     0            0         0  \n",
      "151        NaN     0            0         0  \n",
      "152        NaN     0            0         0  \n",
      "153        NaN     0            0         0  \n",
      "154        NaN     0            0         0  \n",
      "\n",
      "[155 rows x 26 columns]\n",
      "icustay_id               0\n",
      "charttime                0\n",
      "uo_rt_6hr          7410848\n",
      "uo_rt_12hr         7410848\n",
      "uo_rt_24hr         7410848\n",
      "aki_stage                0\n",
      "creat              9700598\n",
      "hadm_id            2755638\n",
      "aniongap_avg       9736843\n",
      "bicarbonate_avg    9726166\n",
      "chloride_avg       9666204\n",
      "hematocrit_avg     9556785\n",
      "hemoglobin_avg     9680495\n",
      "potassium_avg      9478834\n",
      "sodium_avg         9631702\n",
      "bun_avg            9713676\n",
      "wbc_avg            9766158\n",
      "glucose_avg        8710918\n",
      "heartrate_mean     4604520\n",
      "sysbp_mean         5042941\n",
      "diasbp_mean        5044543\n",
      "resprate_mean      4573154\n",
      "spo2_mean          4752951\n",
      "vent                     0\n",
      "vasopressor              0\n",
      "sedative                 0\n",
      "dtype: int64\n",
      "10378681\n"
     ]
    }
   ],
   "source": [
    "print(X_1[X_1['icustay_id'].isin([200001])])\n",
    "# print(X_1[X_1['icustay_id'].isin([200001])].isna().sum())\n",
    "print(X_1.isna().sum())\n",
    "print(X_1.shape[0])\n",
    "\n",
    "# print(X_2[X_2['icustay_id'].isin([200001])])\n",
    "# print(X_2[X_2['icustay_id'].isin([200001])].isna().sum())\n",
    "# print(X_2.isna().sum())\n",
    "\n",
    "# print(X_3[X_3['icustay_id'].isin([200001])])\n",
    "# print(X_3[X_3['icustay_id'].isin([200001])].isna().sum())\n",
    "# print(X_3.isna().sum())\n",
    "\n",
    "# print(X_1.shape)\n",
    "# print(X_4.shape)\n",
    "# print(X_4[X_4['icustay_id'].isin([200001])])\n",
    "# print(X_4.columns[2:])\n",
    "# print(len(X_4.columns[2:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375cadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many NA\n",
    "for i in X['icustay_id'].unique():\n",
    "    X['creat'][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b7fd8eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aggregate timestamps\n",
    "def delete_timestamps(X):\n",
    "    tempX = pd.DataFrame()\n",
    "    for i in X['icustay_id'].unique():\n",
    "        tempa = []\n",
    "        for j in X.columns[2:]:\n",
    "            tempa.append(X[j][X['icustay_id'].isin([i])].mean())\n",
    "        tempa = pd.DataFrame(tempa)\n",
    "        tempX.append(tempa,ignore_index = True)\n",
    "    return tempX\n",
    "\n",
    "X_del = delete_timestamps(X_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390303be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3.plot(figsize=(15, 6))\n",
    "plt.show()\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from matplotlib import pyplot\n",
    "decomposition = seasonal_decompose(X_3, model='additive')\n",
    "fig = decomposition.plot()\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
